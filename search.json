[
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html",
    "href": "mlm_seminar_sose23/lineare_regression.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Das Ziel einer Regression besteht darin, eine Variable durch eine oder mehrere andere Variablen vorherzusagen. An dieser Stelle k√∂nnen wir von einer Art Prognose sprechen. Wenn wir Regressionsmodelle benutzen, wollen wir den Verlauf einer Variable (die abh√§ngige Variable) anhand anderer Variablen (unabh√§ngige Variablen) prognostizieren.\nDie vorhergesagte Variable wird Kriterium, Regressand oder auch abh√§ngige Variable (AV) genannt und wird √ºblicherweise mit \\(y\\) symbolisiert. Die Variablen, die zur Vorhersage der abh√§ngigen Variablen verwendet werden, werden Pr√§diktoren, Regressoren oder unabh√§ngige Variablen (UV) genannt. √úblicherweise werden Pr√§diktoren mit \\(x_{1},~x_{2},~x_{3},~\\dots\\) (oder kurz, mit \\(X\\)) symbolisiert.\nSie erinnern sich wahrscheinlich an Statistik 1 und 2, wo wir meistens nur einen Pr√§diktor zur Vorhersage einer anderen Variable benutzt haben. In diesem Fall sprechen wir von einer einfachen Regression. In diesem Fall k√∂nnen wir eine Vorhersage von \\(y\\) mit der Gleichung der einfachen linearen Regression formalisieren:\n\\[\ny_{m} = b_{0} + b_{1} \\cdot x_{m1} + b_{2} \\cdot x_{m2} + \\dots + e_{m}\n\\tag{1}\\]\nHier steht der Index \\(\\mathbf{m}\\) f√ºr die Untersuchungseinheit (z.B. f√ºr eine Person, oder eine einzelne Messung).\nUm die Grundidee eines Regressionsmodells besser zu verstehen, schauen wir uns nun ein kleines Beispiel an.\n\n\n\nBetrachten Sie die folgende Abbildung:\n\n\n\nAnstieg des Meeresspiegels seit dem Jahr 1993 bis 2021 in Millimeter (Stand: September). In Statista. Zugriff am 15. Mai 2023, von https://de.statista.com/statistik/daten/studie/1056576/umfrage/hoehe-des-meeresspiegels/\n\n\n\n\n\n\n\n\nWas k√∂nnen wir aus dieser Abbildung lernen?\n\n\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Anhand der abgebildeten Zahlen l√§sst sich leicht erkennen, dass seit 1993 der Meeresspiegel pro Jahr um mehrere Millimeter angestiegen ist.\n\n\n\n\nGehen wir davon aus, dass wir die Ver√§nderung im Meeresspiegel, die durch die Abbildung angedeutet wird, m√∂glichst pr√§gnant zusammenfassen wollen. An dieser Stelle stehen uns unterschiedliche Methoden zur Verf√ºgung. Wir k√∂nnten sagen:\n\n\n\n‚ÄúDer Meeresspiegel ist seit 1993 gestiegen‚Äù, oder ‚ÄúDer Meeresspiegel steigt seit 1993 um mehrere Millimeter‚Äù.\n\n\n\nSobald wir aber eine Aussage √ºber die Auspr√§gung dieser Ver√§nderung treffen m√∂chten, br√§uchten wir eine Methode, die uns erlaubt all die kleinen Ver√§nderungen, welche von Jahr zu Jahr gemessen wurden, zusammenzufassen. Eine M√∂glichkeit w√§re, die gemessenen Abweichungen zu mitteln. Damit k√∂nnten wir feststellen, um wie viel der Meeresspiegel im Mittel pro Jahr angestiegen (oder abgefallen ist). Lasst uns diese Berechnung in R durchf√ºhren.\n\n\n\n\n# die Daten k√∂nnen mit diesem Befehl geladen werden\nmeeresspiegel &lt;- read.csv(file=\"https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main/data/meeresspiegel.csv\")\n\n\n\n\n\n# wir k√∂nnen `dplyr` benutzen \n# um die mittlere Abweichung zu berechnen\nrequire(dplyr)\n\nmeeresspiegel %&gt;%\n  # Differenz zwischen zwei aufeinaderefolgende Werte\n  mutate(Abweichung = lead(Anstieg) - Anstieg) %&gt;%\n  # bilde Mittelwert (schlie√ü Fehlendewerte aus)\n  summarize(Mittlere_Abweichung = mean(Abweichung, na.rm = TRUE))\n\n  Mittlere_Abweichung\n1            3.607143\n\n\n\nNun k√∂nnen wir die Abweichung im Meeresspiegel genauer Beschreiben:\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Tats√§chlich steigt der Meeresspiegel im Vergleich zu 1993 pro Jahr um durchschnittlich 3,6 mm.\n\n\n\n\n\n\nAnhand dieser Zahlen liese sich gleich prognostizieren, wie der Meerespiegel in den komenden Jahren ver√§ndern wird. Wenn alles so bleibt wie bisher, k√∂nnen wir davon ausgehen, dass der Meeresspiegel weiteransteigen wird. Das sind keine gute Nachrichten.\nEine gute Nachricht ist allerdings, dass lineare Regressionsmodelle, im Prinzip nichts anderes als die mittlere Abweichung zu berechnen, um eine Vorhersage von \\(y\\) (in unser Beispiel die Abweichung im gemessenen Meeresspiegel) anhand von \\(x\\) (in unser Beispiel, das Jahr der Messung) vorzunehmen. Die genauen Berechnungsschritte sind ein kleines bisschen komplexer, aber die Logik ist im Prinzip die gleiche.\nWir k√∂nnen dies leicht mit einem linearen Regression in R √ºberpr√ºfen.\n\n\nWir k√∂nnen eine lineare Regression mit der Funktion lm() in R berechnen.\n\n# lm() nimmt mehrere Argumente. Heute brauchen wir `data` und `formula`\n# `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\nlin_reg &lt;- lm(\n  # `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\n  data = meeresspiegel, \n  # `formula = 1 +  Anstieg ~ Jahr` sagt der Funktion wie die Formel unserer\n  # Regression aussehen soll\n              formula = 1 +  Anstieg ~ Jahr)\n\nMit summary() k√∂nnen wir uns die Ergebnisse der Regression anzeigen lassen:\n\nsummary(lin_reg)\n\n\nCall:\nlm(formula = 1 + Anstieg ~ Jahr, data = meeresspiegel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4157  -2.7138   0.3881   2.3164   7.7297 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.728e+03  1.882e+02  -35.75   &lt;2e-16 ***\nJahr         3.375e+00  9.375e-02   36.00   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.224 on 27 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9788 \nF-statistic:  1296 on 1 and 27 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nWie kommen diese Zahlen zustande?\nWie k√∂nnen wir aus vergangenen Beobachtungen zuk√ºnftige Beobachtungen vorhersagen?\nWas ist eine gute Vorhersage, und warum sind Vorhersagen mehr oder weniger akkurat?\n\n\nSchauen wir uns ein weiteres Beispiel an.\n\n\n\n\n\nStellen Sie sich vor, dass Sie als ambulante Psychotherapeut:in in einer Praxis arbeiten. Am Anfang eines jeden Quartals erstellen Sie eine Kostenplanung. Die erwartete Anzahl an Behandlungsstunden spielt dabei eine gro√üe Rolle, da Sie damit den erwarteten Wert Ihres Einkommens berechnen k√∂nnen. Gehen wir davon aus, dass Sie pro Patient:in ein Betrag von 66,6 Euro (nach Abzug aller laufenden Kosten) in der Quartalsabrechnung erwarten k√∂nnen.\n\nBetrachten Sie die folgende Abbildung:\n\n\nDie Abbildung zeigt einen linearen Zusammenhang zwischen der Anzahl an Patienten und dem Betrag auf der Abrechnung. Wir k√∂nnen dieser Zusammenhang bildlich darstellen (wie in der Abbildung) aber auch mithilfe mit eines Modells.\n\n\\[Betrag = (wie~von~x~auf~y~kommen?) + Fehler\\]\n\nIn unseren Beispiel, welche Funktion muss auf \\(x\\) angewendet werden, um eine m√∂glichst genaue Sch√§tzung von \\(y\\) zu bekommen?\nWir k√∂nnen diese Fragen mithilfe einer mathematischen Formel ausdrucken.\n\n\\[y = f(x) + Fehler\\]\n\nDies ist die Grundlage eines Regressionsmodells. Regressionsmodelle sind nichts anderes als eine Speziefietzierung dieser Frage. Mit einem Regressionsmodell, k√∂nnen wir die Funktion, die auf \\(x\\) angewendet werden muss, genauer beschreiben.\nDies ist die allgemeine Formel der Regression:\n\n\\[y_{i} = b_{0} + b_{1}~x_{1} + e_{i}\\]\n\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Auspr√§gung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Pr√§diktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden l√§sst erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt\n\n\\(e_{i} =\\) Regressionsresiduum (kurz: Residuum), Residualwert oder Fehlerwert:\n\nDie Differenz zwischen einem vorhergesagten (\\(\\hat{y}\\)) und beobachteten (\\(y\\)) \\(y\\)-Wert.\nJe gr√∂√üer die Fehlerwerte, umso gr√∂√üer ist die Abweichung eines beobachteten vom vorhergesagten Wert."
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html#beispiel-1-der-meeresspiegel-steigt-an",
    "href": "mlm_seminar_sose23/lineare_regression.html#beispiel-1-der-meeresspiegel-steigt-an",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Betrachten Sie die folgende Abbildung:\n\n\n\nAnstieg des Meeresspiegels seit dem Jahr 1993 bis 2021 in Millimeter (Stand: September). In Statista. Zugriff am 15. Mai 2023, von https://de.statista.com/statistik/daten/studie/1056576/umfrage/hoehe-des-meeresspiegels/\n\n\n\n\n\n\n\n\nWas k√∂nnen wir aus dieser Abbildung lernen?\n\n\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Anhand der abgebildeten Zahlen l√§sst sich leicht erkennen, dass seit 1993 der Meeresspiegel pro Jahr um mehrere Millimeter angestiegen ist.\n\n\n\n\nGehen wir davon aus, dass wir die Ver√§nderung im Meeresspiegel, die durch die Abbildung angedeutet wird, m√∂glichst pr√§gnant zusammenfassen wollen. An dieser Stelle stehen uns unterschiedliche Methoden zur Verf√ºgung. Wir k√∂nnten sagen:\n\n\n\n‚ÄúDer Meeresspiegel ist seit 1993 gestiegen‚Äù, oder ‚ÄúDer Meeresspiegel steigt seit 1993 um mehrere Millimeter‚Äù.\n\n\n\nSobald wir aber eine Aussage √ºber die Auspr√§gung dieser Ver√§nderung treffen m√∂chten, br√§uchten wir eine Methode, die uns erlaubt all die kleinen Ver√§nderungen, welche von Jahr zu Jahr gemessen wurden, zusammenzufassen. Eine M√∂glichkeit w√§re, die gemessenen Abweichungen zu mitteln. Damit k√∂nnten wir feststellen, um wie viel der Meeresspiegel im Mittel pro Jahr angestiegen (oder abgefallen ist). Lasst uns diese Berechnung in R durchf√ºhren.\n\n\n\n\n# die Daten k√∂nnen mit diesem Befehl geladen werden\nmeeresspiegel &lt;- read.csv(file=\"https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main/data/meeresspiegel.csv\")\n\n\n\n\n\n# wir k√∂nnen `dplyr` benutzen \n# um die mittlere Abweichung zu berechnen\nrequire(dplyr)\n\nmeeresspiegel %&gt;%\n  # Differenz zwischen zwei aufeinaderefolgende Werte\n  mutate(Abweichung = lead(Anstieg) - Anstieg) %&gt;%\n  # bilde Mittelwert (schlie√ü Fehlendewerte aus)\n  summarize(Mittlere_Abweichung = mean(Abweichung, na.rm = TRUE))\n\n  Mittlere_Abweichung\n1            3.607143\n\n\n\nNun k√∂nnen wir die Abweichung im Meeresspiegel genauer Beschreiben:\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Tats√§chlich steigt der Meeresspiegel im Vergleich zu 1993 pro Jahr um durchschnittlich 3,6 mm."
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html#ist-die-mittlere-abweichung-ein-guter-sch√§tzer",
    "href": "mlm_seminar_sose23/lineare_regression.html#ist-die-mittlere-abweichung-ein-guter-sch√§tzer",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Anhand dieser Zahlen liese sich gleich prognostizieren, wie der Meerespiegel in den komenden Jahren ver√§ndern wird. Wenn alles so bleibt wie bisher, k√∂nnen wir davon ausgehen, dass der Meeresspiegel weiteransteigen wird. Das sind keine gute Nachrichten.\nEine gute Nachricht ist allerdings, dass lineare Regressionsmodelle, im Prinzip nichts anderes als die mittlere Abweichung zu berechnen, um eine Vorhersage von \\(y\\) (in unser Beispiel die Abweichung im gemessenen Meeresspiegel) anhand von \\(x\\) (in unser Beispiel, das Jahr der Messung) vorzunehmen. Die genauen Berechnungsschritte sind ein kleines bisschen komplexer, aber die Logik ist im Prinzip die gleiche.\nWir k√∂nnen dies leicht mit einem linearen Regression in R √ºberpr√ºfen.\n\n\nWir k√∂nnen eine lineare Regression mit der Funktion lm() in R berechnen.\n\n# lm() nimmt mehrere Argumente. Heute brauchen wir `data` und `formula`\n# `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\nlin_reg &lt;- lm(\n  # `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\n  data = meeresspiegel, \n  # `formula = 1 +  Anstieg ~ Jahr` sagt der Funktion wie die Formel unserer\n  # Regression aussehen soll\n              formula = 1 +  Anstieg ~ Jahr)\n\nMit summary() k√∂nnen wir uns die Ergebnisse der Regression anzeigen lassen:\n\nsummary(lin_reg)\n\n\nCall:\nlm(formula = 1 + Anstieg ~ Jahr, data = meeresspiegel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4157  -2.7138   0.3881   2.3164   7.7297 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.728e+03  1.882e+02  -35.75   &lt;2e-16 ***\nJahr         3.375e+00  9.375e-02   36.00   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.224 on 27 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9788 \nF-statistic:  1296 on 1 and 27 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nWie kommen diese Zahlen zustande?\nWie k√∂nnen wir aus vergangenen Beobachtungen zuk√ºnftige Beobachtungen vorhersagen?\nWas ist eine gute Vorhersage, und warum sind Vorhersagen mehr oder weniger akkurat?\n\n\nSchauen wir uns ein weiteres Beispiel an."
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html#beispiel-2-more-patients-more-cash",
    "href": "mlm_seminar_sose23/lineare_regression.html#beispiel-2-more-patients-more-cash",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Stellen Sie sich vor, dass Sie als ambulante Psychotherapeut:in in einer Praxis arbeiten. Am Anfang eines jeden Quartals erstellen Sie eine Kostenplanung. Die erwartete Anzahl an Behandlungsstunden spielt dabei eine gro√üe Rolle, da Sie damit den erwarteten Wert Ihres Einkommens berechnen k√∂nnen. Gehen wir davon aus, dass Sie pro Patient:in ein Betrag von 66,6 Euro (nach Abzug aller laufenden Kosten) in der Quartalsabrechnung erwarten k√∂nnen.\n\nBetrachten Sie die folgende Abbildung:\n\n\nDie Abbildung zeigt einen linearen Zusammenhang zwischen der Anzahl an Patienten und dem Betrag auf der Abrechnung. Wir k√∂nnen dieser Zusammenhang bildlich darstellen (wie in der Abbildung) aber auch mithilfe mit eines Modells.\n\n\\[Betrag = (wie~von~x~auf~y~kommen?) + Fehler\\]\n\nIn unseren Beispiel, welche Funktion muss auf \\(x\\) angewendet werden, um eine m√∂glichst genaue Sch√§tzung von \\(y\\) zu bekommen?\nWir k√∂nnen diese Fragen mithilfe einer mathematischen Formel ausdrucken.\n\n\\[y = f(x) + Fehler\\]\n\nDies ist die Grundlage eines Regressionsmodells. Regressionsmodelle sind nichts anderes als eine Speziefietzierung dieser Frage. Mit einem Regressionsmodell, k√∂nnen wir die Funktion, die auf \\(x\\) angewendet werden muss, genauer beschreiben.\nDies ist die allgemeine Formel der Regression:\n\n\\[y_{i} = b_{0} + b_{1}~x_{1} + e_{i}\\]\n\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Auspr√§gung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Pr√§diktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden l√§sst erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt\n\n\\(e_{i} =\\) Regressionsresiduum (kurz: Residuum), Residualwert oder Fehlerwert:\n\nDie Differenz zwischen einem vorhergesagten (\\(\\hat{y}\\)) und beobachteten (\\(y\\)) \\(y\\)-Wert.\nJe gr√∂√üer die Fehlerwerte, umso gr√∂√üer ist die Abweichung eines beobachteten vom vorhergesagten Wert."
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression_parameters.html",
    "href": "mlm_seminar_sose23/lineare_regression_parameters.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "In den letzten Sitzungen haben wir uns mit der einfachen linearen Regression befasst. Dabei haben wir uns erst einmal daf√ºr entschieden, eine Regression ohne Ber√ºcksichtigung der verschiedenen Subgruppen in den Daten zu berechnen.\nMit einem Pr√§diktor (\\(x_{1}\\) ), lautet die Formel der Regression:\n\\[\n\\widehat{y}_{m} = b_{0} + b_{1} \\cdot x_{m1}\n\\tag{1}\\]\nSie liefert uns zwei Parameter $b_{0} und \\(b_{1}\\).\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Auspr√§gung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Pr√§diktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden l√§sst erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt.\n\n\nUm die ‚ÄúInterpretierbarkeit‚Äù der Parameter (vorallem die des Interzepts.\nWir haben ebenfalls √ºber ‚ÄúZentrierung‚Äù gesprochen. Wenn wir eine Variable an ihren Mittelwert zentrieren, ziehen wir den Mittelwert der Variable von allen Werten (d.h. alle Beobachtungen) der Variable ab. Zentrierung definiert das Minimum und Maximum der Variable neu. Der Mittelwert einer am Mittelwert zentrierten Variable ist nun gleich Null. Wir k√∂nnen Zentrierung als eine ‚ÄúDaten-Vorverarbeitungsstrategie‚Äù betrachten, die die ‚ÄúInterpretierbarkeit‚Äù der Parameter (vor allem die des Intercepts) in einem linearen Modell steigert.\nBetrachten wir z.B. die folgende Abbildung:\n\n\n\n\n\n\n\n\n\nDargestellt sind die Ergebnisse zweier Regressionen. Beide Regressionen beschreiben den Zusammenhang zwischen Schulunlust und Schulleistung. Die linke Seite der Abbildung zeigt die Ergebnisse der Regression ohne Zentrierung von Schulunlust und die rechte Seite mit Zentrierung. Ein Unterschied ist, dass die Spannweite der zentrierten Daten ein anderer ist. Personen, die einen negativen Wert in der zeitrierten Schulunlust-Variable haben, befinden sich unterhalb des Mittelwert der Gesamtstichprobe. Personen mit positiven Werten befinden sich dar√ºber. Ein weiterer Unterschied f√§llt auf, wenn wir die Intercepts der Modelle betrachten. Das Interzept auf der rechten Seite liegt inmitten der Verteilung (auf dem Mittelwert, welcher nach Zentrierung gleich Null ist). Zentrierung kann uns also helfen, das Interzept in einem f√ºr uns interpretierbaren Bereich ‚Äúzu holen‚Äù.\n\n\n\nGehen wir nun zu einem Beispiel √ºber, das die Subgruppen in den Daten ber√ºcksichtigt. Daf√ºr ben√∂tigen wir die folgenden Daten:\n\n# die Daten k√∂nnen mit diesem Befehl geladen werden\nurlRemote &lt;- 'https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main'\nfpathData &lt;- '/data/schulunlust.txt'\ndata_schulleistung &lt;- read.table(paste0(urlRemote, fpathData),\n                          header = TRUE, dec = ',')\n\n# wir werden nur die ersten 5 klassen benutzen\n# wir k√∂nnen die Daten dieser Klassen mit\n# `dplyr`filtern\nrequire(dplyr)\ndata_schulleistung &lt;- data_schulleistung %&gt;%\n  filter(klasse_nr &lt;= 5) %&gt;%\n  # ebenfalls werden wir die variable `unlust`\n  # zentrieren\n  mutate(unlust_c = unlust - mean(unlust))\n\nDiese Daten sind identisch mit den Daten der obigen Abbildung.\n\n\n\nErster Schritt:\n\nBerechnen wir erst einmal eine Gesamt-Regression, ohne die Subgruppen (die einzelnen Klassen) in den Daten zu ber√ºcksichtigen.\nZur Erinnerung: Der R-Befehl, um eine lineare Regression zu berechnen lautet lm(fomula, data)\n\nZweiter Schritt:\n\nBerechnen wir 5 verschiedene Regressionen, eine f√ºr jede Klasse.\nTipp: Sie k√∂nnen die Daten der einzelnen Klassen mit dplyr() filtern.\n\nz.B.: klasse_1 &lt;- data_schulleistung %&gt;% filter(klasse_nr == 1)\nDanach k√∂nnen Sie die Regression mit lm() berechnen.\n\n\nSchreiben Sie \\(b_{0}\\) und \\(b_{1}\\) f√ºr alle Regressionen auf und vergleichen Sie.\n\n\n\nDie Ergebnisse sollten mit der folgenden Abbildung kompatibel sein.\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionsparameter f√ºr das jeweilige \"Klassenmodell\"Modelbeta_0beta_1119.086-0.552219.212-0.266319.310-1.397424.9371.087524.6160.087Note. 1 = Klasse 1; 2 = Klasse 2; etc.\n\n\n\n\n\n\nVergleichen Sie nun die Ergebnisse der einzelnen Regressionen mit den Ergebnissen eines ‚ÄúGesamtregressionsmodells‚Äù (ein Modell √ºber alle Klassen hinweg).\nHier sind die Ergebnisse:\n\nges_mod &lt;- lm(data = data_schulleistung,\n              leistung ~  unlust_c)\n\nges_df &lt;- data.frame(model = 'gesamt',\n                     beta_0 = summary(ges_mod)$coefficients[1],\n                     beta_1 = summary(ges_mod)$coefficients[2])\n\nnice_table(ges_df,\n           col.format.custom = 2:3, format.custom = \"fun\",\n           title = 'Regressionsparameter f√ºr das \"Gesamtmodell\"',\n           width = 0.5)\n\n\nRegressionsparameter f√ºr das \"Gesamtmodell\"modelbeta_0beta_1gesamt21.053-1.128"
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression_parameters.html#bezug-zur-vorherigen-stunde",
    "href": "mlm_seminar_sose23/lineare_regression_parameters.html#bezug-zur-vorherigen-stunde",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "In den letzten Sitzungen haben wir uns mit der einfachen linearen Regression befasst. Dabei haben wir uns erst einmal daf√ºr entschieden, eine Regression ohne Ber√ºcksichtigung der verschiedenen Subgruppen in den Daten zu berechnen.\nMit einem Pr√§diktor (\\(x_{1}\\) ), lautet die Formel der Regression:\n\\[\n\\widehat{y}_{m} = b_{0} + b_{1} \\cdot x_{m1}\n\\tag{1}\\]\nSie liefert uns zwei Parameter $b_{0} und \\(b_{1}\\).\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Auspr√§gung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Pr√§diktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden l√§sst erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt.\n\n\nUm die ‚ÄúInterpretierbarkeit‚Äù der Parameter (vorallem die des Interzepts.\nWir haben ebenfalls √ºber ‚ÄúZentrierung‚Äù gesprochen. Wenn wir eine Variable an ihren Mittelwert zentrieren, ziehen wir den Mittelwert der Variable von allen Werten (d.h. alle Beobachtungen) der Variable ab. Zentrierung definiert das Minimum und Maximum der Variable neu. Der Mittelwert einer am Mittelwert zentrierten Variable ist nun gleich Null. Wir k√∂nnen Zentrierung als eine ‚ÄúDaten-Vorverarbeitungsstrategie‚Äù betrachten, die die ‚ÄúInterpretierbarkeit‚Äù der Parameter (vor allem die des Intercepts) in einem linearen Modell steigert.\nBetrachten wir z.B. die folgende Abbildung:\n\n\n\n\n\n\n\n\n\nDargestellt sind die Ergebnisse zweier Regressionen. Beide Regressionen beschreiben den Zusammenhang zwischen Schulunlust und Schulleistung. Die linke Seite der Abbildung zeigt die Ergebnisse der Regression ohne Zentrierung von Schulunlust und die rechte Seite mit Zentrierung. Ein Unterschied ist, dass die Spannweite der zentrierten Daten ein anderer ist. Personen, die einen negativen Wert in der zeitrierten Schulunlust-Variable haben, befinden sich unterhalb des Mittelwert der Gesamtstichprobe. Personen mit positiven Werten befinden sich dar√ºber. Ein weiterer Unterschied f√§llt auf, wenn wir die Intercepts der Modelle betrachten. Das Interzept auf der rechten Seite liegt inmitten der Verteilung (auf dem Mittelwert, welcher nach Zentrierung gleich Null ist). Zentrierung kann uns also helfen, das Interzept in einem f√ºr uns interpretierbaren Bereich ‚Äúzu holen‚Äù."
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression_parameters.html#ber√ºcksichtigung-von-subgruppen",
    "href": "mlm_seminar_sose23/lineare_regression_parameters.html#ber√ºcksichtigung-von-subgruppen",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Gehen wir nun zu einem Beispiel √ºber, das die Subgruppen in den Daten ber√ºcksichtigt. Daf√ºr ben√∂tigen wir die folgenden Daten:\n\n# die Daten k√∂nnen mit diesem Befehl geladen werden\nurlRemote &lt;- 'https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main'\nfpathData &lt;- '/data/schulunlust.txt'\ndata_schulleistung &lt;- read.table(paste0(urlRemote, fpathData),\n                          header = TRUE, dec = ',')\n\n# wir werden nur die ersten 5 klassen benutzen\n# wir k√∂nnen die Daten dieser Klassen mit\n# `dplyr`filtern\nrequire(dplyr)\ndata_schulleistung &lt;- data_schulleistung %&gt;%\n  filter(klasse_nr &lt;= 5) %&gt;%\n  # ebenfalls werden wir die variable `unlust`\n  # zentrieren\n  mutate(unlust_c = unlust - mean(unlust))\n\nDiese Daten sind identisch mit den Daten der obigen Abbildung.\n\n\n\nErster Schritt:\n\nBerechnen wir erst einmal eine Gesamt-Regression, ohne die Subgruppen (die einzelnen Klassen) in den Daten zu ber√ºcksichtigen.\nZur Erinnerung: Der R-Befehl, um eine lineare Regression zu berechnen lautet lm(fomula, data)\n\nZweiter Schritt:\n\nBerechnen wir 5 verschiedene Regressionen, eine f√ºr jede Klasse.\nTipp: Sie k√∂nnen die Daten der einzelnen Klassen mit dplyr() filtern.\n\nz.B.: klasse_1 &lt;- data_schulleistung %&gt;% filter(klasse_nr == 1)\nDanach k√∂nnen Sie die Regression mit lm() berechnen.\n\n\nSchreiben Sie \\(b_{0}\\) und \\(b_{1}\\) f√ºr alle Regressionen auf und vergleichen Sie.\n\n\n\nDie Ergebnisse sollten mit der folgenden Abbildung kompatibel sein.\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionsparameter f√ºr das jeweilige \"Klassenmodell\"Modelbeta_0beta_1119.086-0.552219.212-0.266319.310-1.397424.9371.087524.6160.087Note. 1 = Klasse 1; 2 = Klasse 2; etc.\n\n\n\n\n\n\nVergleichen Sie nun die Ergebnisse der einzelnen Regressionen mit den Ergebnissen eines ‚ÄúGesamtregressionsmodells‚Äù (ein Modell √ºber alle Klassen hinweg).\nHier sind die Ergebnisse:\n\nges_mod &lt;- lm(data = data_schulleistung,\n              leistung ~  unlust_c)\n\nges_df &lt;- data.frame(model = 'gesamt',\n                     beta_0 = summary(ges_mod)$coefficients[1],\n                     beta_1 = summary(ges_mod)$coefficients[2])\n\nnice_table(ges_df,\n           col.format.custom = 2:3, format.custom = \"fun\",\n           title = 'Regressionsparameter f√ºr das \"Gesamtmodell\"',\n           width = 0.5)\n\n\nRegressionsparameter f√ºr das \"Gesamtmodell\"modelbeta_0beta_1gesamt21.053-1.128"
  },
  {
    "objectID": "about/about_MLM_sose23.html",
    "href": "about/about_MLM_sose23.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Achtung üöß\n\n\n\nDies ist ein ‚Äúlebendiges‚Äù Dokument. Es ist m√∂glich, dass einige Aktuallisierungen und Erg√§nzungen nach dem ersten Seminar-Block vorgenommen werden."
  },
  {
    "objectID": "about/about_MLM_sose23.html#seminarleitung",
    "href": "about/about_MLM_sose23.html#seminarleitung",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Seminarleitung",
    "text": "Seminarleitung\nJos√© C. Garc√≠a Alanis\nAbteilung f√ºr Analyse und Modellierung komplexer Daten\nPsychologisches Institut\nJohannes Gutenberg-Universit√§t Mainz Wallstra√üe 3, Raum 06-255\nD-55122 Mainz\njose.alanis at uni-mainz.de"
  },
  {
    "objectID": "about/about_MLM_sose23.html#organisatorisches-und-wichtige-infos",
    "href": "about/about_MLM_sose23.html#organisatorisches-und-wichtige-infos",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Organisatorisches und wichtige Infos",
    "text": "Organisatorisches und wichtige Infos\n\nAllgemeine Materialien f√ºr das Seminar\n\nWichtige Informationen und Kursmaterialien werden auf die LMS/Moodle-Seite des Seminars ver√∂ffentlicht:\n\nLink zum Seminar auf LMS.\n\nWann: Montags von 12:15 - 13:45 Uhr (17.04.22 - Mo. 17.07.23).\nWo: Seminarraum 01-211 (kleiner H√∂rsaal) im Psychologischen Institut (Binger Str.)\n\n\n\n\n\nInhalt des Seminars\n\nMulti-Level Modelle haben viele Namen. H√§ufig werden sie Mehrebenen-Modelle, Mixed Effects Models oder Random Coefficient Models genannt. Eine weitere h√§ufige Bezeichnung f√ºr Multi-Level Modelle lautet Hierarchische Lineare Modelle. Dies hat zum Grund, dass Multi-Level Modelle eine statische Methode darstellen, die zur Analyse von hierarchisch strukturierten Daten (auch genestete Daten genannt) eingesetzt werden kann. Was genau versteckt sich hinter dem Begriff ‚Äûhierarchisch strukturierte Daten‚Äú und warum sind Multi-Level Modelle ein n√ºtzliches ‚ÄûTool‚Äú, um Erkenntnisse aus dieser Art von Daten zu gewinnen? Mit diesen Fragen werden wir uns im Laufe des Seminars besch√§ftigen.\n\n\n\nLernziele\n\nIn diesem Seminar werden Sie lernen, wie Sie Multi-Level Modelle zur Analyse von ‚Äûhierarchisch organisierten Daten‚Äú anwenden k√∂nnen. Ziel des Seminars ist es, Sie zu theoretisch-konzeptionellen √úberlegungen zu motivieren und Ihnen die statistisch-methodologischen Grundlagen zu vermitteln, sodass Sie in der Lage sind zu entscheiden, wann ein Multi-Level Modell zu Beschreibung von Zusammenh√§ngen, die √ºberlegene statistische Methode darstellt. Des Weiteren werden Sie Kennwerte und Methoden kennenlernen, die eigensetzt werden k√∂nnen, um die Aussagekraft und (potenziell inkrementelle) Validit√§t eines Multi-Level Modells einzusch√§tzen. Am Ende des Seminars werden Sie im Stande sein, ein eigenes Analyseprojekt durchzuf√ºhren, in dem Multi-Level Modelle zum Einsatz kommen. Diese Fragen werden Sie im Laufe des Seminars bearbeiten:\n\n\n\nWas versteht man unter ‚Äûhierarchisch strukturierten/organisierten Daten‚Äú?\n\nWie k√∂nnen diese erkannt werden?\nWelche Konsequenzen (im Sinne statistisch-methodologischer Einschr√§nkungen) bringen hierarchisch strukturierte Daten mit sich?\nWelche theoretisch-konzeptuelle √úberlegungen m√ºssen ber√ºcksichtigt werden?\n\nWie unterschieden sich Multi-Level Modelle von anderen statistischen Analyseverfahren?\n\nWas sind Gemeinsamkeiten?\n\nWie werden Multi-Level Modelle gesch√§tzt? (Was beschreiben sie?)\n\nWelche Arten von Modellen gibt es?\n\nWie k√∂nnen Multi-Level Modelle in der Programmiersprache R spezifiziert werden?\nWie sind die Ergebnisse eines Multilevel-Modells zu interpretieren?\nWie sind die Ergebnisse eines Multilevel-Modells zu bewerten?\n\nz.B. im Sinne ihrer Aussagekraft, Reliabilit√§t und Validit√§t."
  },
  {
    "objectID": "about/about_MLM_sose23.html#begleitendes-tutorium",
    "href": "about/about_MLM_sose23.html#begleitendes-tutorium",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Begleitendes Tutorium",
    "text": "Begleitendes Tutorium\n\nDas Seminar wird von einem Tutorium (Softwarekurs) begleitet. Im Tutorium werden Sie die Grundlagen des Statistiksoftware und Programmiersprache R. Sie werden in dem Umgang mit Daten geschult und werden verschiedene Techniken zur Datenaufbereitung erlernen und ein√ºben. Dieser Teil des Tutoriums findet im ersten Teil des Semesters statt. Sie sollten in eines der beiden Tutorien angemeldet sein, entweder am Montag oder am Mittwoch. Besuchen das Tutorium. Erfahrungsgem√§√ü k√∂nnen viele Fragen und Start-Schwierigkeiten im Umgang mit R im Tutorium gut aufgefangen und gel√∂st werden. Im zweiten Teil des Semesters findet ein vertiefendes Tutorium statt. Dieser baut auf die, Grundkenntnissen, die Sie im ersten Teil des Semersten erlernt haben werden. Im zweiten Teil des Semesters besuchen Sie das Tutorium bei mir. Die Termine f√ºr Tutorium bleiben wie gehabt. Sie m√ºssen das Tutorium nicht wechseln. Bitte bleiben Sie in Ihren Gruppen, ich werde die Veranstaltung von den jeweiligen Kollegen:innen √ºbernehmen und zur gewohnten Zeit anbieten. Wir eine Reihe von Materialien zusammengestellt, um Ihnen den Einstieg in die Programmiersprache R zur erleichtern. Diese k√∂nnen Sie unter dem folgenden Link erreichen:\n\n\n\nR-Kurs-Buch von der Abteilung Analyse und Modellierung komplexer Daten."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der M√∂glichkeit Datens√§tze effizient zusammenzufassen noch viele weitere Funktionen, die es erm√∂glichen tiefergehende √Ñnderungen an einem Datensatz vorzunehmen. Dazu geh√∂ren bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abh√§ngigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#fortgeschrittene-dplyr-funktionen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#fortgeschrittene-dplyr-funktionen",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der M√∂glichkeit Datens√§tze effizient zusammenzufassen noch viele weitere Funktionen, die es erm√∂glichen tiefergehende √Ñnderungen an einem Datensatz vorzunehmen. Dazu geh√∂ren bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abh√§ngigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#case_when---if-else-verkn√ºpf√ºngen-f√ºr-multiple-bedingungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#case_when---if-else-verkn√ºpf√ºngen-f√ºr-multiple-bedingungen",
    "title": "Advanced Dplyr",
    "section": "2 case_when() - if-else Verkn√ºpf√ºngen f√ºr multiple Bedingungen",
    "text": "2 case_when() - if-else Verkn√ºpf√ºngen f√ºr multiple Bedingungen\nDie case_when Funktion in dplyr erm√∂glicht es, basierend auf bestimmten Bedingungen verschiedene Werte f√ºr eine Spalte in einem Datensatz auszuw√§hlen und abh√§ngig vom Variablenwert einen neue Variable zu erstellen. Es ist eine sehr einfache M√∂glichkeit, if-else Statements in eine dplyr-Pipeline zu integrieren.\nDie Syntax eines case_when() Befehles entspricht einer einfachen if oder if-else Verkn√ºpfung. Eine if-else Verkn√ºpfung pr√ºft eine Bedingung und f√ºhrt wenn diese Erf√ºllt ist einen definierten Befehl aus. Ist die Bedingung nicht erf√ºllt, wird andernfalls (else) ein anderer Befehl ausgef√ºhrt:\n\n\n# Vektor von Zahlen darauf testen, ob sie gerade oder ungerade sind:\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei geteilt keinen Rest hat, dann schreibe Gerade, ansonsten Ungerade.\n\nifelse(zahlen %% 2 == 0,\"Gerade\",\"Ungerade\")\n\ncase_when() funktioniert nach der Gleichen Logik:\n\n\n\nDabei wird immer zuerst die if-Bedinungen und deren Output definiert und anschlie√üend mit TRUE die else Bedingung, welche angibt was passiert wenn die if - Bedingung nicht zutrifft:\n\n\n\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei\n# geteilt keinen Rest hat, dann schreibe Gerade, \n# ansonsten Ungerade.\n\ncase_when(zahlen %% 2 == 0 ~ \"Gerade\",\n          TRUE ~\"Ungerade\")\n\nDer Unterschied von case_when() und ifelse() ist, dass mit case_when() auch mehrere Bedingungen definiert werden k√∂nnen (z.B. wenn eine Variable in mehrere Kategorien eingeteilt werden soll):\n\nHier gibt es auch ein finales else Statement, dieses muss aber nicht sein, solange die Kategorien alle F√§lle abdecken!\n\n2.0.1 case_when: Beispiele\nDas folgende ist ein einfaches Beispiel, bei dem case_when() verwendet wird, um in einem Datensatz eine neue Spalte Alter_Kategorie anhand des Werts in der Spalte Alter zu kategorisieren. Wenn eine neue Spalte erstellt werden soll, muss case_when() immer mit mutate() kombiniert werden:\n\n# Beispiel Datensatz erstellen\ndf &lt;- data.frame(Name = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45))\nhead(df)\n\n# mutate und case_when verwenden, um neue Variable in Abh√§ngigkeit vom Alter zu erstellen:\ndf %&gt;% \n  mutate(Alter_Kategorie = case_when(\n    Alter &lt;= 25 ~ \"jung\",\n    Alter &gt; 25 & Alter &lt;= 35 ~ \"mittel\",\n    Alter &gt; 35 ~ \"alt\"\n  ))\n\nEin weiteres Beispiel haben wir letzte Woche bei der MPT-Modellierung der Daten von Frenken et al.¬†gesehen. Im Datensatz ist shoot als 0 und not shoot als 1 kodiert. F√ºr die MPT Modellierung ben√∂tigen wir aber f√ºr jede Kategorie (Black / Gun, Black / Phone, White / Gun, White / Phone) die Hits und Misses, also die Fehlerraten. Der Datensatz hat praktischerweise eine Spalte, die genau kodiert, was die VP gesehen hat:\n\n\nhead(Study_2_dm)\n\n# A tibble: 6 √ó 6\n  subj_idx stimulus stim     rt response condition\n     &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt; &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;    \n1        0 gun      bg    0.506        0 black    \n2        0 phone    wp    0.437        0 white    \n3        0 phone    wp    0.483        0 white    \n4        0 phone    bp    0.594        1 black    \n5        0 phone    wp    0.552        1 white    \n6        0 gun      wg    0.417        1 white    \n\n\nIn der zweiten Spalte stimulus ist angebenen, welches Objekt der jeweils zusammen mit der Hautfarbe gezeigt wurde. Da wir wissen, dass nur bei ‚Äúgun‚Äù geschossen werden darf, k√∂nnen wir nun in Kombination mit response (0 - shoot, 1 - not shoot) die Hits mit case_when() in einer neuen Spalte ACC kodieren.\nDazu Verkn√ºpfen wir hier zwei Bedingungen mit dem & Operator. Die erste Bedingung bezieht sich auf die Spalte stimulus, hier muss gepr√ºft werden, welcher Stimulus gezeigt wurde (Gun vs.¬†Phone). Die zweite Bedingung bezieht sich auf die Spalte response. Hier muss gepr√ºft werden, ob geschossen wurde oder nicht. Ingesamt m√ºssen also vier Bedinungen definiert werden, f√ºr jede Kombination von Objekt und Response:\n\nAccuracy in Abh√§ngigkeit von Gegebener Response und Stimulus\n\n\nStimulus\nResponse\nAccuracy\n\n\n\n\nGun\n0\n1\n\n\nGun\n1\n0\n\n\nPhone\n0\n0\n\n\nPhone\n1\n1\n\n\n\nDies m√ºssen wir nun in ein case_when() Befehl √ºbernehmen\n\nMit mutate() die neue Outputspalte benennen\nInnerhalb von mutate() mit case_when() die Accuracy in Abh√§ngigkeit der Spalten Stimulus und Response umkodieren:\n\n\n\nfreq_dat &lt;- Study_2_dm %&gt;% mutate(ACC = case_when(stimulus == \"gun\" & response == 0 ~ 1,\n                                                  stimulus == \"gun\" & response == 1 ~ 0,\n                                                  stimulus == \"phone\" & response == 1 ~ 1,\n                                                  stimulus ==\"phone\" & response == 0 ~ 0))\n\nAnschlie√üend m√ºssen wir nun die Hits und Misses ausz√§hlen. Generell eignen sich zum Ausz√§hlen von bestimmten Bedingungskombinationen oder Trials die Funktionen group_by(), summarise() und n(). n() ist eine einfache Z√§hlfunktion, welche innerhalb von group_by() %&gt;% summarise() dazu f√ºhrt, dass die alle Beobachtungen der gruppierten Variablen (z.B. Subject & Bedingung) innerhalb von summarise() gez√§hlt werden.\nAlso zum Beispiel, wieviele Beobachtung von Subject 1 gibt es in Bedingung A, B und C. Dies macht aber nur Sinn, wenn ihr Daten auf Trial-Ebene (also f√ºr jede Versuchsperson alle Antworten √ºber das ganez Experiment) vorliegen habt. Dies ist hier der Fall, da f√ºr jede Person und jede Bedingung, die diese Person durchlaufen hat, die gegebenen Antworten im Datensatz in der Spalte responses vorliegen.\n\n\nfreq_dat &lt;- freq_dat %&gt;% group_by(subj_idx,stim) %&gt;% \n  summarise(hits = sum(ACC), \n            ntrials=n(), \n            miss=ntrials-hits)\n\n`summarise()` has grouped output by 'subj_idx'. You can override using the\n`.groups` argument.\n\nhead(freq_dat)\n\n# A tibble: 6 √ó 5\n# Groups:   subj_idx [2]\n  subj_idx stim   hits ntrials  miss\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n1        0 bg       30      31     1\n2        0 bp       28      29     1\n3        0 wg       23      27     4\n4        0 wp       28      33     5\n5        1 bg       28      30     2\n6        1 bp       16      18     2\n\n\nWas passiert hier genau ? Schritt f√ºr Schritt:\n\ngroup_by(subj_idx, stim) : Wir gruppieren zun√§chst nach Subject und Bedingung\n\nsummarise(...\n\nhits = sum(ACC) - Wir haben alle Hits mit case_when als 1 kodiert, also k√∂nnen wir die Hits einfach berechnen, indem wir diese aufsummieren.\nnTrials = n() - Ausz√§hlen wieviele Responses ein Subject in jeder stimulus Bedinungen gegeben hat (Gesamtanzahl von gegebenen Antworten in einer Bedingungen pro Person\nmiss = ntrials - hits Alles was kein Hit ist muss ein Miss sein ! Daher k√∂nnen wir einfach die Hits von der Gesamtzahl der Antworten abziehen und erhalten die Anzahl der Misses in jeder Bedinung pro Subject !"
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "title": "Advanced Dplyr",
    "section": "3 Long- und Wide-Datenformat - Datentransformation in dplyr",
    "text": "3 Long- und Wide-Datenformat - Datentransformation in dplyr\nNun haben wir die Daten von Frenken et al.¬†neu kodiert und die n√∂tigen Informationen im Datensatz, um mit MPTinR zu arbeiten. Allerdings liegen die Daten noch im sogenannten long - Format vor. Oftmals ist es jedoch notwendig, Daten entweder von einem long in ein wide Format oder umgekehrt zu transfomieren. Im Kontext von Datenanalyse und statistischer Modellierung gibt es in der Regel zwei Haupttypen von Datenformaten: wide-Format (breit-Format) und long-Format (lang-Format).\n\n3.1 Wide-Format\n\nIn einem wide Format sind die verschiedenen Merkmale einer einzigen Beobachtung in separaten Spalten dargestellt.\nEine einzige Zeile in einem breiten Datensatz repr√§sentiert eine Beobachtung.\nBeispiel: Ein Datensatz, der Informationen √ºber die Leistung von Sch√ºlern in verschiedenen F√§chern (Mathematik, Englisch, Wissenschaft) enth√§lt, w√§re in einem breiten Format dargestellt, wobei jede Spalte einem bestimmten Fach entspricht.\nVorteil: Es ist einfach, schnelle √úbersichten √ºber gro√üe Datenmengen zu erhalten.\n\nBeispiel:\n\nwide_df &lt;- data.frame(Sch√ºler_ID = c(1, 2, 3), \n                      Mathe = c(89, 76, 92), \n                      Englisch = c(92, 88, 95), \n                      Wissenschaft = c(88, 72, 98))\nwide_df\n##   Sch√ºler_ID Mathe Englisch Wissenschaft\n## 1          1    89       92           88\n## 2          2    76       88           72\n## 3          3    92       95           98\n\n\n\n3.2 Long-Format\n\nIm long Format sind alle Merkmale einer einzigen Beobachtung in einer Zeile dargestellt.\nEine einzige Spalte in einem long Datensatz repr√§sentiert ein bestimmtes Merkmal.\nBeispiel: Ein Datensatz, der Informationen √ºber die Leistung von Sch√ºlern in verschiedenen F√§chern (Mathematik, Englisch, Wissenschaft) enth√§lt, k√∂nnte in einem langen Format dargestellt werden, wobei jede Zeile einer bestimmten Sch√ºler-Fach-Kombination entspricht.\nVorteil: Es ist einfach, bestimmte Merkmale f√ºr verschiedene Beobachtungen zu vergleichen oder zu analysieren. Au√üerdem ist es f√ºr manche statistische Methoden, wie lineare Regression oder einer ANOVA, das bevorzugte Format.\n\nBeispiel:\n\nlong_df &lt;- data.frame(Sch√ºler_ID = c(rep(1, 3), rep(2, 3), rep(3, 3)), \n                      Fach = c(rep(\"Mathe\", 3), rep(\"Englisch\", 3), rep(\"Wissenschaft\", 3)), \n                      Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\nlong_df\n##   Sch√ºler_ID         Fach Note\n## 1          1        Mathe   89\n## 2          1        Mathe   92\n## 3          1        Mathe   88\n## 4          2     Englisch   76\n## 5          2     Englisch   88\n## 6          2     Englisch   72\n## 7          3 Wissenschaft   92\n## 8          3 Wissenschaft   95\n## 9          3 Wissenschaft   98\n\nEs ist wichtig, das Konzept des wide- und long-Formats zu verstehen, da es bei der Datenaufbereitung und Analyse eine wichtige Rolle spielt. Zum Beispiel kann ein wide -Datensatz schwer zu analysieren sein, wenn man Vergleiche zwischen bestimmten Merkmalen √ºber mehrere Beobachtungen hinweg machen m√∂chte (Beispiel Ergebnisse der Diffusionsmodellierung!). Hier ist es oft besser, den Datensatz in ein long-format zu bringen.\n\n\n3.3 pivot - Funktionen in dplyr\nDie Funktionen pivot_wider und pivot_longer geh√∂ren zu den Funktionen von dplyr und dienen dazu, Datens√§tze zu transformieren.\npivot_wider verwandelt einen long-Format Datensatz in einen wide-Format Datensatz, indem es die Werte einer bestimmten Spalte zu neuen Spalten umbenennt. Dies kann manuell durchgef√ºhrt werden, aber pivot_wider macht dies automatisch und erleichtert so die Datentransformation:\n\n\npivot_longer verwandelt einen wide-Format Datensatz in einen long-Format Datensatz, indem es die Spalten mit bestimmten Werten in einer neuen Spalte zusammenfasst.\nEin Beispiel f√ºr den Einsatz von pivot_wider:\n\n# long data example\nlong_data &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\", \"John\", \"Jane\", \"Jim\"),\n  subject = c(\"physics\", \"physics\", \"physics\", \"math\", \"math\", \"math\"),\n  score = c(85, 90, 80, 75, 80, 70)\n)\n\nlong_data\n## # A tibble: 6 √ó 3\n##   name  subject score\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 John  physics    85\n## 2 Jane  physics    90\n## 3 Jim   physics    80\n## 4 John  math       75\n## 5 Jane  math       80\n## 6 Jim   math       70\n\n\n# pivot to wide format\npivot_wider(long_data, names_from = subject, values_from = score)\n## # A tibble: 3 √ó 3\n##   name  physics  math\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n## 1 John       85    75\n## 2 Jane       90    80\n## 3 Jim        80    70\n\nEin Beispiel f√ºr den Einsatz von pivot_longer:\n\n# wide data example\nwide_df &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\"),\n  Mathe_Note = c(85, 90, 80),\n  Englisch_Note = c(75, 80, 70)\n)\n\n\nwide_df\n## # A tibble: 3 √ó 3\n##   name  Mathe_Note Englisch_Note\n##   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n## 1 John          85            75\n## 2 Jane          90            80\n## 3 Jim           80            70\n\n\n# pivot to long format\npivot_longer(wide_df, cols = c(Mathe_Note, Englisch_Note), \n             names_to = \"Sch√ºler\", values_to = \"Note\")\n## # A tibble: 6 √ó 3\n##   name  Sch√ºler        Note\n##   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n## 1 John  Mathe_Note       85\n## 2 John  Englisch_Note    75\n## 3 Jane  Mathe_Note       90\n## 4 Jane  Englisch_Note    80\n## 5 Jim   Mathe_Note       80\n## 6 Jim   Englisch_Note    70\n\n\n\n3.4 Argumente f√ºr pivot_longer und pivot_wider\npivot_longer ben√∂tigt mindestens zwei Argumente:\n\ncols: Dies ist ein Zeichenvektor, der angibt, welche Spalten im Datensatz zusammengefasst werden sollen. -Sch√ºler_ID bedeutet zum Beispiel, dass alle Spalten au√üer Sch√ºler_ID zusammengefasst werden sollen.\nnames_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die zusammengefassten Werte gespeichert werden.\nvalues_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die Werte gespeichert werden, die aus den zusammengefassten Spalten stammen.\n\n\npivot_wider ben√∂tigt mindestens zwei Argumente:\n\nnames_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Namen f√ºr die neuen Spalten verwendet werden soll.\nvalues_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Werte f√ºr die neuen Spalten verwendet werden soll.\n\n\nBeide Funktionen haben auch weitere optionale Argumente wie values_fill und names_prefix oder names_sep()um die Daten bei Bedarf weiter anzupassen.\nHier nun die Umformung der Daten von Frenken et al., welche wir vom long Format in das wide Format bringen m√ºssen:\n\n\n#| echo: true\n#| output: true\n#| warning: false\n#| code-overflow: wrap\n#| collapse: true\n\nfreq_dat %&gt;% pivot_wider(names_from = c(\"stim\"),\n                         values_from = c(\"hits\",\"miss\"),\n                         id_cols=\"subj_idx\")\n\n# A tibble: 137 √ó 9\n# Groups:   subj_idx [137]\n   subj_idx hits_bg hits_bp hits_wg hits_wp miss_bg miss_bp miss_wg miss_wp\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1        0      30      28      23      28       1       1       4       5\n 2        1      28      16      27      11       2       2       2       1\n 3        3      25      10      33      23       0       0       1       1\n 4        4      35      24      27      33       1       0       0       0\n 5        5      31      28      25      23       1       2       4       4\n 6        6      34      26      18      31       0       0       5       0\n 7        7      29      24      21      36       2       0       1       1\n 8        8      22      29      25      28       0       2      10       2\n 9        9      28      23      22      22       2       0       2       1\n10       10      26      35      25      24       4       0       0       3\n# ‚Ñπ 127 more rows\n\n\nWas passiert hier genau ? Schritt f√ºr Schritt:\n\nnames_from = c(stim) - Die neuen Spalten sollen aus der Spalte stim benannt werden - (bg,bp,wp,wg)\nvalues_from = c(\"hits\",\"miss\") - in den neuen Spalten sollen die Werte der Hits und Miss Spalten stehen - durch den ersten und zweiten Schritt entstehen also Spalten, die jeweils hits_bg etc. enthalten. Also die Hits aus der Bedingung ‚Äúbg‚Äù usw.\nid_cols - dies soll f√ºr jedes Subject einzeln geschehen."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#√ºbungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#√ºbungen",
    "title": "Advanced Dplyr",
    "section": "4 √úbungen",
    "text": "4 √úbungen\n\n4.1 case_when\n1.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung ‚ÄúNote_Kategorie‚Äù zu erstellen, die ‚ÄúSehr gut‚Äù f√ºr Noten √ºber 90, ‚ÄúGut‚Äù f√ºr Noten zwischen 80 und 90 und ‚ÄúSchlecht‚Äù f√ºr Noten unter 80 angibt.\n\ndf &lt;- data.frame(Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\n# Your Code Here\n\n2.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung ‚ÄúBestanden‚Äù zu erstellen, um zu Pr√ºfen ob ein Sch√ºler einer bestimmten Schulform eine Pr√ºfung bestanden hat. Die Bestehensgrenzen sind wie folgt.\n-   F√ºr die Hauptschule liegt die Bestehensgrenze bei 50 %\n-   F√ºr die Realschule liegt die Bestehensgrenze bei 60 %\n-   F√ºr das Gymnasium liegt die Bestehensgrenze bei 70 %\nKodieren Sie das bestehen entweder mit ‚ÄúPass‚Äù oder ‚ÄúFail‚Äù.\n\ndf &lt;- data.frame(Note = runif(100,min=0, max=100),\n                 Schulform = sample(c(\"Gymnasium\",\"Realschule\", \"Hauptschule\"),\n                                    size = 100, replace = T))\n# Your Code here\n\n3.) Nutzen Sie die Funktion case_when und die dplyr-Library in R, um eine neue Spalte in dem Datensatz ‚Äúdf‚Äù zu erstellen, die die Einkommenskategorie jeder Person basierend auf ihrem Berufsstatus und ihrem Einkommen kategorisiert. Die Einkommenskategorien sollten wie folgt sein:\n\nF√ºr Angestellte mit einem Einkommen von bis zu 50.000: ‚Äúniedrig‚Äù\nF√ºr Angestellte mit einem Einkommen zwischen 50.000 und 75.000: ‚Äúmittel‚Äù\nF√ºr Angestellte mit einem Einkommen √ºber 75.000: ‚Äúhoch‚Äù\nF√ºr Freiberufler mit einem Einkommen von bis zu 60.000: ‚Äúniedrig‚Äù\nF√ºr Freiberufler mit einem Einkommen zwischen 60.000 und 100.000: ‚Äúmittel‚Äù\nF√ºr Freiberufler mit einem Einkommen √ºber 100.000: ‚Äúhoch‚Äù\nF√ºr Ruhest√§ndler mit einem Einkommen von bis zu 30.000: ‚Äúniedrig‚Äù\nF√ºr Ruhest√§ndler mit einem Einkommen √ºber 30.000: ‚Äúmittel_hoch‚Äù\n\n\ndf &lt;- data.frame(ID = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45),\n                 Berufsstatus = c(\"Angestellter\", \"Freiberufler\", \"Ruhest√§ndler\"),\n                 Einkommen = c(45000, 75000, 32000))\n\n\n\n4.2 Pivotting\n1.) Konvertieren Sie die folgenden Datensatz von wide to long. Erstellen Sie aus den Spalten zwei neue Spalten mit den Namen (month und index). Nutzen Sie dazu das Argument names_sep = \"_\". Die Werte sollen in die Spalte ‚ÄúN‚Äù geschrieben werden.\nTip: Sie m√ºssen bei names_to einen Vektor mit den Namen der neuen Spalten angeben.\n\ndf_wide &lt;- data.frame(\n  Jan_sales = c(10, 40, 70),\n  Feb_sales = c(20, 50, 80),\n  Mar_sales = c(30, 60, 90)\n)\n\n# Your Code here\n\n2.) Konvertieren Sie die folgendn Datensatz von long in wide.\n\n    df_long &lt;- data.frame(\n      ID = c(\"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\"),\n      month = c(\"Jan\", \"Jan\", \"Jan\", \"Feb\", \"Feb\", \"Feb\", \"Mar\", \"Mar\", \"Mar\"),\n      sales = c(10, 40, 70, 20, 50, 80, 30, 60, 90))\n      \n\n    # Your Code Here"
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der M√∂glichkeit Datens√§tze effizient zusammenzufassen noch viele weitere Funktionen, die es erm√∂glichen tiefergehende √Ñnderungen an einem Datensatz vorzunehmen. Dazu geh√∂ren bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). case_when() kann dazu genutzt werden, Variablen in Abh√§ngigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#fortgeschrittene-dplyr-funktionen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#fortgeschrittene-dplyr-funktionen",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der M√∂glichkeit Datens√§tze effizient zusammenzufassen noch viele weitere Funktionen, die es erm√∂glichen tiefergehende √Ñnderungen an einem Datensatz vorzunehmen. Dazu geh√∂ren bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). case_when() kann dazu genutzt werden, Variablen in Abh√§ngigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#case_when---if-else-verkn√ºpf√ºngen-f√ºr-multiple-bedingungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#case_when---if-else-verkn√ºpf√ºngen-f√ºr-multiple-bedingungen",
    "title": "Advanced Dplyr",
    "section": "2 case_when() - if-else Verkn√ºpf√ºngen f√ºr multiple Bedingungen",
    "text": "2 case_when() - if-else Verkn√ºpf√ºngen f√ºr multiple Bedingungen\nDie case_when Funktion in dplyr erm√∂glicht es, basierend auf bestimmten Bedingungen verschiedene Werte f√ºr eine Spalte in einem Datensatz auszuw√§hlen und abh√§ngig vom Variablenwert einen neue Variable zu erstellen. Es ist eine sehr einfache M√∂glichkeit, if-else Statements in eine dplyr-Pipeline zu integrieren.\nDie Syntax eines case_when() Befehles entspricht einer einfachen if oder if-else Verkn√ºpfung. Eine if-else Verkn√ºpfung pr√ºft eine Bedingung und f√ºhrt wenn diese Erf√ºllt ist einen definierten Befehl aus. Ist die Bedingung nicht erf√ºllt, wird andernfalls (else) ein anderer Befehl ausgef√ºhrt:\n\n\n\nCode\n# Vektor von Zahlen darauf testen, ob sie gerade oder ungerade sind:\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei geteilt keinen Rest hat, dann schreibe Gerade, ansonsten Ungerade.\n\nifelse(zahlen %% 2 == 0,\"Gerade\",\"Ungerade\")\n\n\ncase_when() funktioniert nach der Gleichen Logik:\n\n\n\nDabei wird immer zuerst die if-Bedinungen und deren Output definiert und anschlie√üend mit TRUE die else Bedingung, welche angibt was passiert wenn die if - Bedingung nicht zutrifft:\n\n\n\n\nCode\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei\n# geteilt keinen Rest hat, dann schreibe Gerade, \n# ansonsten Ungerade.\n\ncase_when(zahlen %% 2 == 0 ~ \"Gerade\",\n          TRUE ~\"Ungerade\")\n\n\nDer Unterschied von case_when() und ifelse() ist, dass mit case_when() auch mehrere Bedingungen definiert werden k√∂nnen (z.B. wenn eine Variable in mehrere Kategorien eingeteilt werden soll):\n\nHier gibt es auch ein finales else Statement, dieses muss aber nicht sein, solange die Kategorien alle F√§lle abdecken!\n\n2.0.1 case_when(): Beispiele\nDas folgende ist ein einfaches Beispiel, bei dem case_when() verwendet wird, um in einem Datensatz eine neue Spalte Alter_Kategorie anhand des Werts in der Spalte Alter zu kategorisieren. Wenn eine neue Spalte erstellt werden soll, muss case_when() immer mit mutate() kombiniert werden:\n\n\nCode\n# Beispiel Datensatz erstellen\ndf &lt;- data.frame(Name = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45))\nhead(df)\n\n# mutate und case_when verwenden, um neue Variable in Abh√§ngigkeit vom Alter zu erstellen:\ndf %&gt;% \n  mutate(Alter_Kategorie = case_when(\n    Alter &lt;= 25 ~ \"jung\",\n    Alter &gt; 25 & Alter &lt;= 35 ~ \"mittel\",\n    Alter &gt; 35 ~ \"alt\"\n  ))"
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "title": "Advanced Dplyr",
    "section": "3 Long- und Wide-Datenformat - Datentransformation in dplyr",
    "text": "3 Long- und Wide-Datenformat - Datentransformation in dplyr\nNun haben wir die Daten von Frenken et al.¬†neu kodiert und die n√∂tigen Informationen im Datensatz, um mit MPTinR zu arbeiten. Allerdings liegen die Daten noch im sogenannten long - Format vor. Oftmals ist es jedoch notwendig, Daten entweder von einem long in ein wide Format oder umgekehrt zu transfomieren. Im Kontext von Datenanalyse und statistischer Modellierung gibt es in der Regel zwei Haupttypen von Datenformaten: wide-Format (breit-Format) und long-Format (lang-Format).\n\n3.1 Wide-Format\n\nIn einem wide Format sind die verschiedenen Merkmale einer einzigen Beobachtung in separaten Spalten dargestellt.\nEine einzige Zeile in einem breiten Datensatz repr√§sentiert eine Beobachtung.\nBeispiel: Ein Datensatz, der Informationen √ºber die Leistung von Sch√ºlern in verschiedenen F√§chern (Mathematik, Englisch, Wissenschaft) enth√§lt, w√§re in einem breiten Format dargestellt, wobei jede Spalte einem bestimmten Fach entspricht.\nVorteil: Es ist einfach, schnelle √úbersichten √ºber gro√üe Datenmengen zu erhalten.\n\nBeispiel:\n\n\nCode\nwide_df &lt;- data.frame(Sch√ºler_ID = c(1, 2, 3), \n                      Mathe = c(89, 76, 92), \n                      Englisch = c(92, 88, 95), \n                      Wissenschaft = c(88, 72, 98))\nwide_df\n##   Sch√ºler_ID Mathe Englisch Wissenschaft\n## 1          1    89       92           88\n## 2          2    76       88           72\n## 3          3    92       95           98\n\n\n\n\n3.2 Long-Format\n\nIm long Format sind alle Merkmale einer einzigen Beobachtung in einer Zeile dargestellt.\nEine einzige Spalte in einem long Datensatz repr√§sentiert ein bestimmtes Merkmal.\nBeispiel: Ein Datensatz, der Informationen √ºber die Leistung von Sch√ºlern in verschiedenen F√§chern (Mathematik, Englisch, Wissenschaft) enth√§lt, k√∂nnte in einem langen Format dargestellt werden, wobei jede Zeile einer bestimmten Sch√ºler-Fach-Kombination entspricht.\nVorteil: Es ist einfach, bestimmte Merkmale f√ºr verschiedene Beobachtungen zu vergleichen oder zu analysieren. Au√üerdem ist es f√ºr manche statistische Methoden, wie lineare Regression oder einer ANOVA, das bevorzugte Format.\n\nBeispiel:\n\n\nCode\nlong_df &lt;- data.frame(Sch√ºler_ID = c(rep(1, 3), rep(2, 3), rep(3, 3)), \n                      Fach = c(rep(\"Mathe\", 3), rep(\"Englisch\", 3), rep(\"Wissenschaft\", 3)), \n                      Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\nlong_df\n##   Sch√ºler_ID         Fach Note\n## 1          1        Mathe   89\n## 2          1        Mathe   92\n## 3          1        Mathe   88\n## 4          2     Englisch   76\n## 5          2     Englisch   88\n## 6          2     Englisch   72\n## 7          3 Wissenschaft   92\n## 8          3 Wissenschaft   95\n## 9          3 Wissenschaft   98\n\n\nEs ist wichtig, das Konzept des wide- und long-Formats zu verstehen, da es bei der Datenaufbereitung und Analyse eine wichtige Rolle spielt. Zum Beispiel kann ein wide -Datensatz schwer zu analysieren sein, wenn man Vergleiche zwischen bestimmten Merkmalen √ºber mehrere Beobachtungen hinweg machen m√∂chte (z.b. bei Varianzanalysen). Hier ist es oft besser, den Datensatz in ein long-Format zu bringen.Umgekehrt kann es der Fall sein, das eine bestimmte Analyseform oder Modellierung die Daten im wide-Format\n\n\n3.3 pivot - Funktionen in dplyr\nDie Funktionen pivot_wider und pivot_longer geh√∂ren zu den Funktionen von dplyr und dienen dazu, Datens√§tze zu transformieren.\n\npivot_wider verwandelt einen long-Format Datensatz in einen wide-Format Datensatz, indem es die Werte einer bestimmten Spalte zu neuen Spalten umbenennt. Dies kann manuell durchgef√ºhrt werden, aber pivot_wider macht dies automatisch und erleichtert so die Datentransformation:\npivot_longer verwandelt einen wide-Format Datensatz in einen long-Format Datensatz, indem es die Spalten mit bestimmten Werten in einer neuen Spalte zusammenfasst.\n\nEin Beispiel f√ºr den Einsatz von pivot_wider:\n\n\nCode\n# long data example\nlong_data &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\", \"John\", \"Jane\", \"Jim\"),\n  subject = c(\"physics\", \"physics\", \"physics\", \"math\", \"math\", \"math\"),\n  score = c(85, 90, 80, 75, 80, 70)\n)\n\nlong_data\n## # A tibble: 6 √ó 3\n##   name  subject score\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 John  physics    85\n## 2 Jane  physics    90\n## 3 Jim   physics    80\n## 4 John  math       75\n## 5 Jane  math       80\n## 6 Jim   math       70\n\n\n# pivot to wide format\npivot_wider(long_data, names_from = subject, values_from = score)\n## # A tibble: 3 √ó 3\n##   name  physics  math\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n## 1 John       85    75\n## 2 Jane       90    80\n## 3 Jim        80    70\n\n\nEin Beispiel f√ºr den Einsatz von pivot_longer:\n\n\nCode\n# wide data example\nwide_df &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\"),\n  Mathe_Note = c(85, 90, 80),\n  Englisch_Note = c(75, 80, 70)\n)\n\n\nwide_df\n## # A tibble: 3 √ó 3\n##   name  Mathe_Note Englisch_Note\n##   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n## 1 John          85            75\n## 2 Jane          90            80\n## 3 Jim           80            70\n\n\n# pivot to long format\npivot_longer(wide_df, cols = c(Mathe_Note, Englisch_Note), \n             names_to = \"Sch√ºler\", values_to = \"Note\")\n## # A tibble: 6 √ó 3\n##   name  Sch√ºler        Note\n##   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n## 1 John  Mathe_Note       85\n## 2 John  Englisch_Note    75\n## 3 Jane  Mathe_Note       90\n## 4 Jane  Englisch_Note    80\n## 5 Jim   Mathe_Note       80\n## 6 Jim   Englisch_Note    70\n\n\n\n\n3.4 Argumente f√ºr pivot_longer und pivot_wider\npivot_longer ben√∂tigt mindestens zwei Argumente:\n\ncols: Dies ist ein Zeichenvektor, der angibt, welche Spalten im Datensatz zusammengefasst werden sollen. -Sch√ºler_ID bedeutet zum Beispiel, dass alle Spalten au√üer Sch√ºler_ID zusammengefasst werden sollen.\nnames_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die zusammengefassten Werte gespeichert werden.\nvalues_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die Werte gespeichert werden, die aus den zusammengefassten Spalten stammen.\n\n\npivot_wider ben√∂tigt mindestens zwei Argumente:\n\nnames_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Namen f√ºr die neuen Spalten verwendet werden soll.\nvalues_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Werte f√ºr die neuen Spalten verwendet werden soll.\n\n\nBeide Funktionen haben auch weitere optionale Argumente wie values_fill und names_prefix oder names_sep()um die Daten bei Bedarf weiter anzupassen."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#praktisches-beispiel",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#praktisches-beispiel",
    "title": "Advanced Dplyr",
    "section": "4 Praktisches Beispiel",
    "text": "4 Praktisches Beispiel\nIm n√§chstes Beipsiel werden wir einen Datensatz umformen, mit dem wir uns im n√§chsten Workshop zu MPT-Modellen besch√§ftigen werden. Der Datensatz enth√§lt f√ºnf Spalten, die die Daten aus einem Priming-Experiment codieren. Insgesamt hat jede Versuchsperson 4 Bedingungen durchlaufen, die in der Spalte stim codiert sind (bg,bp,wg,wp). In jeder dieser Bedingungen mussten die VP eine bin√§re Entscheidungsaufgabe bearbeiten. in den Spalten hits und miss ist codiert, ob die VP die richtige oder falsche Entscheidung getroffen haben. Die Spalte ntrials gibt die Gesamtzahl der Versuche einer Person pro Bedingung an.\n\n\nCode\nhead(freq_dat,5)\n\n\n# A tibble: 5 √ó 5\n# Groups:   subj_idx [2]\n  subj_idx stim   hits ntrials  miss\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n1        0 bg       30      31     1\n2        0 bp       28      29     1\n3        0 wg       23      27     4\n4        0 wp       28      33     5\n5        1 bg       28      30     2\n\n\nUm in der Praxis mit dem Datensatz arbeiten zu k√∂nnen, m√ºssen wir diesen jedoch zun√§chst in das richtige Format bringen. Der Datensatz liegt momentan im long-Format vor, f√ºr bestimmte Analysen muss dieser jedoch in das wide-Format umgewandelt werden. Das f√ºhrt dazu, dass es f√ºr jede Bedingung je eine Spalte f√ºr hits und misses geben wird, also z.B. hits_bg und misses_bg.\nNach der Umformung, sieht der Datensatz dann wie folgt aus:\n\n\nCode\n#| echo: true\n#| output: true\n#| warning: false\n#| code-overflow: wrap\n#| collapse: true\n\nhead(freq_dat %&gt;% pivot_wider(names_from = c(\"stim\"),\n                         values_from = c(\"hits\",\"miss\"),\n                         id_cols=\"subj_idx\"),5)\n\n\n# A tibble: 5 √ó 9\n# Groups:   subj_idx [5]\n  subj_idx hits_bg hits_bp hits_wg hits_wp miss_bg miss_bp miss_wg miss_wp\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1        0      30      28      23      28       1       1       4       5\n2        1      28      16      27      11       2       2       2       1\n3        3      25      10      33      23       0       0       1       1\n4        4      35      24      27      33       1       0       0       0\n5        5      31      28      25      23       1       2       4       4\n\n\nWas passiert hier genau ? Schritt f√ºr Schritt:\n\nnames_from = c(stim) - Die neuen Spalten sollen aus der Spalte stim benannt werden - (bg,bp,wp,wg)\nvalues_from = c(\"hits\",\"miss\") - in den neuen Spalten sollen die Werte der Hits und Miss Spalten stehen - durch den ersten und zweiten Schritt entstehen also Spalten, die jeweils hits_bg etc. enthalten. Also die Hits aus der Bedingung ‚Äúbg‚Äù usw.\nid_cols - dies soll f√ºr jedes Subject einzeln geschehen."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#√ºbungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#√ºbungen",
    "title": "Advanced Dplyr",
    "section": "5 √úbungen",
    "text": "5 √úbungen\n\n5.1 case_when()\n1.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung ‚ÄúNote_Kategorie‚Äù zu erstellen, die ‚ÄúSehr gut‚Äù f√ºr Noten √ºber 90, ‚ÄúGut‚Äù f√ºr Noten zwischen 80 und 90 und ‚ÄúSchlecht‚Äù f√ºr Noten unter 80 angibt.\n\n\nCode\ndf &lt;- data.frame(Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\n# Your Code Here\n\n\n2.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung ‚ÄúBestanden‚Äù zu erstellen, um zu Pr√ºfen ob ein Sch√ºler einer bestimmten Schulform eine Pr√ºfung bestanden hat. Die Bestehensgrenzen sind wie folgt.\n-   F√ºr die Hauptschule liegt die Bestehensgrenze bei 50 %\n-   F√ºr die Realschule liegt die Bestehensgrenze bei 60 %\n-   F√ºr das Gymnasium liegt die Bestehensgrenze bei 70 %\nKodieren Sie das bestehen entweder mit ‚ÄúPass‚Äù oder ‚ÄúFail‚Äù.\n\n\nCode\ndf &lt;- data.frame(Note = runif(100,min=0, max=100),\n                 Schulform = sample(c(\"Gymnasium\",\"Realschule\", \"Hauptschule\"),\n                                    size = 100, replace = T))\n# Your Code here\n\n\n3.) Nutzen Sie die Funktion case_when und die dplyr-Library in R, um eine neue Spalte in dem Datensatz ‚Äúdf‚Äù zu erstellen, die die Einkommenskategorie jeder Person basierend auf ihrem Berufsstatus und ihrem Einkommen kategorisiert. Die Einkommenskategorien sollten wie folgt sein:\n\nF√ºr Angestellte mit einem Einkommen von bis zu 50.000: ‚Äúniedrig‚Äù\nF√ºr Angestellte mit einem Einkommen zwischen 50.000 und 75.000: ‚Äúmittel‚Äù\nF√ºr Angestellte mit einem Einkommen √ºber 75.000: ‚Äúhoch‚Äù\nF√ºr Freiberufler mit einem Einkommen von bis zu 60.000: ‚Äúniedrig‚Äù\nF√ºr Freiberufler mit einem Einkommen zwischen 60.000 und 100.000: ‚Äúmittel‚Äù\nF√ºr Freiberufler mit einem Einkommen √ºber 100.000: ‚Äúhoch‚Äù\nF√ºr Ruhest√§ndler mit einem Einkommen von bis zu 30.000: ‚Äúniedrig‚Äù\nF√ºr Ruhest√§ndler mit einem Einkommen √ºber 30.000: ‚Äúmittel_hoch‚Äù\n\n\n\nCode\ndf &lt;- data.frame(ID = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45),\n                 Berufsstatus = c(\"Angestellter\", \"Freiberufler\", \"Ruhest√§ndler\"),\n                 Einkommen = c(45000, 75000, 32000))\n\n\n# Your Code here\n\n\n\n\n5.2 Pivotting\n1.) Konvertieren Sie die folgenden Datensatz von wide to long. Erstellen Sie aus den Spalten zwei neue Spalten mit den Namen (month und index). Nutzen Sie dazu das Argument names_sep = \"_\". Die Werte sollen in die Spalte ‚ÄúN‚Äù geschrieben werden.\nTip: Sie m√ºssen bei names_to einen Vektor mit den Namen der neuen Spalten angeben.\n\n\nCode\ndf_wide &lt;- data.frame(\n  Jan_sales = c(10, 40, 70),\n  Feb_sales = c(20, 50, 80),\n  Mar_sales = c(30, 60, 90)\n)\n\n# Your Code here\n\n\n2.) Konvertieren Sie die folgendn Datensatz von long in wide.\n\n\nCode\n    df_long &lt;- data.frame(\n      ID = c(\"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\"),\n      month = c(\"Jan\", \"Jan\", \"Jan\", \"Feb\", \"Feb\", \"Feb\", \"Mar\", \"Mar\", \"Mar\"),\n      sales = c(10, 40, 70, 20, 50, 80, 30, 60, 90))\n      \n\n    # Your Code Here"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin2.html#why-models",
    "href": "cogmod_sem_sose23/Termin2.html#why-models",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Why Models?",
    "text": "Why Models?\nAls Psychologen wollen wir menschliches Verhalten beschreiben, vorhersagen und letztendlich erkl√§ren. Wir unterscheiden uns damit nicht wesentlich von einem Physiker, der Ph√§nomene der Welt erkl√§ren m√∂chte. Allerdings m√∂chten Psychologen kognitive Ph√§nomene erkl√§ren wie zum Beispiel\n\nGed√§chtnisprozesse\nSprachverarbeitung\nGedankenabschweifen\n\nund noch viele weitere kognitive Prozesse. Allerdings ist es schwierig nur anhand von Daten, wie zum Beispiel Reaktionszeiten und Fehlerraten, und verbalen Theorien kognitive Prozesse zu verstehen."
  },
  {
    "objectID": "cogmod_sem_sose23/Termin2.html#studienleistung",
    "href": "cogmod_sem_sose23/Termin2.html#studienleistung",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Studienleistung",
    "text": "Studienleistung\nDie Studienleistung f√ºr das Seminar besteht in der w√∂chentlichen Abgabe von Aufgaben, die sich auf den behandelten Stoff beziehen. Die Aufgaben werden √ºber Assignments in Teams abgegeben und sind jeweils bis Montags 12:00 Uhr vor dem jeweilig n√§chsten Seminar f√§llig. Am Ende des Semesters wird dann ein Portfolio mit allen Aufgaben abgegeben, was die Gesamtstudienleistung darstellt. Es m√ºssen zum bestehen alle Aufgaben eingereicht werden !\nDie Aufgaben werden mit fortschreitendem Semester anspruchsvoller und k√∂nnen folgendes umfassen:\n\nFragen zum Stoff\nProgrammieraufgaben in R\nDatenanalyse und Modellierung mit R"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin2.html#teams-channel",
    "href": "cogmod_sem_sose23/Termin2.html#teams-channel",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Teams Channel ",
    "text": "Teams Channel \nWir werden ausschlie√ülich mit Teams und dieser Website arbeiten ! Alle wichtigen Ank√ºndigungen werden im Teams Channel gemacht, daher ist es wichtig regelem√§√üig dort hineinzuschauen. Auf dieser Website werden kurze Zusammenfassungen und weitere Materialien (Slides, Literatur, Skripte und Videos) f√ºr die Seminareinheiten zur Verf√ºgung gestellt, sodass ihr alles aus einer Hand nacharbeiten k√∂nnt.\n\nFahrplan\n\n\nFahrplan\n\n\n\nKW\nThema Seminar\nLink\nMaterialien\n\n\n\n\n16\nOrganisation und Ablauf\nZusammenfassung\n\n\n\n17\nEinf√ºhrung I: Grundlagen der Modellierung\n\n \n\n\n19\nEinf√ºhrung II: Grundlagen der Modellierung\n\n  \n\n\n20\nParametersch√§tzung I: Diskrepanzfunktionen & Sch√§tzalgorithmen\n\n  \n\n\n21\nParametersch√§tzung II: Maximum Likelihood & Beyond\n\n\n\n\n23\nParametersch√§tzung III: Hands On in R Parameter Estimation\n\n\n\n\n24\nAdvanced R for Cognitive Modeling\n\n\n\n\n25\nMultinomial Processing Tree Models (Theorie)\n\n\n\n\n26\nAnwendung von MPT Modellen (R-Sitzung)\n\n\n\n\n27\nDrift Diffusion Models (Theory)\n\n\n\n\n28\nMemory Measurement Model (M3)\n\n\n\n\n29\nAnwendung des M3 Modells\n\n\n\n\n30\nAbgabe des kompletten Portfolios"
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Auf der nachfolgenden Seite findet ihr den kompletten Workshop zur MPT Modellierung. Zudem das Tutorial als Video in dem alles Schritt f√ºr Schritt durchgegangen wird! Da das Video aus dem letzten Semester ist, es kann kleinere Abweichungen geben !\n\n\n\nHeute Besch√§ftigen wir uns mit der Anwendung von MPT Modellen in R. Hierzu nutzen wir einen Datensatz von Frenken et al.¬†(2022), der sich mit der Verarbeitung von Stereotypen befasst. Zun√§chst m√ºssen wir aber die entsprechenden Packages installieren. Wir nutzen das Package (MPTinR), das viele Funktionalit√§ten f√ºr das MPT Modelling mit sich bringt ! Zus√§tzlich installieren wir noch ggpubr und effectsize um sp√§ter einige nette Features beim Plotten und Analysieren der Ergebnisse zu haben:\n\n\nCode\n# Install MPT Package\n# install.packages(\"MPTinR\")\n# install.packages(\"ggpubr\")\n# install.packages(\"effectsize\")\n\n\nNun laden wir die Pakete die wir f√ºr den weiteren Verlauf brauchen werden sowie unseren Datensatz ein:\n\n\nCode\nlibrary(MPTinR)\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(effectsize)\n\n\nStudy_2_dm &lt;- read_csv(\"Data/Study_2_dm.csv\", \n                       col_types = cols(...1 = col_skip(), \n                                        stimulus = col_factor(levels = c(\"gun\",\"phone\")), \n                                        response = col_integer(), \n                                        condition = col_factor(levels = c(\"black\",\"white\"))))\n\n\n\n\n\nWie schon in der letzten Sitzung vorgestellt handelt es sich beim beim First Person Shooter Task (FPST) um ein Paradigma, mit dem Stereotype und deren Auswirkungen auf Entscheidungen untersucht werden k√∂nnen. Hierbei werden √ºblicherweise verschiedene Ethnien (z.B. schwarze oder wei√üe Personen), entweder mit einer Waffe (threat) oder einem ungef√§hrlichem Objekt (z.B. Telefon, harmless) gezeigt. Die Versuchspersonen werden instruiert unabh√§ngig von der Hautdfarbe so schnell und korrekt wie m√∂glich auf bewaffnete Ziele zu schiessen. Im Gegensatz dazu soll nicht auf unbewaffnete Ziele geschossen werden.\n\n\n\nVerschiedene Paradigmen des FPST\n\n\n\n\n\nWir werden heute versuchen, die Daten von Frenken et al. mit dem Process Dissociation Model zu fitten. Dieses m√ºssen wir aber zun√§chst in R definieren. Die Modellgleichungen leiten sich aus folgendem zugrundeliegendem Wahrscheinlichkeitsbaum ab:\n\n\n\nProcess Dissociation Model (Payne,2001)\n\n\nDas Modell hat nur zwei freie Parameter, zum einen c , der die Wahrscheinlichkeit angibt das kontrollierte Verarbeitungsprozesse aktiviert werden und zum anderen a der die Wahrscheinlichkeit angibt, dass diese Kontrolle fehlschl√§gt und eine stereotype Verarbeitung aktiviert wird.\n\nZur Erinnerung: das experimentelle Paradigma in Payne et al.¬†ist ein Sequential Priming Task, bei dem die Versuchspersonen ein schwarzes oder wei√ües Gesicht gesehen haben und dann entscheiden sollten, ob es sich bei dem anschlie√üend gezeigten Objekt um einen Waffe oder ein Werkzeug handelt:\n\n\nIn unserem Fall ist das Paradigma also √§hnlich, die Versuchspersonen sehen entweder einen Schwarzen oder einen Wei√üen (unbewaffnet vs.¬†bewaffnet) und sollen dann entscheiden, ob geschossen werden soll oder nicht. Das heisst, wir k√∂nnen das Modell im Prinzip ekaxt so wie es ist, auf die Daten von Frenken et al. √ºbertragen.\n\nNun m√ºssen wir das Modell allerdings definieren. Hierzu m√ºssen die Gleichungen f√ºr jeden Wahrscheinlichkeitsbaum aufgestellt werden. Jeder Baum repr√§sentiert eine Zielkategorie:\n\n\n\n\nHautfarbe\nObjekt\n\n\n\n\nSchwarz\nWaffe\n\n\nWei√ü\nWaffe\n\n\nSchwarz\nTelefon\n\n\nWei√ü\nTelefon\n\n\n\nEs m√ºssen nun f√ºr jede dieser Outcome Kategorien Gleichungen f√ºr die unterschiedlichen Pfade definiert werden, welche zu den jeweiligen ‚ÄúHits‚Äù und ‚ÄúMisses‚Äù f√ºhren (‚Äú+‚Äù bzw. ‚Äú-‚Äùauf der Abbildung). Zus√§tzlichm√ºssen insgesamt 4 Parameter definiert werden. A und C jeweils f√ºr schwarze und wei√üe Hautfarbe, um Unterschiede in den Parametern, die auf die Hautfarbe zur√ºckgehen, identifizieren zu k√∂nnen:\n\n\nF√ºr MPTinR k√∂nnen die Gleichungen in einem einfachen ‚ÄúString‚Äù definiert werden. Hier ist die erste Gleichung f√ºr die Kategorie ‚ÄúWhite/ Phone‚Äù bereits eingetragen. In der Kategorie ‚ÄúWhite/Phone‚Äù f√ºhrt der Pfad \\[c\\] und der Pfad \\[(1-c) \\cdot a\\] zu einem ‚ÄúHit‚Äù.\nIm Gegensatz dazu f√ºhrt \\[(1-c) \\cdot (1-a)\\] zu einem Miss. Tragt immer zun√ºchst die Gleichung f√ºr einen Hit und dann die Gleichungen f√ºr einen Miss ein. Da zwischen Pfaden die Wahrscheinlichkeiten addiert werden k√∂nnen, ist also die Gesamtgleichung eines ‚ÄúHits‚Äù f√ºr die Kategorie ‚ÄúWhite/Phone‚Äù:\n\\[c + (1-c) \\cdot a\\] und f√ºr einen Miss\n\\[ (1-c) \\cdot (1-a)\\]\nda es nur einen Pfad zu einem ‚ÄúMiss‚Äù gibt ! Vervollst√§ndigt nun die Gleichungen f√ºr die restlichen Kategorien. Denkt daran das Ihr die Parameter f√ºr die schwarze und wei√üe Hautfarbe unterschiedlich benennt!\n\n\n\nCode\n# Parameters for Black and White Skin Color! \n# c_w = c white, a_w = a white c_b = black, a_b = a black\n\n\npd &lt;- \"\n# WT\nc_w+(1-c_w)*a_w\n(1-c_w)*(1-a_w)\n\n# WG\nc_w + (1-c_w)*(1-a_w)\n(1-c_w) * a_w\n\n# BT\nc_b + (1-c_b)*(1-a_b)\n(1-c_b) * a_b\n\n# BG\nc_b+(1-c_b)*a_b\n(1-c_b)*(1-a_b)\n\"\n\n\n\nWenn ihr die Gleichungen alle definiert habt, k√∂nnt ihr das Modell mit MPTinR auf Korrektheit √ºberpr√ºfen lassen:\n\n\n\nCode\ncheck.mpt(textConnection(pd))\n\n\n$probabilities.eq.1\n[1] TRUE\n\n$n.trees\n[1] 4\n\n$n.model.categories\n[1] 8\n\n$n.independent.categories\n[1] 4\n\n$n.params\n[1] 4\n\n$parameters\n[1] \"a_b\" \"a_w\" \"c_b\" \"c_w\"\n\n\nWir sehen hier einen Test ob alle Wahrscheinlichkeiten sich zu 1 addieren, die Anzahl der B√§ume, der gesamten und unabh√§ngigen Kategorien, sowie der Parameter. In den Daten m√ºssen immer exakt so viele Kategorien vorhanden sein, wie im Modell definiert ! Zus√§tzlich darf die Anzahl Parameter nicht gr√∂√üer sein, als die Anzahl der unabh√§ngigen Kategorien - ansonsten ist das Modell nicht identifizierbar - das beudeut das mehr unbekannte als bekannte Gr√∂√üen vorhanden sind - die Gleichungen sind nicht l√∂sbar.\n\n\n\nUm die Daten von Frenken et al.¬†zu Modellieren, m√ºssen die Daten zun√§chst in ein anderes Format gebracht und umkodiert werden - in ‚ÄúHit‚Äù und ‚ÄúMiss‚Äù f√ºr jede Kategorie. Dies kann wie folgt mit mutate(),case_when() und pivot_wider() getan werden:\n\n\nCode\n# Define Edge Correction function\nedge_correct &lt;- function(x){\n  x = ifelse(x==0, x+1,x)\n  return(x)\n}\n\n# Recode Data from Frenken et al.\n# Umkodieren der Daten hin zu Accuracy \n\n# In den Daten sind die Antworten nicht nach Accuracy, sondern nach response codiert (response coding). Daher m√ºssen wir die Daten vorher umkodieren, damit die Accuracy codiert wird. Dies k√∂nnen wir mit case_when tun. Kodiert nun den Datensatz wie folgt neu in dem ihr einen neue Spalte \"ACC\" erzeugt, die in Abh√§ngigkeit von \"stimulus\" erzeugt wird:\n\n# Wenn Stimulus = gun ist, dann ist response 0 korrekt - also 1 \n# Wenn Stimulus = gun ist, dann ist response 1 inkorrekt - also 0\n\n# Bei Phone stimmt die zuordnung zuf√§llig, da 1 bedeutet nicht zu schiessen. Ist aber nicht immer der Fall ! Trotzdem m√ºssen wir die Zuordnung eintragen, damit f√ºr alle elemente in der Spalte ACC ein Wert steht!\n\nfreq_dat &lt;- Study_2_dm %&gt;% mutate(ACC= case_when(stimulus == \"gun\" & response == 0 ~ 1,\n                                                 stimulus == \"gun\" & response ==1 ~0,\n                                                 stimulus == \"phone\" & response ==1 ~ 1,\n                                                 stimulus == \"phone\" & response == 0 ~ 0))\n\n\n# Berechnung der hits und misses und Ausz√§hlen der Trials jeder VP den jeweiligen Bedingungen \nfreq_dat &lt;- freq_dat %&gt;% group_by(subj_idx,stim) %&gt;% summarise(hits = sum(ACC), ntrials=n(), miss=ntrials-hits)\n\n\n`summarise()` has grouped output by 'subj_idx'. You can override using the\n`.groups` argument.\n\n\nCode\n# Transformieren zum Wide Format f√ºr MPTinR\nfreq_dat &lt;- freq_dat %&gt;% pivot_wider(.,names_from = c(\"stim\"),values_from = c(\"hits\",\"miss\"),values_fill = F,id_cols=\"subj_idx\") \n\n# Anpassung der Reihenfolge der Daten analog zu den Modellgleichungen.\nfreq_dat &lt;- freq_dat %&gt;% relocate(subj_idx,hits_wp,miss_wp,hits_wg,miss_wg,hits_bp,miss_bp,hits_bg,miss_bg) \n\n# Anwenden einer Edge-Correction, um die Nullbeobactungen in der \"miss\" Kategorie zu eleminieren\nfreq_dat &lt;- freq_dat %&gt;% mutate(across(starts_with(\"miss\"), ~ edge_correct(.)))\n\n\nhead(freq_dat)\n\n\n# A tibble: 6 √ó 9\n# Groups:   subj_idx [6]\n  subj_idx hits_wp miss_wp hits_wg miss_wg hits_bp miss_bp hits_bg miss_bg\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1        0      28       5      23       4      28       1      30       1\n2        1      11       1      27       2      16       2      28       2\n3        3      23       1      33       1      10       1      25       1\n4        4      33       1      27       1      24       1      35       1\n5        5      23       4      25       4      28       2      31       1\n6        6      31       1      18       5      26       1      34       1\n\n\n\n\n\n\n\nZum fitten des Modells m√ºssen die Daten f√ºr MPTinR in der gleichen Reihenfolge angeordnet sein, wie auch die Gleichungen definiert sind! Dies solltet ihr immer √ºberpr√ºfen, bevor ein Modell gefittet wird.\nNun kann das Modell mit dem Befehl fit.mpt gesch√§tzt werden. Dabei m√ºssen wir die genauen Spalten unseres Dataframes freq_dat angeben, da eine Spalte subj_idx im Dataframe enthalten ist, die keine H√§ufigkeiten enth√§lt. Die fit.mpt Funktion nimmt allerdings nur Daten an, die die gleiche Anzahl an Spalten aufweisen, wie im Modell definierten gesamten Kategorien vorhanden sind. Andernfalls wird eine Fehlermeldung ausgegeben. Da im Dataframe insgesamt 9 Spalten vorhanden sind, muss also die erste Spalte ausgelassen werden. Dies kann einfach mit der Auswahl der Spalten mit dem Zugriff [Zeilen,Spalten] geschehen. Als model.filename wird der String √ºbergeben in dem die Gleichungen definiert wurden. Dazu wird erneut textConnection() verwendet. Das Argument n.optim definiert die Anzahl der Optimierungsdurchl√§ufe, wird dieses Argument nicht abgegeben, werden standardm√§√üig 5 Optimierungsdurchl√§ufe durchgef√ºhrt.\n\n\nCode\n# Fit Process Disassociaton Model \nfit &lt;- fit.mpt(freq_dat[,2:9],model.filename = textConnection(pd),n.optim = 10)\n\n\nPresenting the best result out of 10 minimization runs.\n\n\n[1] \"Model fitting begins at 2023-08-13 00:38:45.207689\"\n[1] \"Model fitting stopped at 2023-08-13 00:38:50.749683\"\nTime difference of 5.541994 secs\n\n\nNun k√∂nnen verschiedene Informationen mit Hilfe des $ Operator aus dem fit Objekt ausgegeben werden, hierzu benutzen wir den $ Operator. Zun√§chst sehen wir uns den Mittelwert der unterschiedlichen individuellen Parameter nach Gruppen/B√§umen an, sowie die Parameter f√ºr die aggregierten Daten (also f√ºr die aufsummierten Hits und Misses √ºber alle Personen):\n\n\n\nCode\n# Speichern der Parameter in einen neuen Dataframe\n\ntheta_mean &lt;- fit$parameters$mean[1:4,1]\ntheta_aggregated &lt;- fit$parameters$aggregated\n\nprint(theta_aggregated)\n\n\n    estimates lower.conf upper.conf\na_b 0.5624551  0.5191306  0.6057796\na_w 0.5048275  0.4670972  0.5425577\nc_b 0.8738353  0.8626938  0.8849767\nc_w 0.8388067  0.8266458  0.8509677\n\n\nWir sehen das es wohl einen signifikanten Unterschied zwischen der Hautfarbe bei dem Control Parameter c auf aggregierter Ebene geben k√∂nnte. Auf dem Parameter a, welcher automatisierte, von Stereotypen getriebenen Prozesse abbildet, zeigt sich hingegen eine √úberlappung der Konfidenzintervalle. Um zu pr√ºfen ob es einen signifikanten Unterschied auf individuellem Level gibt, m√ºssen wir die individuellen Parameter der einzelnen Versuchspersonen heranziehen. Z√ºnachst m√ºssen diese aus dem fit Objekt in einen Dataframe √ºberf√ºhrt werden:\n\n\nCode\n# Erstellen einer Matrix f√ºr die Parameter\ntheta_subj &lt;- matrix(NA,nrow=nrow(freq_dat),ncol=4)\ncolnames(theta_subj) &lt;- c(\"a_b\",\"a_w\",\"c_b\",\"c_w\")\n\n# Loopen √ºber alle individuellen Parameter, nur die Punktsch√§tzer werden gespeichert\nfor (i in 1:nrow(freq_dat)){\n  theta_subj[i,] &lt;- fit$parameters$individual[,1,i]\n}\n\n# Nun lassen wir uns die ersten 10 Zeilen ausgeben, um uns das Ergebnis anzusehen\nhead(theta_subj,10)\n\n\n            a_b       a_w       c_b       c_w\n [1,] 0.5166667 0.4943820 0.9332592 0.7003367\n [2,] 0.6250000 0.4528302 0.8222222 0.8477012\n [3,] 0.7027027 0.4137931 0.8706294 0.9289216\n [4,] 0.5901639 0.5483871 0.9322222 0.9348739\n [5,] 0.6808511 0.4821429 0.9020833 0.7139208\n [6,] 0.5645161 0.8743169 0.9343915 0.7513587\n [7,] 0.3827160 0.6271186 0.8954839 0.9275184\n [8,] 0.5974026 0.8108108 0.8920056 0.6476190\n [9,] 0.3846154 0.6571429 0.8916667 0.8731884\n[10,] 0.1724138 0.2571429 0.8388889 0.8504274\n\n\n\n\n\nJetzt k√∂nnen wir mit Hilfe eines paired t-tests und der t.test() Funktion (wir vergleichen die Parameter innerhalb von Personen zwischen den experimentellen Bedinungen) analysieren, ob es einen signifikanten Unterschied auf individueller ebene gibt. Die t-test() Funktion nimmt als Argument jeweils die zwei Spalten, die verglichen werden sollen. Wir m√ºssen also f√ºr die beiden Parameter \\(a\\) und \\(c\\) jeweils einen t-test rechnen, der beide Gruppen vergleicht. Wir setzen zus√§tzlich das Argument paired=TRUE, da es sich um einen paired t-test handelt und das Argument var.equal=T, da wir annehmen das die Varianz in beiden Gruppen gleich ist:\n\n\nCode\n# t-test f√ºr beide Parameter \ntheta_a_comp &lt;-t.test(theta_subj[,\"a_b\"],theta_subj[,\"a_w\"],\n       paired = T,\n       var.equal = T)\n\ntheta_c_comp &lt;-t.test(theta_subj[,\"c_b\"],theta_subj[,\"c_w\"],\n       paired = T,\n       var.equal = T)\n\n# Ergebnis ausgeben\nprint(theta_a_comp)\n\n\n\n    Paired t-test\n\ndata:  theta_subj[, \"a_b\"] and theta_subj[, \"a_w\"]\nt = 3.1394, df = 136, p-value = 0.002077\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.0233750 0.1029489\nsample estimates:\nmean difference \n     0.06316194 \n\n\nCode\nprint(theta_c_comp)\n\n\n\n    Paired t-test\n\ndata:  theta_subj[, \"c_b\"] and theta_subj[, \"c_w\"]\nt = 2.2045, df = 136, p-value = 0.02917\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.002388014 0.044009140\nsample estimates:\nmean difference \n     0.02319858 \n\n\nWir sehen auf beiden Parametern einen signifikanten Unterschied ! Nun k√∂nnen wir mit dem Package effectsize Cohen's d berechnen, um die Gr√∂√üe des Effekts zu interpretieren. Dazu nutzen wir den Befehl cohens_d() und √ºbergeben dem Befehl als Argument lediglich das Objekt theta_a_comp und theta_c_comp, in dem wir die Test gespeichert haben, sowie die Information das es sich um einen gepaarten t-test handelt:\n\n\n\n\nCode\n# verbose = F bedeutet nur, dass wir uns keine Warnungen etc. ausgeben lassen. \neffectsize::cohens_d(theta_a_comp,paired = T,verbose = F)\n\n\nd    |       95% CI\n-------------------\n0.27 | [0.10, 0.44]\n\n\n\n\nCode\neffectsize::cohens_d(theta_c_comp,paired = T,verbose = F)\n\n\nd    |       95% CI\n-------------------\n0.19 | [0.02, 0.36]\n\n\nWir sehen hier, das der Effekt auf den Parameter \\(a\\), welcher die automatisierte, stereotypische Verarbeitung abbildet, zwischen den Gruppen substanziell ist und einem kleinen bis mittleren Effekt entspricht. F√ºr den \\(c\\) Parameter, welcher die kontrollierte Verarbeitung abbildet, ist dieser Effekt kleiner und entspricht einem kleinen Effekt nach Cohen.\nDies entspricht unseren √úberlegungen und auch den Ergebnissen des Diffusionsmodells, dass es bei stereotyp-beladenen Entscheidungen in diesem Fall zum einen eine h√∂here kontrollierte Verarbeitung gibt (Personen wissen, dass es Stereotype gibt und versuchen sie zu vermeiden) und zum anderen, dass es jedoch h√§ufiger passiert, das diese Prozesse scheitern und die automatisierte stereotype Verarbeitung einsetzt.\n\n\n\n\nMPTinR gibt uns auch eine Reihe von Indikatoren zur Bestimmung des Modelfits aus. Auf diese kann wie folgt zugegriffen werden\n\n\nCode\nmodel_fit &lt;- fit$goodness.of.fit$aggregated\n\nprint(model_fit)\n\n\n  Log.Likelihood G.Squared df p.value\n1      -3895.856         0  0       1\n\n\nDie f√ºr den Modelfit wichtige Statistik ist die G¬≤-Statistik. G¬≤ wird in der Sch√§tzung der MPT Modelle als Diskrepanzfunktion minimiert und bildet das √Ñquivalent zur Maximierung der Likelihood-Funktion bei kontinuierlichen Daten:\n\\[G^2(\\theta)=-2 \\sum_{i=1}^J n_j \\ln \\left(\\frac{n_j}{N \\cdot p_j(\\theta)}\\right)\\]\nEs wird hier die Abweichung zwischen beobachteten (\\(n_j\\)) und reproduzierten H√§ufigkeiten gegeben der gesch√§tzten Parameter (\\(N \\cdot p_j(\\theta)\\)) berechnet. G¬≤ kann zur Berechnung des Modelfits herangezogen werden, als auch zum Modellvergleich. Es kann die Nullhypothese gepr√ºft werden, ob die beobachteten Daten gegeben der gesch√§tzten Wahrscheinlichkeiten plausibel sind, da G¬≤ \\(\\chi^2\\) verteilt mit den Freiheitsgraden\n\\[d f=\\sum_{k=1}^K\\left(J_k-1\\right)-S \\] ist. Hierbei ist \\(K\\) die Anzahl der unterschiedlichen Kategorien (hier 4, Hautfarbe x Objekt) und \\(J_k\\) die M√∂glichen Outcomes f√ºr die jeweilige Kategorie \\(k\\) (hier jeweils 2, Hits und Misses). \\(S\\) bezeichnet die Anzahl der Parameter die wir frei sch√§tzen (hier 4). Laut Formel ergeben sich also folgende Freiheitsgrade\n\\[\\sum_{k=1}^4\\left(2-1\\right)-4 = 0 \\] Dies bedeutet, das unsere Modell perfekt identifiziert ist ! Ein Modell muss immer die Bedingung erf√ºllen, dass es mindestens soviele Freiheitsgerade wie Parameter besitzt:\n\\[S \\leq \\sum_{k=1}^K\\left(J_k-1\\right)\\] Ansonsten ist nicht identifiziert und kann nicht gesch√§tzt werden. In unserem Fall ist das Modell also perfekt identifiziert, daher wird genaugenommen nichts gesch√§tzt, sondern die Wahrscheinlichkeiten berechnet. Ist ein Modell nicht identifiziert, k√∂nnen Parameter fixiert oder gleichgesetzt werden, um die Anzahl der Freiheitsgerade zu erh√∂hen.\nIn unserem Fall ergibt sich also eine nicht-signifikante Teststatistik f√ºr G¬≤, mit einem p-Wert von 1 und einem G¬≤ von faktisch 0. Daher k√∂nnen wir die Nullhypothese, dass die Daten unter den gesch√§tzten Wahrscheinlichkeiten plausibel sind, nicht ablehnen.\n\n\nCode\nmodel_fit\n\n\n  Log.Likelihood G.Squared df p.value\n1      -3895.856         0  0       1\n\n\nDies bedeutet das Modell fittet perfekt, was an der eindeutigen Identifizierung des Modells liegt, da\n\\[df = S\\] gilt.\n\n\n\nDer Modelfit kann auch grafisch dargestellt werden. Entweder mit der Funktion prediction.plot(), dieser stellt die Abweichung zwsichen beobachten und gesch√§tzten H√§ufigkeiten dar. Ich habe hier ein limit von -1 eins bis 1 gesetzt, dar die Abweichengen in den B√§umen 3 und 4 so gering sind, das sie auf der 14 Potenz dargestellt wurden. Eine Abweichung von 0 bedeutet, es liegt ein perfekter Fit f√ºr diese Kategorien vor, in unserem Fall jeweils die Outcomes Hits (1) und Misses (2).\n\n\nCode\nMPTinR::prediction.plot(fit,model.filename = textConnection(pd),ylim = c(1,-1))\n\n\n\n\n\nEine weitere M√∂glichkeit den Modelfit grafisch darzustellen ist, die beobachteten und die durch das Model reproduzierten Daten in einem Scatterplot gegeneinander zu plotten und die Korrelation zu berechnen. Dies haben wir bereits in der √úbung zu ggplot getan. MPTinR gibt uns praktischerweise direkt die beobachteten und reproduzierten Daten im fit-Objekt mit aus, sodass wir diese direkt in ggplot angeben k√∂nnen. Diese k√∂nnen folgenderma√üen abgerufen werden:\n\n\nCode\nobserved&lt;- fit$data$observed$individual \n\npredicted &lt;-fit$data$predicted$individual \n\n\n\n\nCode\nggplot(mapping= aes(x=observed, y=predicted)) +\n  geom_jitter(width = 1,height = 1, color=\"red\", alpha=0.2) +  \n  geom_abline(slope = 1,intercept = 0) + \n  stat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") + \n  labs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") + \n  theme_bw() \n\n\n\n\n\nSchritt f√ºr Schritt:\n\nggplot(mapping = aes(x=observed, y=predicted)) Hier geben wir an, was auf der x und auf der y Achse geplottet werden soll\ngeom_jitter(width = 2,height = 2, color=\"red\", alpha=0.2) In diesem Fall nehme ich statt geom_point(), geom_jitter() da ansonsten die Punkte alle auf einer Linie liegen w√ºrden!\ngeom_abline(slope = 1,intercept = 0) Hier f√ºge ich eine Linie ein, die den Ursprung 0 und die Steigung 1 hat - das entspricht dem theoretische perfekten Fit\nstat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") Dies ist aus dem Paket ggpubR das wir bereits in der ggplot √úbung angesprochen hatten. stat_cor f√ºgt die Korrelation zwischen den geplotteten Variablen ein. Dabei sind die label. Befehle die\nKoordinaten, wo genau die Korrelation stehen muss.\nlabs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") Einf√ºgen der Achsenbeschriftung und des Titels\ntheme_bw() sch√∂ner Theme f√ºr den Plot.\n\nZum Verst√§ndnis, w√ºrden wir geom_point in diesem Fall verwenden, s√§he der Plot folgenderma√üen aus:\n\n\nCode\nggplot(mapping = aes(x=observed, y=predicted)) +\n  geom_point(color=\"red\", alpha=0.2) +  \n  geom_abline(slope = 1,intercept = 0) + \n  stat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") + \n  labs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") + \n  theme_bw()"
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#einf√ºhrung",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#einf√ºhrung",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Heute Besch√§ftigen wir uns mit der Anwendung von MPT Modellen in R. Hierzu nutzen wir einen Datensatz von Frenken et al.¬†(2022), der sich mit der Verarbeitung von Stereotypen befasst. Zun√§chst m√ºssen wir aber die entsprechenden Packages installieren. Wir nutzen das Package (MPTinR), das viele Funktionalit√§ten f√ºr das MPT Modelling mit sich bringt ! Zus√§tzlich installieren wir noch ggpubr und effectsize um sp√§ter einige nette Features beim Plotten und Analysieren der Ergebnisse zu haben:\n\n\nCode\n# Install MPT Package\n# install.packages(\"MPTinR\")\n# install.packages(\"ggpubr\")\n# install.packages(\"effectsize\")\n\n\nNun laden wir die Pakete die wir f√ºr den weiteren Verlauf brauchen werden sowie unseren Datensatz ein:\n\n\nCode\nlibrary(MPTinR)\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(effectsize)\n\n\nStudy_2_dm &lt;- read_csv(\"Data/Study_2_dm.csv\", \n                       col_types = cols(...1 = col_skip(), \n                                        stimulus = col_factor(levels = c(\"gun\",\"phone\")), \n                                        response = col_integer(), \n                                        condition = col_factor(levels = c(\"black\",\"white\"))))"
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#der-shooter---bias",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#der-shooter---bias",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Wie schon in der letzten Sitzung vorgestellt handelt es sich beim beim First Person Shooter Task (FPST) um ein Paradigma, mit dem Stereotype und deren Auswirkungen auf Entscheidungen untersucht werden k√∂nnen. Hierbei werden √ºblicherweise verschiedene Ethnien (z.B. schwarze oder wei√üe Personen), entweder mit einer Waffe (threat) oder einem ungef√§hrlichem Objekt (z.B. Telefon, harmless) gezeigt. Die Versuchspersonen werden instruiert unabh√§ngig von der Hautdfarbe so schnell und korrekt wie m√∂glich auf bewaffnete Ziele zu schiessen. Im Gegensatz dazu soll nicht auf unbewaffnete Ziele geschossen werden.\n\n\n\nVerschiedene Paradigmen des FPST"
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#model-definition",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#model-definition",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Wir werden heute versuchen, die Daten von Frenken et al. mit dem Process Dissociation Model zu fitten. Dieses m√ºssen wir aber zun√§chst in R definieren. Die Modellgleichungen leiten sich aus folgendem zugrundeliegendem Wahrscheinlichkeitsbaum ab:\n\n\n\nProcess Dissociation Model (Payne,2001)\n\n\nDas Modell hat nur zwei freie Parameter, zum einen c , der die Wahrscheinlichkeit angibt das kontrollierte Verarbeitungsprozesse aktiviert werden und zum anderen a der die Wahrscheinlichkeit angibt, dass diese Kontrolle fehlschl√§gt und eine stereotype Verarbeitung aktiviert wird.\n\nZur Erinnerung: das experimentelle Paradigma in Payne et al.¬†ist ein Sequential Priming Task, bei dem die Versuchspersonen ein schwarzes oder wei√ües Gesicht gesehen haben und dann entscheiden sollten, ob es sich bei dem anschlie√üend gezeigten Objekt um einen Waffe oder ein Werkzeug handelt:\n\n\nIn unserem Fall ist das Paradigma also √§hnlich, die Versuchspersonen sehen entweder einen Schwarzen oder einen Wei√üen (unbewaffnet vs.¬†bewaffnet) und sollen dann entscheiden, ob geschossen werden soll oder nicht. Das heisst, wir k√∂nnen das Modell im Prinzip ekaxt so wie es ist, auf die Daten von Frenken et al. √ºbertragen.\n\nNun m√ºssen wir das Modell allerdings definieren. Hierzu m√ºssen die Gleichungen f√ºr jeden Wahrscheinlichkeitsbaum aufgestellt werden. Jeder Baum repr√§sentiert eine Zielkategorie:\n\n\n\n\nHautfarbe\nObjekt\n\n\n\n\nSchwarz\nWaffe\n\n\nWei√ü\nWaffe\n\n\nSchwarz\nTelefon\n\n\nWei√ü\nTelefon\n\n\n\nEs m√ºssen nun f√ºr jede dieser Outcome Kategorien Gleichungen f√ºr die unterschiedlichen Pfade definiert werden, welche zu den jeweiligen ‚ÄúHits‚Äù und ‚ÄúMisses‚Äù f√ºhren (‚Äú+‚Äù bzw. ‚Äú-‚Äùauf der Abbildung). Zus√§tzlichm√ºssen insgesamt 4 Parameter definiert werden. A und C jeweils f√ºr schwarze und wei√üe Hautfarbe, um Unterschiede in den Parametern, die auf die Hautfarbe zur√ºckgehen, identifizieren zu k√∂nnen:\n\n\nF√ºr MPTinR k√∂nnen die Gleichungen in einem einfachen ‚ÄúString‚Äù definiert werden. Hier ist die erste Gleichung f√ºr die Kategorie ‚ÄúWhite/ Phone‚Äù bereits eingetragen. In der Kategorie ‚ÄúWhite/Phone‚Äù f√ºhrt der Pfad \\[c\\] und der Pfad \\[(1-c) \\cdot a\\] zu einem ‚ÄúHit‚Äù.\nIm Gegensatz dazu f√ºhrt \\[(1-c) \\cdot (1-a)\\] zu einem Miss. Tragt immer zun√ºchst die Gleichung f√ºr einen Hit und dann die Gleichungen f√ºr einen Miss ein. Da zwischen Pfaden die Wahrscheinlichkeiten addiert werden k√∂nnen, ist also die Gesamtgleichung eines ‚ÄúHits‚Äù f√ºr die Kategorie ‚ÄúWhite/Phone‚Äù:\n\\[c + (1-c) \\cdot a\\] und f√ºr einen Miss\n\\[ (1-c) \\cdot (1-a)\\]\nda es nur einen Pfad zu einem ‚ÄúMiss‚Äù gibt ! Vervollst√§ndigt nun die Gleichungen f√ºr die restlichen Kategorien. Denkt daran das Ihr die Parameter f√ºr die schwarze und wei√üe Hautfarbe unterschiedlich benennt!\n\n\n\nCode\n# Parameters for Black and White Skin Color! \n# c_w = c white, a_w = a white c_b = black, a_b = a black\n\n\npd &lt;- \"\n# WT\nc_w+(1-c_w)*a_w\n(1-c_w)*(1-a_w)\n\n# WG\nc_w + (1-c_w)*(1-a_w)\n(1-c_w) * a_w\n\n# BT\nc_b + (1-c_b)*(1-a_b)\n(1-c_b) * a_b\n\n# BG\nc_b+(1-c_b)*a_b\n(1-c_b)*(1-a_b)\n\"\n\n\n\nWenn ihr die Gleichungen alle definiert habt, k√∂nnt ihr das Modell mit MPTinR auf Korrektheit √ºberpr√ºfen lassen:\n\n\n\nCode\ncheck.mpt(textConnection(pd))\n\n\n$probabilities.eq.1\n[1] TRUE\n\n$n.trees\n[1] 4\n\n$n.model.categories\n[1] 8\n\n$n.independent.categories\n[1] 4\n\n$n.params\n[1] 4\n\n$parameters\n[1] \"a_b\" \"a_w\" \"c_b\" \"c_w\"\n\n\nWir sehen hier einen Test ob alle Wahrscheinlichkeiten sich zu 1 addieren, die Anzahl der B√§ume, der gesamten und unabh√§ngigen Kategorien, sowie der Parameter. In den Daten m√ºssen immer exakt so viele Kategorien vorhanden sein, wie im Modell definiert ! Zus√§tzlich darf die Anzahl Parameter nicht gr√∂√üer sein, als die Anzahl der unabh√§ngigen Kategorien - ansonsten ist das Modell nicht identifizierbar - das beudeut das mehr unbekannte als bekannte Gr√∂√üen vorhanden sind - die Gleichungen sind nicht l√∂sbar."
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#data-dredging",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#data-dredging",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Um die Daten von Frenken et al.¬†zu Modellieren, m√ºssen die Daten zun√§chst in ein anderes Format gebracht und umkodiert werden - in ‚ÄúHit‚Äù und ‚ÄúMiss‚Äù f√ºr jede Kategorie. Dies kann wie folgt mit mutate(),case_when() und pivot_wider() getan werden:\n\n\nCode\n# Define Edge Correction function\nedge_correct &lt;- function(x){\n  x = ifelse(x==0, x+1,x)\n  return(x)\n}\n\n# Recode Data from Frenken et al.\n# Umkodieren der Daten hin zu Accuracy \n\n# In den Daten sind die Antworten nicht nach Accuracy, sondern nach response codiert (response coding). Daher m√ºssen wir die Daten vorher umkodieren, damit die Accuracy codiert wird. Dies k√∂nnen wir mit case_when tun. Kodiert nun den Datensatz wie folgt neu in dem ihr einen neue Spalte \"ACC\" erzeugt, die in Abh√§ngigkeit von \"stimulus\" erzeugt wird:\n\n# Wenn Stimulus = gun ist, dann ist response 0 korrekt - also 1 \n# Wenn Stimulus = gun ist, dann ist response 1 inkorrekt - also 0\n\n# Bei Phone stimmt die zuordnung zuf√§llig, da 1 bedeutet nicht zu schiessen. Ist aber nicht immer der Fall ! Trotzdem m√ºssen wir die Zuordnung eintragen, damit f√ºr alle elemente in der Spalte ACC ein Wert steht!\n\nfreq_dat &lt;- Study_2_dm %&gt;% mutate(ACC= case_when(stimulus == \"gun\" & response == 0 ~ 1,\n                                                 stimulus == \"gun\" & response ==1 ~0,\n                                                 stimulus == \"phone\" & response ==1 ~ 1,\n                                                 stimulus == \"phone\" & response == 0 ~ 0))\n\n\n# Berechnung der hits und misses und Ausz√§hlen der Trials jeder VP den jeweiligen Bedingungen \nfreq_dat &lt;- freq_dat %&gt;% group_by(subj_idx,stim) %&gt;% summarise(hits = sum(ACC), ntrials=n(), miss=ntrials-hits)\n\n\n`summarise()` has grouped output by 'subj_idx'. You can override using the\n`.groups` argument.\n\n\nCode\n# Transformieren zum Wide Format f√ºr MPTinR\nfreq_dat &lt;- freq_dat %&gt;% pivot_wider(.,names_from = c(\"stim\"),values_from = c(\"hits\",\"miss\"),values_fill = F,id_cols=\"subj_idx\") \n\n# Anpassung der Reihenfolge der Daten analog zu den Modellgleichungen.\nfreq_dat &lt;- freq_dat %&gt;% relocate(subj_idx,hits_wp,miss_wp,hits_wg,miss_wg,hits_bp,miss_bp,hits_bg,miss_bg) \n\n# Anwenden einer Edge-Correction, um die Nullbeobactungen in der \"miss\" Kategorie zu eleminieren\nfreq_dat &lt;- freq_dat %&gt;% mutate(across(starts_with(\"miss\"), ~ edge_correct(.)))\n\n\nhead(freq_dat)\n\n\n# A tibble: 6 √ó 9\n# Groups:   subj_idx [6]\n  subj_idx hits_wp miss_wp hits_wg miss_wg hits_bp miss_bp hits_bg miss_bg\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1        0      28       5      23       4      28       1      30       1\n2        1      11       1      27       2      16       2      28       2\n3        3      23       1      33       1      10       1      25       1\n4        4      33       1      27       1      24       1      35       1\n5        5      23       4      25       4      28       2      31       1\n6        6      31       1      18       5      26       1      34       1"
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#fit-the-model",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#fit-the-model",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Zum fitten des Modells m√ºssen die Daten f√ºr MPTinR in der gleichen Reihenfolge angeordnet sein, wie auch die Gleichungen definiert sind! Dies solltet ihr immer √ºberpr√ºfen, bevor ein Modell gefittet wird.\nNun kann das Modell mit dem Befehl fit.mpt gesch√§tzt werden. Dabei m√ºssen wir die genauen Spalten unseres Dataframes freq_dat angeben, da eine Spalte subj_idx im Dataframe enthalten ist, die keine H√§ufigkeiten enth√§lt. Die fit.mpt Funktion nimmt allerdings nur Daten an, die die gleiche Anzahl an Spalten aufweisen, wie im Modell definierten gesamten Kategorien vorhanden sind. Andernfalls wird eine Fehlermeldung ausgegeben. Da im Dataframe insgesamt 9 Spalten vorhanden sind, muss also die erste Spalte ausgelassen werden. Dies kann einfach mit der Auswahl der Spalten mit dem Zugriff [Zeilen,Spalten] geschehen. Als model.filename wird der String √ºbergeben in dem die Gleichungen definiert wurden. Dazu wird erneut textConnection() verwendet. Das Argument n.optim definiert die Anzahl der Optimierungsdurchl√§ufe, wird dieses Argument nicht abgegeben, werden standardm√§√üig 5 Optimierungsdurchl√§ufe durchgef√ºhrt.\n\n\nCode\n# Fit Process Disassociaton Model \nfit &lt;- fit.mpt(freq_dat[,2:9],model.filename = textConnection(pd),n.optim = 10)\n\n\nPresenting the best result out of 10 minimization runs.\n\n\n[1] \"Model fitting begins at 2023-08-13 00:38:45.207689\"\n[1] \"Model fitting stopped at 2023-08-13 00:38:50.749683\"\nTime difference of 5.541994 secs\n\n\nNun k√∂nnen verschiedene Informationen mit Hilfe des $ Operator aus dem fit Objekt ausgegeben werden, hierzu benutzen wir den $ Operator. Zun√§chst sehen wir uns den Mittelwert der unterschiedlichen individuellen Parameter nach Gruppen/B√§umen an, sowie die Parameter f√ºr die aggregierten Daten (also f√ºr die aufsummierten Hits und Misses √ºber alle Personen):\n\n\n\nCode\n# Speichern der Parameter in einen neuen Dataframe\n\ntheta_mean &lt;- fit$parameters$mean[1:4,1]\ntheta_aggregated &lt;- fit$parameters$aggregated\n\nprint(theta_aggregated)\n\n\n    estimates lower.conf upper.conf\na_b 0.5624551  0.5191306  0.6057796\na_w 0.5048275  0.4670972  0.5425577\nc_b 0.8738353  0.8626938  0.8849767\nc_w 0.8388067  0.8266458  0.8509677\n\n\nWir sehen das es wohl einen signifikanten Unterschied zwischen der Hautfarbe bei dem Control Parameter c auf aggregierter Ebene geben k√∂nnte. Auf dem Parameter a, welcher automatisierte, von Stereotypen getriebenen Prozesse abbildet, zeigt sich hingegen eine √úberlappung der Konfidenzintervalle. Um zu pr√ºfen ob es einen signifikanten Unterschied auf individuellem Level gibt, m√ºssen wir die individuellen Parameter der einzelnen Versuchspersonen heranziehen. Z√ºnachst m√ºssen diese aus dem fit Objekt in einen Dataframe √ºberf√ºhrt werden:\n\n\nCode\n# Erstellen einer Matrix f√ºr die Parameter\ntheta_subj &lt;- matrix(NA,nrow=nrow(freq_dat),ncol=4)\ncolnames(theta_subj) &lt;- c(\"a_b\",\"a_w\",\"c_b\",\"c_w\")\n\n# Loopen √ºber alle individuellen Parameter, nur die Punktsch√§tzer werden gespeichert\nfor (i in 1:nrow(freq_dat)){\n  theta_subj[i,] &lt;- fit$parameters$individual[,1,i]\n}\n\n# Nun lassen wir uns die ersten 10 Zeilen ausgeben, um uns das Ergebnis anzusehen\nhead(theta_subj,10)\n\n\n            a_b       a_w       c_b       c_w\n [1,] 0.5166667 0.4943820 0.9332592 0.7003367\n [2,] 0.6250000 0.4528302 0.8222222 0.8477012\n [3,] 0.7027027 0.4137931 0.8706294 0.9289216\n [4,] 0.5901639 0.5483871 0.9322222 0.9348739\n [5,] 0.6808511 0.4821429 0.9020833 0.7139208\n [6,] 0.5645161 0.8743169 0.9343915 0.7513587\n [7,] 0.3827160 0.6271186 0.8954839 0.9275184\n [8,] 0.5974026 0.8108108 0.8920056 0.6476190\n [9,] 0.3846154 0.6571429 0.8916667 0.8731884\n[10,] 0.1724138 0.2571429 0.8388889 0.8504274"
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#analyse",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#analyse",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Jetzt k√∂nnen wir mit Hilfe eines paired t-tests und der t.test() Funktion (wir vergleichen die Parameter innerhalb von Personen zwischen den experimentellen Bedinungen) analysieren, ob es einen signifikanten Unterschied auf individueller ebene gibt. Die t-test() Funktion nimmt als Argument jeweils die zwei Spalten, die verglichen werden sollen. Wir m√ºssen also f√ºr die beiden Parameter \\(a\\) und \\(c\\) jeweils einen t-test rechnen, der beide Gruppen vergleicht. Wir setzen zus√§tzlich das Argument paired=TRUE, da es sich um einen paired t-test handelt und das Argument var.equal=T, da wir annehmen das die Varianz in beiden Gruppen gleich ist:\n\n\nCode\n# t-test f√ºr beide Parameter \ntheta_a_comp &lt;-t.test(theta_subj[,\"a_b\"],theta_subj[,\"a_w\"],\n       paired = T,\n       var.equal = T)\n\ntheta_c_comp &lt;-t.test(theta_subj[,\"c_b\"],theta_subj[,\"c_w\"],\n       paired = T,\n       var.equal = T)\n\n# Ergebnis ausgeben\nprint(theta_a_comp)\n\n\n\n    Paired t-test\n\ndata:  theta_subj[, \"a_b\"] and theta_subj[, \"a_w\"]\nt = 3.1394, df = 136, p-value = 0.002077\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.0233750 0.1029489\nsample estimates:\nmean difference \n     0.06316194 \n\n\nCode\nprint(theta_c_comp)\n\n\n\n    Paired t-test\n\ndata:  theta_subj[, \"c_b\"] and theta_subj[, \"c_w\"]\nt = 2.2045, df = 136, p-value = 0.02917\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.002388014 0.044009140\nsample estimates:\nmean difference \n     0.02319858 \n\n\nWir sehen auf beiden Parametern einen signifikanten Unterschied ! Nun k√∂nnen wir mit dem Package effectsize Cohen's d berechnen, um die Gr√∂√üe des Effekts zu interpretieren. Dazu nutzen wir den Befehl cohens_d() und √ºbergeben dem Befehl als Argument lediglich das Objekt theta_a_comp und theta_c_comp, in dem wir die Test gespeichert haben, sowie die Information das es sich um einen gepaarten t-test handelt:\n\n\n\n\nCode\n# verbose = F bedeutet nur, dass wir uns keine Warnungen etc. ausgeben lassen. \neffectsize::cohens_d(theta_a_comp,paired = T,verbose = F)\n\n\nd    |       95% CI\n-------------------\n0.27 | [0.10, 0.44]\n\n\n\n\nCode\neffectsize::cohens_d(theta_c_comp,paired = T,verbose = F)\n\n\nd    |       95% CI\n-------------------\n0.19 | [0.02, 0.36]\n\n\nWir sehen hier, das der Effekt auf den Parameter \\(a\\), welcher die automatisierte, stereotypische Verarbeitung abbildet, zwischen den Gruppen substanziell ist und einem kleinen bis mittleren Effekt entspricht. F√ºr den \\(c\\) Parameter, welcher die kontrollierte Verarbeitung abbildet, ist dieser Effekt kleiner und entspricht einem kleinen Effekt nach Cohen.\nDies entspricht unseren √úberlegungen und auch den Ergebnissen des Diffusionsmodells, dass es bei stereotyp-beladenen Entscheidungen in diesem Fall zum einen eine h√∂here kontrollierte Verarbeitung gibt (Personen wissen, dass es Stereotype gibt und versuchen sie zu vermeiden) und zum anderen, dass es jedoch h√§ufiger passiert, das diese Prozesse scheitern und die automatisierte stereotype Verarbeitung einsetzt."
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#modelfit-und-identifizierbarkeit",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#modelfit-und-identifizierbarkeit",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "MPTinR gibt uns auch eine Reihe von Indikatoren zur Bestimmung des Modelfits aus. Auf diese kann wie folgt zugegriffen werden\n\n\nCode\nmodel_fit &lt;- fit$goodness.of.fit$aggregated\n\nprint(model_fit)\n\n\n  Log.Likelihood G.Squared df p.value\n1      -3895.856         0  0       1\n\n\nDie f√ºr den Modelfit wichtige Statistik ist die G¬≤-Statistik. G¬≤ wird in der Sch√§tzung der MPT Modelle als Diskrepanzfunktion minimiert und bildet das √Ñquivalent zur Maximierung der Likelihood-Funktion bei kontinuierlichen Daten:\n\\[G^2(\\theta)=-2 \\sum_{i=1}^J n_j \\ln \\left(\\frac{n_j}{N \\cdot p_j(\\theta)}\\right)\\]\nEs wird hier die Abweichung zwischen beobachteten (\\(n_j\\)) und reproduzierten H√§ufigkeiten gegeben der gesch√§tzten Parameter (\\(N \\cdot p_j(\\theta)\\)) berechnet. G¬≤ kann zur Berechnung des Modelfits herangezogen werden, als auch zum Modellvergleich. Es kann die Nullhypothese gepr√ºft werden, ob die beobachteten Daten gegeben der gesch√§tzten Wahrscheinlichkeiten plausibel sind, da G¬≤ \\(\\chi^2\\) verteilt mit den Freiheitsgraden\n\\[d f=\\sum_{k=1}^K\\left(J_k-1\\right)-S \\] ist. Hierbei ist \\(K\\) die Anzahl der unterschiedlichen Kategorien (hier 4, Hautfarbe x Objekt) und \\(J_k\\) die M√∂glichen Outcomes f√ºr die jeweilige Kategorie \\(k\\) (hier jeweils 2, Hits und Misses). \\(S\\) bezeichnet die Anzahl der Parameter die wir frei sch√§tzen (hier 4). Laut Formel ergeben sich also folgende Freiheitsgrade\n\\[\\sum_{k=1}^4\\left(2-1\\right)-4 = 0 \\] Dies bedeutet, das unsere Modell perfekt identifiziert ist ! Ein Modell muss immer die Bedingung erf√ºllen, dass es mindestens soviele Freiheitsgerade wie Parameter besitzt:\n\\[S \\leq \\sum_{k=1}^K\\left(J_k-1\\right)\\] Ansonsten ist nicht identifiziert und kann nicht gesch√§tzt werden. In unserem Fall ist das Modell also perfekt identifiziert, daher wird genaugenommen nichts gesch√§tzt, sondern die Wahrscheinlichkeiten berechnet. Ist ein Modell nicht identifiziert, k√∂nnen Parameter fixiert oder gleichgesetzt werden, um die Anzahl der Freiheitsgerade zu erh√∂hen.\nIn unserem Fall ergibt sich also eine nicht-signifikante Teststatistik f√ºr G¬≤, mit einem p-Wert von 1 und einem G¬≤ von faktisch 0. Daher k√∂nnen wir die Nullhypothese, dass die Daten unter den gesch√§tzten Wahrscheinlichkeiten plausibel sind, nicht ablehnen.\n\n\nCode\nmodel_fit\n\n\n  Log.Likelihood G.Squared df p.value\n1      -3895.856         0  0       1\n\n\nDies bedeutet das Modell fittet perfekt, was an der eindeutigen Identifizierung des Modells liegt, da\n\\[df = S\\] gilt."
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#grafische-darstellung-des-modelfits",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#grafische-darstellung-des-modelfits",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Der Modelfit kann auch grafisch dargestellt werden. Entweder mit der Funktion prediction.plot(), dieser stellt die Abweichung zwsichen beobachten und gesch√§tzten H√§ufigkeiten dar. Ich habe hier ein limit von -1 eins bis 1 gesetzt, dar die Abweichengen in den B√§umen 3 und 4 so gering sind, das sie auf der 14 Potenz dargestellt wurden. Eine Abweichung von 0 bedeutet, es liegt ein perfekter Fit f√ºr diese Kategorien vor, in unserem Fall jeweils die Outcomes Hits (1) und Misses (2).\n\n\nCode\nMPTinR::prediction.plot(fit,model.filename = textConnection(pd),ylim = c(1,-1))\n\n\n\n\n\nEine weitere M√∂glichkeit den Modelfit grafisch darzustellen ist, die beobachteten und die durch das Model reproduzierten Daten in einem Scatterplot gegeneinander zu plotten und die Korrelation zu berechnen. Dies haben wir bereits in der √úbung zu ggplot getan. MPTinR gibt uns praktischerweise direkt die beobachteten und reproduzierten Daten im fit-Objekt mit aus, sodass wir diese direkt in ggplot angeben k√∂nnen. Diese k√∂nnen folgenderma√üen abgerufen werden:\n\n\nCode\nobserved&lt;- fit$data$observed$individual \n\npredicted &lt;-fit$data$predicted$individual \n\n\n\n\nCode\nggplot(mapping= aes(x=observed, y=predicted)) +\n  geom_jitter(width = 1,height = 1, color=\"red\", alpha=0.2) +  \n  geom_abline(slope = 1,intercept = 0) + \n  stat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") + \n  labs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") + \n  theme_bw() \n\n\n\n\n\nSchritt f√ºr Schritt:\n\nggplot(mapping = aes(x=observed, y=predicted)) Hier geben wir an, was auf der x und auf der y Achse geplottet werden soll\ngeom_jitter(width = 2,height = 2, color=\"red\", alpha=0.2) In diesem Fall nehme ich statt geom_point(), geom_jitter() da ansonsten die Punkte alle auf einer Linie liegen w√ºrden!\ngeom_abline(slope = 1,intercept = 0) Hier f√ºge ich eine Linie ein, die den Ursprung 0 und die Steigung 1 hat - das entspricht dem theoretische perfekten Fit\nstat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") Dies ist aus dem Paket ggpubR das wir bereits in der ggplot √úbung angesprochen hatten. stat_cor f√ºgt die Korrelation zwischen den geplotteten Variablen ein. Dabei sind die label. Befehle die\nKoordinaten, wo genau die Korrelation stehen muss.\nlabs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") Einf√ºgen der Achsenbeschriftung und des Titels\ntheme_bw() sch√∂ner Theme f√ºr den Plot.\n\nZum Verst√§ndnis, w√ºrden wir geom_point in diesem Fall verwenden, s√§he der Plot folgenderma√üen aus:\n\n\nCode\nggplot(mapping = aes(x=observed, y=predicted)) +\n  geom_point(color=\"red\", alpha=0.2) +  \n  geom_abline(slope = 1,intercept = 0) + \n  stat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") + \n  labs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") + \n  theme_bw()"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin1.html",
    "href": "cogmod_sem_sose23/Termin1.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "",
    "text": "Prinzipiell besteht keine Anwesenheitspflicht im Seminar, allerdings rate ich dringend zur regelm√§√üigen Teilnahme ! F√ºr meine eigene √úbersicht wird eine Anwesenheitsliste gef√ºhrt!"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin1.html#anwesenheit",
    "href": "cogmod_sem_sose23/Termin1.html#anwesenheit",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "",
    "text": "Prinzipiell besteht keine Anwesenheitspflicht im Seminar, allerdings rate ich dringend zur regelm√§√üigen Teilnahme ! F√ºr meine eigene √úbersicht wird eine Anwesenheitsliste gef√ºhrt!"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin1.html#studienleistung",
    "href": "cogmod_sem_sose23/Termin1.html#studienleistung",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Studienleistung",
    "text": "Studienleistung\nDie Studienleistung f√ºr das Seminar besteht in der w√∂chentlichen Abgabe von Aufgaben, die sich auf den behandelten Stoff beziehen. Die Aufgaben werden √ºber Assignments in Teams abgegeben und sind jeweils bis Montags 12:00 Uhr vor dem jeweilig n√§chsten Seminar f√§llig. Am Ende des Semesters wird dann ein Portfolio mit allen Aufgaben abgegeben, was die Gesamtstudienleistung darstellt. Es m√ºssen zum bestehen alle Aufgaben eingereicht werden !\nDie Aufgaben werden mit fortschreitendem Semester anspruchsvoller und k√∂nnen folgendes umfassen:\n\nFragen zum Stoff\nProgrammieraufgaben in R\nDatenanalyse und Modellierung mit R"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin1.html#teams-channel",
    "href": "cogmod_sem_sose23/Termin1.html#teams-channel",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Teams Channel ",
    "text": "Teams Channel \nWir werden ausschlie√ülich mit Teams und dieser Website arbeiten ! Alle wichtigen Ank√ºndigungen werden im Teams Channel gemacht, daher ist es wichtig regelem√§√üig dort hineinzuschauen. Auf dieser Website werden kurze Zusammenfassungen und weitere Materialien (Slides, Literatur, Skripte und Videos) f√ºr die Seminareinheiten zur Verf√ºgung gestellt, sodass ihr alles aus einer Hand nacharbeiten k√∂nnt.\n\nFahrplan\n\n\nFahrplan\n\n\n\nKW\nThema Seminar\nLink\nMaterialien\n\n\n\n\n16\nOrganisation und Ablauf\nZusammenfassung\n\n\n\n17\nEinf√ºhrung I: Grundlagen der Modellierung\n\n \n\n\n19\nEinf√ºhrung II: Grundlagen der Modellierung\n\n  \n\n\n20\nParametersch√§tzung I: Diskrepanzfunktionen & Sch√§tzalgorithmen\n\n  \n\n\n21\nParametersch√§tzung II: Maximum Likelihood & Beyond\n\n\n\n\n23\nParametersch√§tzung III: Hands On in R Parameter Estimation\n\n\n\n\n24\nAdvanced R for Cognitive Modeling\n\n\n\n\n25\nMultinomial Processing Tree Models (Theorie)\n\n\n\n\n26\nAnwendung von MPT Modellen (R-Sitzung)\n\n\n\n\n27\nDrift Diffusion Models (Theory)\n\n\n\n\n28\nMemory Measurement Model (M3)\n\n\n\n\n29\nAnwendung des M3 Modells\n\n\n\n\n30\nAbgabe des kompletten Portfolios"
  },
  {
    "objectID": "cogmod_sem_sose23/ML_Workshop.html",
    "href": "cogmod_sem_sose23/ML_Workshop.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "",
    "text": "Im letzten Seminar haben wir sehr ausf√ºhrlich √ºber Maximum Likelihood Estimation (MLE) gesprochen. Heute werden wir einige √úbungen dazu in R programmieren, um ein besseres Verst√§ndnis f√ºr diese Methode zu entwickeln.\nDas Grundprinzip der Maximum-Likelihood-Sch√§tzung besteht darin, die Parameter einer statistischen Verteilung so zu bestimmen, dass die Wahrscheinlichkeit, die beobachteten Daten gegeben bestimmter Parameterwerte, maximiert wird. Gegeben eine Verteilungsfunktion \\(f(x;\\theta)\\), wobei \\(x\\) die beobachteten Daten und \\(\\theta\\) die unbekannten Parameter sind, wird die Likelihood-Funktion definiert als \\[L(\\theta|x)=\\prod_{i=1}^{n} f(x_i;\\theta)\\], wobei \\(n\\) die Anzahl der Datenpunkte ist.\nDas Maximum-Likelihood-Sch√§tzverfahren besteht darin, die Werte von \\(\\theta\\) zu finden, die die Likelihood-Funktion maximieren. Dies kann durch Maximierung des Logarithmus der Likelihood-Funktion mathematisch vereinfacht werden, daher wird oftmals die Log-Likelihood Funktion maximiert und anstelle des Produktes, die Summe √ºber alle Funktionswerte gebildet:\n\\[\\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log f(x_i;\\theta)\\]\n\n\nNehmen wir an, wir haben an einer Sttichprobe die Intelligenz·∫Éerte erhoben und m√∂chten nun den Mittelwert des IQs anhand der Daten mit MLE sch√§tzen. Hierzu brauchen wir zun√§chst eine Dichtefunktion, √ºber die wir die Likelihood berechnen k√∂nnen. Da der IQ in der Population normalverteilt ist k√∂nnen wir hierf√ºr die Normaverteilung heranziehen um eine Likelihoodfunktion zu definieren:\n\\[f(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\nalso ist die Likelihoodfunktion gegeben als\n\\[L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\n\\] bzw. vereinfacht sich zu folgender Formel, wenn wir den Logarhitmus nehmen:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]"
  },
  {
    "objectID": "cogmod_sem_sose23/ML_Workshop.html#einf√ºhrung",
    "href": "cogmod_sem_sose23/ML_Workshop.html#einf√ºhrung",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "",
    "text": "Im letzten Seminar haben wir sehr ausf√ºhrlich √ºber Maximum Likelihood Estimation (MLE) gesprochen. Heute werden wir einige √úbungen dazu in R programmieren, um ein besseres Verst√§ndnis f√ºr diese Methode zu entwickeln.\nDas Grundprinzip der Maximum-Likelihood-Sch√§tzung besteht darin, die Parameter einer statistischen Verteilung so zu bestimmen, dass die Wahrscheinlichkeit, die beobachteten Daten gegeben bestimmter Parameterwerte, maximiert wird. Gegeben eine Verteilungsfunktion \\(f(x;\\theta)\\), wobei \\(x\\) die beobachteten Daten und \\(\\theta\\) die unbekannten Parameter sind, wird die Likelihood-Funktion definiert als \\[L(\\theta|x)=\\prod_{i=1}^{n} f(x_i;\\theta)\\], wobei \\(n\\) die Anzahl der Datenpunkte ist.\nDas Maximum-Likelihood-Sch√§tzverfahren besteht darin, die Werte von \\(\\theta\\) zu finden, die die Likelihood-Funktion maximieren. Dies kann durch Maximierung des Logarithmus der Likelihood-Funktion mathematisch vereinfacht werden, daher wird oftmals die Log-Likelihood Funktion maximiert und anstelle des Produktes, die Summe √ºber alle Funktionswerte gebildet:\n\\[\\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log f(x_i;\\theta)\\]\n\n\nNehmen wir an, wir haben an einer Sttichprobe die Intelligenz·∫Éerte erhoben und m√∂chten nun den Mittelwert des IQs anhand der Daten mit MLE sch√§tzen. Hierzu brauchen wir zun√§chst eine Dichtefunktion, √ºber die wir die Likelihood berechnen k√∂nnen. Da der IQ in der Population normalverteilt ist k√∂nnen wir hierf√ºr die Normaverteilung heranziehen um eine Likelihoodfunktion zu definieren:\n\\[f(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\nalso ist die Likelihoodfunktion gegeben als\n\\[L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\n\\] bzw. vereinfacht sich zu folgender Formel, wenn wir den Logarhitmus nehmen:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]"
  },
  {
    "objectID": "cogmod_sem_sose23/ML_Workshop.html#mle-in-r",
    "href": "cogmod_sem_sose23/ML_Workshop.html#mle-in-r",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "MLE in R",
    "text": "MLE in R\n\n√úbung 1: MLE by Hand in R\nUm in R mit der Likelihoodfunktion zu arbeiten, m√ºssen wir zun√§chst Daten simulieren. Hierzu benutzen wir die Funktion rnorm(). Bitte nutzt zun√§chste die Hilfefunktion, um mit rnorm() eine Stichproben von 10000 Werten zu genieren (N = 100), mit dem Mittelwert \\(100\\) und einer Standardabweichung von \\(15\\). Speichert den Output in der Variable ‚Äúiq‚Äù ab. Berechnet f√ºr die gezogene Stichprobe separat Mittelwert und Standardabweichung\n\nset.seed(666)\n# Stichprobe von Werten generieren\niq &lt;- rnorm(100,100,15)\n\n# Mittelwert und Standardabweichung berechnen\n\nmean(iq)\n\n[1] 98.99997\n\nsd(iq)\n\n[1] 15.44065\n\n\nZun√§chst wollen wir versuchen, die Likelihoodfunktion in R Code zu √ºbertragen. Nehmt hierzu die Gleichung der Log-Likelihoodfunktion, die wir weiter oben definiert haben:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]\nund dr√ºckt sie in R-Code aus. Ihr ben√∂tigt dazu folgende mathematischen Funktionen:\n\n\n\nFunktion\nCode\n\n\n\n\nSumme Bilden\nsum()\n\n\nQuadrieren\nx^2\n\n\nLogarhitmus\nlog()\n\n\n\\(\\pi\\)\nPi\n\n\nn\nStichprobengr√∂√üe (hier: 100)\n\n\n\nDefiniert zun√§chst zwei Variablen f√ºr unterschiedliche Parametervorschl√§ge f√ºr den Mittelwert von \\(100\\) (=low_iq) und \\(120\\) (high_iq),die Standardabweichung (=sigma) ist f√ºr beide Samples gleich (\\(\\sigma\\) = 15). Als Daten nutzen wir die generierten IQ Werte iq_low und iq_high. In der Gleichung sind \\(x\\) die Daten, also die IQ-Werte aus unserem iq Vektor, \\(\\sigma\\) ist die Standardabweichung und \\(\\mu\\) die unterschiedlichen Vorschl√§ge f√ºr die Mittelwerte, als entweder high_iq oder low_iq. Stellt nun die Gleichung f√ºr beide Parametervorschl√§ge (high vs.¬†low) in R auf. Speichert die Ergebnisse unter den Variablen ll_low und ll_high ab.\n\n# Define sigma and mu\nhigh_iq &lt;- 120\nlow_iq &lt;- 100\nN &lt;- 100\n\nsigma &lt;- 15\n\n\n# Define LL-Equation\nll_low &lt;- -N/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((iq - low_iq)^2)\nll_high &lt;- -N/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((iq - high_iq)^2)\n\nIhr habt nun f√ºr beide Parametervorschl√§ge, also einmal f√ºr einen Mittelwert von 100 und einmal von einem Mittelwert von 120, die log-likelihood f√ºr die vorliegenden Daten berechnet. Welcher Mittelwert ist nach dieser Likelihood unter den gegebenen Daten wahrscheinlicher ?\n\n\nExkurs: Funktionen in R\nFunktionen sind in der Programmierung wichtige Bausteine, um Code wiederverwendbar zu machen und komplexe Aufgaben zu strukturieren. Eine nimmt in der Regel Eingabewerte (Argumente) entgegen, f√ºhrt Operationen oder Berechnungen durch und gibt ein Ergebnis zur√ºck. In R k√∂nnen Funktionen mit dem Schl√ºsselwort function definiert werden. Die Syntax besteht aus dem Funktionsnamen, den Eingabe-Parametern in Klammern, den auszuf√ºhrenden Anweisungen innerhalb des Funktionsk√∂rpers und dem R√ºckgabewert mit return(). Funktionen in R k√∂nnen dann mit den angegebenen Argumenten aufgerufen werden, um den gew√ºnschten Code auszuf√ºhren. Die Verwendung von Funktionen erleichtert das Schreiben, Lesen und Verstehen von Code, da Aufgaben in kleine, wiederverwendbare Einheiten aufgeteilt werden k√∂nnen.\nHier ein einfaches Beispiel einer Funktion, die die Quadratzahl des Inputarguments ausgibt:\n\n# Funktion zur Berechnung der Quadratzahl\nsquare &lt;- function(x) {\n  result &lt;- x^2\n  return(result)\n}\n\n# Verwendung der Funktion\nnum &lt;- 5\nsquared_num &lt;- square(num)\nprint(squared_num)\n\n[1] 25\n\n\nInnerhalb einer Funktion in R kann auf die Input-Argumente zugegriffen werden, indem man ihre Namen verwendet. Die Input-Argumente werden in der Funktion als Parameter definiert. Du kannst diese Parameter dann innerhalb der Funktion verwenden, um auf die √ºbergebenen Werte zuzugreifen und damit Berechnungen oder Operationen durchzuf√ºhren.\nZum Beispiel, wenn wir eine Funktion addition() definieren m√∂chten, die zwei Zahlen addiert, k√∂nnten wir die Input-Argumente a und b verwenden:\n\naddition &lt;- function(a, b) {\n  sum &lt;- a + b\n  return(sum)\n}\n\naddition(2,2)\n\n[1] 4\n\n\n\n\n√úbung 2: MLE Using optim()\n\nDiskrepanzfunktion definieren\nNun haben wir im Prinzip ‚Äúby Hand‚Äù eine MLE Sch√§tzung durchgef√ºhrt - zwar nicht iterativ, denn haben wir haben nur zwei m√∂gliche Parameterwerte im Lichte der gegebenen Daten nach der MLE bewertet! R bietet aber auch die M√∂glichkeit, mit der Funktion optim(), eine SIMPLEX Optimierung nach einer gegebenen Diskrepanzfunktion durchzuf√ºhren. Dazu m√ºssen wir der Funktion allerdings eine Funktion √ºbergeben.\nUm nun die optim() zu nutzen im iterativ Parameter nach MLE zu sch√§tzen und durch simplex zu minimieren, m√ºssen wir die gleiche log-likelihood Funktion von √úbung 1 in eine Funktion √ºberf√ºhren. Das ist ganz einfach, denn wir k√∂nnen uns nun, da wir das Grundprinzip von MLE verstanden haben, das Leben mit der in R verf√ºgbaren dnorm() Funktion erleichtern. Diese berechnet ebenfalls die Wahrscheinlichkeit (genauer die Dichte) f√ºr Datenpunkte, gegeben bestimmter Parameter:\n\n# Unsere Gleichung\nll_low &lt;- -100/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((iq - low_iq)^2) \nprint(ll_low)\n\n[1] -415.3721\n\n# dnorm() aus R\nll_dnorm &lt;- sum(dnorm(iq,mean=low_iq,sd=sigma,T))\nprint(ll_dnorm)\n\n[1] -415.3721\n\n\nEure Aufgabe ist es nun, eine Funktion zu definieren, die sie aufsummierte log-likelihood ausgibt, wenn Ihr Parameterwerte eingebt. Dazu nutzt ihr die folgende Funktionen aus R:\n\n\n\nFunktion\nCode\n\n\n\n\nSumme Bilden\nsum()\n\n\nLikelihood\ndnorm(daten,mean=,sd=, log=TRUE)\n\n\nLogarhitmus\nlog()\n\n\n\nDie Funktion soll folgende Input-Argumente besitzen:\n\nEinen Vektor Daten (unsere IQ Daten sind ein Vektor)\nEinen Vektor theta, der nur zwei Eintr√§ge enth√§lt - theta¬†ist der Argumentname, um innerhalb der Funktion auf den Input zuzugreifen. Was ihr letztendlich der Funktion als theta √ºbergebt, muss nicht theta hei√üen !\n\nWeiterhin soll diese Funktion die aufsummierte log-Likelihood ausgeben. Definiert diese Funktion unter dem Namen MLE. Die Inputargumente m√ºssen wie im oberen Beispiel einer einfachen Funktion nur mit Namen definiert werden, nicht mit Datentyp. Diesen habe ich nur zum Verst√§ndnis mit angegeben.\n2.) M√ºssen folgende Operationen innerhalb der Funktion ausgef√ºhrt werden\n\nBerechnung der Likelihood unter der verwendung von dnorm(). b.) Aufsummierung der berechneten Likelihood mit sum()\nBerechnung der Deviance - hierzu muss die aufsummierte Likelihood mal -2\ngenommen werden.\n\nWir k√∂nnen hierbei auf die Berechnung des Logarhitmus verzichten, da dnorm() schon den die log-Likelihood mit ausgibt. Hierzu muss allerdings das Argument log = TRUE gesetzt werden ! Zur Erinnerung, auf bestimmte Elemente eines Vektors greift ihr folgenderma√üen zu (dies ist wichtig zu wissen, da Ihr mit den Inputwerten innerhalb der Funktion arbeiten m√ºsst):\n\n# Vektor Definieren\nvektor &lt;- c(1,2,3)\n\n# Erstes Element\n\nvektor[1]\n\n[1] 1\n\n# Zweites Element\n\nvektor[2]\n\n[1] 2\n\n# Drittes Element\n\nvektor[3]\n\n[1] 3\n\n\n3.) Es muss mit return() das Endprodukt zur√ºckgeben, wie in den beiden einfachen Beispielen vorher !\n\n### YOUR CODE HERE \n\nMLE &lt;- function(Daten, theta) \n{\n  mu &lt;- theta[1]\n  sigma &lt;- theta[2]\n  \n  LL &lt;- -2*(sum(dnorm(Daten,mean=mu,sd=sigma,log = T)))\n  \n  return(LL)\n  \n}\n\n# Test your function with different values for theta\ntheta &lt;- c(80,20)\nMLE(iq,theta)\n\n[1] 932.1913\n\n\n\n\nOptimieren der Parameter mit optim()\nNun da unsere ML-Diskrepanzfunktion funktioniert, m√ºssen wir diese nat√ºrlich minimieren - dazu k√∂nnen wir die Funktion optim() nutzen, die in R zur Verf√ºgung steht. Mit optim() ist standardm√§√üig der SIMPLEX -Algorithmus eingestellt. Es k√∂nnen aber auch andere Algorhithmen genutzt werden. Es werden der Funktion drei Hauptargumente √ºbergeben:\n\npar - ein Vektor mit den Startwerten der Parametersch√§tzung. Die Startwerte sollten nicht √ºberm√§√üig von den erwarteten Werten abweichen. Sch√§tzt man also einen Mittelwert von IQ Daten, macht es keinen Sinn, den Startwert f√ºr den Mittelwert auf 1 zu setzen, da der ‚Äúwahre Wert‚Äù vermutlich zwischen 75 und 150 liegen wird. Gleiches gilt f√ºr die Standardabweichung.\nfn= - die Diskrepanzfunktion die es zu minimieren gilt. Hier die von uns definierte MLE() Funktion.\nDie Daten - diese √ºbergebt ihr mit dem Namen, den Ihr in eurer Funktion verwendet habt, also hier Daten = iq.\n\nOptimiert nun die definierte Funktion mit optim() und speichert die Ergebnisse im Objekt fit.\n\n# YOUR CODE HERE\n\nfit &lt;- optim(par=c(mu=50,sigma=10), fn =MLE, Daten=iq)\n\nfit$par\n\n      mu    sigma \n99.00413 15.36639 \n\n# Biased Fit Example \n\niq_pop &lt;- rnorm(1000,mean=100,sd=15)\niq_bias &lt;- sample(iq_pop,25)\n\nmean(iq_bias)\n\n[1] 103.8359\n\n# Fitting Sample\nfit &lt;- optim(par=c(mu=50,sigma=10), fn =MLE, Daten=iq_bias)\n\nfit$par\n\n       mu     sigma \n103.84127  13.60439"
  },
  {
    "objectID": "cogmod_sem_sose23/ML_Workshop.html#fazit",
    "href": "cogmod_sem_sose23/ML_Workshop.html#fazit",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Fazit",
    "text": "Fazit\nIn diesem Tutorial haben wir uns mit der Maximum-Likelihood-Sch√§tzung (MLE) in R besch√§ftigt und die Funktion `optim()` verwendet, um die Sch√§tzung durchzuf√ºhren. Zun√§chst haben wir die Likelihood-Funktion selbst definiert und verschiedene Parameterwerte getestet, um das Konzept der MLE besser zu verstehen.\nDurch die Verwendung von `optim()` konnten wir die MLE iterativ implementieren und die optimalen Parameterwerte finden, die die Likelihood-Funktion maximieren (oder die Deviance minimieren). Wir haben gesehen, dass `optim()` eine effiziente Methode zur numerischen Optimierung ist und verschiedene Algorithmen zur Verf√ºgung stellt.\nDie Maximum-Likelihood-Sch√§tzung ist ein leistungsstarkes Werkzeug, um Parameter in statistischen Modellen zu sch√§tzen. Es erm√∂glicht uns, die Wahrscheinlichkeit der beobachteten Daten unter verschiedenen Annahmen zu maximieren und die besten Parameterwerte zu ermitteln."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der M√∂glichkeit Datens√§tze effizient zusammenzufassen noch viele weitere Funktionen, die es erm√∂glichen tiefergehende √Ñnderungen an einem Datensatz vorzunehmen. Dazu geh√∂ren bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abh√§ngigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#fortgeschrittene-dplyr-funktionen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#fortgeschrittene-dplyr-funktionen",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der M√∂glichkeit Datens√§tze effizient zusammenzufassen noch viele weitere Funktionen, die es erm√∂glichen tiefergehende √Ñnderungen an einem Datensatz vorzunehmen. Dazu geh√∂ren bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abh√§ngigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#case_when---if-else-verkn√ºpf√ºngen-f√ºr-multiple-bedingungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#case_when---if-else-verkn√ºpf√ºngen-f√ºr-multiple-bedingungen",
    "title": "Advanced Dplyr",
    "section": "2 case_when() - if-else Verkn√ºpf√ºngen f√ºr multiple Bedingungen",
    "text": "2 case_when() - if-else Verkn√ºpf√ºngen f√ºr multiple Bedingungen\nDie case_when Funktion in dplyr erm√∂glicht es, basierend auf bestimmten Bedingungen verschiedene Werte f√ºr eine Spalte in einem Datensatz auszuw√§hlen und abh√§ngig vom Variablenwert einen neue Variable zu erstellen. Es ist eine sehr einfache M√∂glichkeit, if-else Statements in eine dplyr-Pipeline zu integrieren.\nDie Syntax eines case_when() Befehles entspricht einer einfachen if oder if-else Verkn√ºpfung. Eine if-else Verkn√ºpfung pr√ºft eine Bedingung und f√ºhrt wenn diese Erf√ºllt ist einen definierten Befehl aus. Ist die Bedingung nicht erf√ºllt, wird andernfalls (else) ein anderer Befehl ausgef√ºhrt:\n\n\n# Vektor von Zahlen darauf testen, ob sie gerade oder ungerade sind:\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei geteilt keinen Rest hat, dann schreibe Gerade, ansonsten Ungerade.\n\nifelse(zahlen %% 2 == 0,\"Gerade\",\"Ungerade\")\n\ncase_when() funktioniert nach der Gleichen Logik:\n\n\n\nDabei wird immer zuerst die if-Bedinungen und deren Output definiert und anschlie√üend mit TRUE die else Bedingung, welche angibt was passiert wenn die if - Bedingung nicht zutrifft:\n\n\n\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei\n# geteilt keinen Rest hat, dann schreibe Gerade, \n# ansonsten Ungerade.\n\ncase_when(zahlen %% 2 == 0 ~ \"Gerade\",\n          TRUE ~\"Ungerade\")\n\nDer Unterschied von case_when() und ifelse() ist, dass mit case_when() auch mehrere Bedingungen definiert werden k√∂nnen (z.B. wenn eine Variable in mehrere Kategorien eingeteilt werden soll):\n\nHier gibt es auch ein finales else Statement, dieses muss aber nicht sein, solange die Kategorien alle F√§lle abdecken!\n\n2.0.1 case_when: Beispiele\nDas folgende ist ein einfaches Beispiel, bei dem case_when() verwendet wird, um in einem Datensatz eine neue Spalte Alter_Kategorie anhand des Werts in der Spalte Alter zu kategorisieren. Wenn eine neue Spalte erstellt werden soll, muss case_when() immer mit mutate() kombiniert werden:\n\n# Beispiel Datensatz erstellen\ndf &lt;- data.frame(Name = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45))\nhead(df)\n\n# mutate und case_when verwenden, um neue Variable in Abh√§ngigkeit vom Alter zu erstellen:\ndf %&gt;% \n  mutate(Alter_Kategorie = case_when(\n    Alter &lt;= 25 ~ \"jung\",\n    Alter &gt; 25 & Alter &lt;= 35 ~ \"mittel\",\n    Alter &gt; 35 ~ \"alt\"\n  ))\n\nEin weiteres Beispiel haben wir letzte Woche bei der MPT-Modellierung der Daten von Frenken et al.¬†gesehen. Im Datensatz ist shoot als 0 und not shoot als 1 kodiert. F√ºr die MPT Modellierung ben√∂tigen wir aber f√ºr jede Kategorie (Black / Gun, Black / Phone, White / Gun, White / Phone) die Hits und Misses, also die Fehlerraten. Der Datensatz hat praktischerweise eine Spalte, die genau kodiert, was die VP gesehen hat:\n\n\nhead(Study_2_dm)\n\n# A tibble: 6 √ó 6\n  subj_idx stimulus stim     rt response condition\n     &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt; &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;    \n1        0 gun      bg    0.506        0 black    \n2        0 phone    wp    0.437        0 white    \n3        0 phone    wp    0.483        0 white    \n4        0 phone    bp    0.594        1 black    \n5        0 phone    wp    0.552        1 white    \n6        0 gun      wg    0.417        1 white    \n\n\nIn der zweiten Spalte stimulus ist angebenen, welches Objekt der jeweils zusammen mit der Hautfarbe gezeigt wurde. Da wir wissen, dass nur bei ‚Äúgun‚Äù geschossen werden darf, k√∂nnen wir nun in Kombination mit response (0 - shoot, 1 - not shoot) die Hits mit case_when() in einer neuen Spalte ACC kodieren.\nDazu Verkn√ºpfen wir hier zwei Bedingungen mit dem & Operator. Die erste Bedingung bezieht sich auf die Spalte stimulus, hier muss gepr√ºft werden, welcher Stimulus gezeigt wurde (Gun vs.¬†Phone). Die zweite Bedingung bezieht sich auf die Spalte response. Hier muss gepr√ºft werden, ob geschossen wurde oder nicht. Ingesamt m√ºssen also vier Bedinungen definiert werden, f√ºr jede Kombination von Objekt und Response:\n\nAccuracy in Abh√§ngigkeit von Gegebener Response und Stimulus\n\n\nStimulus\nResponse\nAccuracy\n\n\n\n\nGun\n0\n1\n\n\nGun\n1\n0\n\n\nPhone\n0\n0\n\n\nPhone\n1\n1\n\n\n\nDies m√ºssen wir nun in ein case_when() Befehl √ºbernehmen\n\nMit mutate() die neue Outputspalte benennen\nInnerhalb von mutate() mit case_when() die Accuracy in Abh√§ngigkeit der Spalten Stimulus und Response umkodieren:\n\n\n\nfreq_dat &lt;- Study_2_dm %&gt;% mutate(ACC = case_when(stimulus == \"gun\" & response == 0 ~ 1,\n                                                  stimulus == \"gun\" & response == 1 ~ 0,\n                                                  stimulus == \"phone\" & response == 1 ~ 1,\n                                                  stimulus ==\"phone\" & response == 0 ~ 0))\n\nAnschlie√üend m√ºssen wir nun die Hits und Misses ausz√§hlen. Generell eignen sich zum Ausz√§hlen von bestimmten Bedingungskombinationen oder Trials die Funktionen group_by(), summarise() und n(). n() ist eine einfache Z√§hlfunktion, welche innerhalb von group_by() %&gt;% summarise() dazu f√ºhrt, dass die alle Beobachtungen der gruppierten Variablen (z.B. Subject & Bedingung) innerhalb von summarise() gez√§hlt werden.\nAlso zum Beispiel, wieviele Beobachtung von Subject 1 gibt es in Bedingung A, B und C. Dies macht aber nur Sinn, wenn ihr Daten auf Trial-Ebene (also f√ºr jede Versuchsperson alle Antworten √ºber das ganez Experiment) vorliegen habt. Dies ist hier der Fall, da f√ºr jede Person und jede Bedingung, die diese Person durchlaufen hat, die gegebenen Antworten im Datensatz in der Spalte responses vorliegen.\n\n\nfreq_dat &lt;- freq_dat %&gt;% group_by(subj_idx,stim) %&gt;% \n  summarise(hits = sum(ACC), \n            ntrials=n(), \n            miss=ntrials-hits)\n\n`summarise()` has grouped output by 'subj_idx'. You can override using the\n`.groups` argument.\n\nhead(freq_dat)\n\n# A tibble: 6 √ó 5\n# Groups:   subj_idx [2]\n  subj_idx stim   hits ntrials  miss\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n1        0 bg       30      31     1\n2        0 bp       28      29     1\n3        0 wg       23      27     4\n4        0 wp       28      33     5\n5        1 bg       28      30     2\n6        1 bp       16      18     2\n\n\nWas passiert hier genau ? Schritt f√ºr Schritt:\n\ngroup_by(subj_idx, stim) : Wir gruppieren zun√§chst nach Subject und Bedingung\n\nsummarise(...\n\nhits = sum(ACC) - Wir haben alle Hits mit case_when als 1 kodiert, also k√∂nnen wir die Hits einfach berechnen, indem wir diese aufsummieren.\nnTrials = n() - Ausz√§hlen wieviele Responses ein Subject in jeder stimulus Bedinungen gegeben hat (Gesamtanzahl von gegebenen Antworten in einer Bedingungen pro Person\nmiss = ntrials - hits Alles was kein Hit ist muss ein Miss sein ! Daher k√∂nnen wir einfach die Hits von der Gesamtzahl der Antworten abziehen und erhalten die Anzahl der Misses in jeder Bedinung pro Subject !"
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "title": "Advanced Dplyr",
    "section": "3 Long- und Wide-Datenformat - Datentransformation in dplyr",
    "text": "3 Long- und Wide-Datenformat - Datentransformation in dplyr\nNun haben wir die Daten von Frenken et al.¬†neu kodiert und die n√∂tigen Informationen im Datensatz, um mit MPTinR zu arbeiten. Allerdings liegen die Daten noch im sogenannten long - Format vor. Oftmals ist es jedoch notwendig, Daten entweder von einem long in ein wide Format oder umgekehrt zu transfomieren. Im Kontext von Datenanalyse und statistischer Modellierung gibt es in der Regel zwei Haupttypen von Datenformaten: wide-Format (breit-Format) und long-Format (lang-Format).\n\n3.1 Wide-Format\n\nIn einem wide Format sind die verschiedenen Merkmale einer einzigen Beobachtung in separaten Spalten dargestellt.\nEine einzige Zeile in einem breiten Datensatz repr√§sentiert eine Beobachtung.\nBeispiel: Ein Datensatz, der Informationen √ºber die Leistung von Sch√ºlern in verschiedenen F√§chern (Mathematik, Englisch, Wissenschaft) enth√§lt, w√§re in einem breiten Format dargestellt, wobei jede Spalte einem bestimmten Fach entspricht.\nVorteil: Es ist einfach, schnelle √úbersichten √ºber gro√üe Datenmengen zu erhalten.\n\nBeispiel:\n\nwide_df &lt;- data.frame(Sch√ºler_ID = c(1, 2, 3), \n                      Mathe = c(89, 76, 92), \n                      Englisch = c(92, 88, 95), \n                      Wissenschaft = c(88, 72, 98))\nwide_df\n##   Sch√ºler_ID Mathe Englisch Wissenschaft\n## 1          1    89       92           88\n## 2          2    76       88           72\n## 3          3    92       95           98\n\n\n\n3.2 Long-Format\n\nIm long Format sind alle Merkmale einer einzigen Beobachtung in einer Zeile dargestellt.\nEine einzige Spalte in einem long Datensatz repr√§sentiert ein bestimmtes Merkmal.\nBeispiel: Ein Datensatz, der Informationen √ºber die Leistung von Sch√ºlern in verschiedenen F√§chern (Mathematik, Englisch, Wissenschaft) enth√§lt, k√∂nnte in einem langen Format dargestellt werden, wobei jede Zeile einer bestimmten Sch√ºler-Fach-Kombination entspricht.\nVorteil: Es ist einfach, bestimmte Merkmale f√ºr verschiedene Beobachtungen zu vergleichen oder zu analysieren. Au√üerdem ist es f√ºr manche statistische Methoden, wie lineare Regression oder einer ANOVA, das bevorzugte Format.\n\nBeispiel:\n\nlong_df &lt;- data.frame(Sch√ºler_ID = c(rep(1, 3), rep(2, 3), rep(3, 3)), \n                      Fach = c(rep(\"Mathe\", 3), rep(\"Englisch\", 3), rep(\"Wissenschaft\", 3)), \n                      Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\nlong_df\n##   Sch√ºler_ID         Fach Note\n## 1          1        Mathe   89\n## 2          1        Mathe   92\n## 3          1        Mathe   88\n## 4          2     Englisch   76\n## 5          2     Englisch   88\n## 6          2     Englisch   72\n## 7          3 Wissenschaft   92\n## 8          3 Wissenschaft   95\n## 9          3 Wissenschaft   98\n\nEs ist wichtig, das Konzept des wide- und long-Formats zu verstehen, da es bei der Datenaufbereitung und Analyse eine wichtige Rolle spielt. Zum Beispiel kann ein wide -Datensatz schwer zu analysieren sein, wenn man Vergleiche zwischen bestimmten Merkmalen √ºber mehrere Beobachtungen hinweg machen m√∂chte (Beispiel Ergebnisse der Diffusionsmodellierung!). Hier ist es oft besser, den Datensatz in ein long-format zu bringen.\n\n\n3.3 pivot - Funktionen in dplyr\nDie Funktionen pivot_wider und pivot_longer geh√∂ren zu den Funktionen von dplyr und dienen dazu, Datens√§tze zu transformieren.\npivot_wider verwandelt einen long-Format Datensatz in einen wide-Format Datensatz, indem es die Werte einer bestimmten Spalte zu neuen Spalten umbenennt. Dies kann manuell durchgef√ºhrt werden, aber pivot_wider macht dies automatisch und erleichtert so die Datentransformation:\n\n\npivot_longer verwandelt einen wide-Format Datensatz in einen long-Format Datensatz, indem es die Spalten mit bestimmten Werten in einer neuen Spalte zusammenfasst.\nEin Beispiel f√ºr den Einsatz von pivot_wider:\n\n# long data example\nlong_data &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\", \"John\", \"Jane\", \"Jim\"),\n  subject = c(\"physics\", \"physics\", \"physics\", \"math\", \"math\", \"math\"),\n  score = c(85, 90, 80, 75, 80, 70)\n)\n\nlong_data\n## # A tibble: 6 √ó 3\n##   name  subject score\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 John  physics    85\n## 2 Jane  physics    90\n## 3 Jim   physics    80\n## 4 John  math       75\n## 5 Jane  math       80\n## 6 Jim   math       70\n\n\n# pivot to wide format\npivot_wider(long_data, names_from = subject, values_from = score)\n## # A tibble: 3 √ó 3\n##   name  physics  math\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n## 1 John       85    75\n## 2 Jane       90    80\n## 3 Jim        80    70\n\nEin Beispiel f√ºr den Einsatz von pivot_longer:\n\n# wide data example\nwide_df &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\"),\n  Mathe_Note = c(85, 90, 80),\n  Englisch_Note = c(75, 80, 70)\n)\n\n\nwide_df\n## # A tibble: 3 √ó 3\n##   name  Mathe_Note Englisch_Note\n##   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n## 1 John          85            75\n## 2 Jane          90            80\n## 3 Jim           80            70\n\n\n# pivot to long format\npivot_longer(wide_df, cols = c(Mathe_Note, Englisch_Note), \n             names_to = \"Sch√ºler\", values_to = \"Note\")\n## # A tibble: 6 √ó 3\n##   name  Sch√ºler        Note\n##   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n## 1 John  Mathe_Note       85\n## 2 John  Englisch_Note    75\n## 3 Jane  Mathe_Note       90\n## 4 Jane  Englisch_Note    80\n## 5 Jim   Mathe_Note       80\n## 6 Jim   Englisch_Note    70\n\n\n\n3.4 Argumente f√ºr pivot_longer und pivot_wider\npivot_longer ben√∂tigt mindestens zwei Argumente:\n\ncols: Dies ist ein Zeichenvektor, der angibt, welche Spalten im Datensatz zusammengefasst werden sollen. -Sch√ºler_ID bedeutet zum Beispiel, dass alle Spalten au√üer Sch√ºler_ID zusammengefasst werden sollen.\nnames_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die zusammengefassten Werte gespeichert werden.\nvalues_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die Werte gespeichert werden, die aus den zusammengefassten Spalten stammen.\n\n\npivot_wider ben√∂tigt mindestens zwei Argumente:\n\nnames_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Namen f√ºr die neuen Spalten verwendet werden soll.\nvalues_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Werte f√ºr die neuen Spalten verwendet werden soll.\n\n\nBeide Funktionen haben auch weitere optionale Argumente wie values_fill und names_prefix oder names_sep()um die Daten bei Bedarf weiter anzupassen.\nHier nun die Umformung der Daten von Frenken et al., welche wir vom long Format in das wide Format bringen m√ºssen:\n\n\n#| echo: true\n#| output: true\n#| warning: false\n#| code-overflow: wrap\n#| collapse: true\n\nfreq_dat %&gt;% pivot_wider(names_from = c(\"stim\"),\n                         values_from = c(\"hits\",\"miss\"),\n                         id_cols=\"subj_idx\")\n\n# A tibble: 137 √ó 9\n# Groups:   subj_idx [137]\n   subj_idx hits_bg hits_bp hits_wg hits_wp miss_bg miss_bp miss_wg miss_wp\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1        0      30      28      23      28       1       1       4       5\n 2        1      28      16      27      11       2       2       2       1\n 3        3      25      10      33      23       0       0       1       1\n 4        4      35      24      27      33       1       0       0       0\n 5        5      31      28      25      23       1       2       4       4\n 6        6      34      26      18      31       0       0       5       0\n 7        7      29      24      21      36       2       0       1       1\n 8        8      22      29      25      28       0       2      10       2\n 9        9      28      23      22      22       2       0       2       1\n10       10      26      35      25      24       4       0       0       3\n# ‚Ñπ 127 more rows\n\n\nWas passiert hier genau ? Schritt f√ºr Schritt:\n\nnames_from = c(stim) - Die neuen Spalten sollen aus der Spalte stim benannt werden - (bg,bp,wp,wg)\nvalues_from = c(\"hits\",\"miss\") - in den neuen Spalten sollen die Werte der Hits und Miss Spalten stehen - durch den ersten und zweiten Schritt entstehen also Spalten, die jeweils hits_bg etc. enthalten. Also die Hits aus der Bedingung ‚Äúbg‚Äù usw.\nid_cols - dies soll f√ºr jedes Subject einzeln geschehen."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#√ºbungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#√ºbungen",
    "title": "Advanced Dplyr",
    "section": "4 √úbungen",
    "text": "4 √úbungen\n\n4.1 case_when\n1.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung ‚ÄúNote_Kategorie‚Äù zu erstellen, die ‚ÄúSehr gut‚Äù f√ºr Noten √ºber 90, ‚ÄúGut‚Äù f√ºr Noten zwischen 80 und 90 und ‚ÄúSchlecht‚Äù f√ºr Noten unter 80 angibt.\n\ndf &lt;- data.frame(Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\n# Your Code Here\ndf_categorized &lt;- df %&gt;% \n  mutate(Note_Kategorie = case_when(\n    Note &gt;= 90 ~ \"Sehr gut\",\n    Note &gt;= 80 & Note &lt; 90 ~ \"Gut\",\n    Note &lt; 80 ~ \"Schlecht\"\n  ))\n\n2.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung ‚ÄúBestanden‚Äù zu erstellen, um zu Pr√ºfen ob ein Sch√ºler einer bestimmten Schulform eine Pr√ºfung bestanden hat. Die Bestehensgrenzen sind wie folgt.\n-   F√ºr die Hauptschule liegt die Bestehensgrenze bei 50 %\n-   F√ºr die Realschule liegt die Bestehensgrenze bei 60 %\n-   F√ºr das Gymnasium liegt die Bestehensgrenze bei 70 %\nKodieren Sie das bestehen entweder mit ‚ÄúPass‚Äù oder ‚ÄúFail‚Äù.\n\ndf &lt;- data.frame(Note = runif(100,min=0, max=100),\n                 Schulform = sample(c(\"Gymnasium\",\"Realschule\", \"Hauptschule\"),\n                                    size = 100, replace = T))\n# Your Code here\ndf %&gt;% \n  mutate(Pruefungsergebnis = case_when(\n    Schulform == \"Hauptschule\" & Note &gt;= 50 ~ \"Pass\",\n    Schulform == \"Realschule\" & Note &gt;= 60 ~ \"Pass\",\n    Schulform == \"Gymnasium\" & Note &gt;= 70 ~ \"Pass\",\n    TRUE ~ \"Fail\"\n  ))\n\n3.) Nutzen Sie die Funktion case_when und die dplyr-Library in R, um eine neue Spalte in dem Datensatz ‚Äúdf‚Äù zu erstellen, die die Einkommenskategorie jeder Person basierend auf ihrem Berufsstatus und ihrem Einkommen kategorisiert. Die Einkommenskategorien sollten wie folgt sein:\n\nF√ºr Angestellte mit einem Einkommen von bis zu 50.000: ‚Äúniedrig‚Äù\nF√ºr Angestellte mit einem Einkommen zwischen 50.000 und 75.000: ‚Äúmittel‚Äù\nF√ºr Angestellte mit einem Einkommen √ºber 75.000: ‚Äúhoch‚Äù\nF√ºr Freiberufler mit einem Einkommen von bis zu 60.000: ‚Äúniedrig‚Äù\nF√ºr Freiberufler mit einem Einkommen zwischen 60.000 und 100.000: ‚Äúmittel‚Äù\nF√ºr Freiberufler mit einem Einkommen √ºber 100.000: ‚Äúhoch‚Äù\nF√ºr Ruhest√§ndler mit einem Einkommen von bis zu 30.000: ‚Äúniedrig‚Äù\nF√ºr Ruhest√§ndler mit einem Einkommen √ºber 30.000: ‚Äúmittel_hoch‚Äù\n\n\ndf &lt;- data.frame(ID = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45),\n                 Berufsstatus = c(\"Angestellter\", \"Freiberufler\", \"Ruhest√§ndler\"),\n                 Einkommen = c(45000, 75000, 32000))\n\ndf %&gt;% \n  mutate(Einkommenskategorie = case_when(\n    Berufsstatus == \"Angestellter\" & Einkommen &lt;= 50000 ~ \"niedrig\",\n    Berufsstatus == \"Angestellter\" & Einkommen &gt; 50000 & Einkommen &lt;= 75000 ~ \"mittel\",\n    Berufsstatus == \"Angestellter\" & Einkommen &gt; 75000 ~ \"hoch\",\n    Berufsstatus == \"Freiberufler\" & Einkommen &lt;= 60000 ~ \"niedrig\",\n    Berufsstatus == \"Freiberufler\" & Einkommen &gt; 60000 & Einkommen &lt;= 100000 ~ \"mittel\",\n    Berufsstatus == \"Freiberufler\" & Einkommen &gt; 100000 ~ \"hoch\",\n    Berufsstatus == \"Ruhest√§ndler\" & Einkommen &lt;= 30000 ~ \"niedrig\",\n    Berufsstatus == \"Ruhest√§ndler\" & Einkommen &gt; 30000 ~ \"mittel_hoch\"\n  ))\n##      ID Alter Berufsstatus Einkommen Einkommenskategorie\n## 1 Peter    25 Angestellter     45000             niedrig\n## 2  Anna    35 Freiberufler     75000              mittel\n## 3   Max    45 Ruhest√§ndler     32000         mittel_hoch\n\n\n\n4.2 Pivotting\n1.) Konvertieren Sie die folgenden Datensatz von wide to long. Erstellen Sie aus den Spalten zwei neue Spalten mit den Namen (month und index). Nutzen Sie dazu das Argument names_sep = \"_\". Die Werte sollen in die Spalte ‚ÄúN‚Äù geschrieben werden.\nTip: Sie m√ºssen bei names_to einen Vektor mit den Namen der neuen Spalten angeben.\n\ndf_wide &lt;- data.frame(\n  Jan_sales = c(10, 40, 70),\n  Feb_sales = c(20, 50, 80),\n  Mar_sales = c(30, 60, 90)\n)\n\n# Your Code here\ndf_long &lt;- df_wide %&gt;% pivot_longer(cols=everything(),names_to = c(\"month\",\"index\"), values_to = \"N\",names_sep = \"_\")\n\n2.) Konvertieren Sie die folgendn Datensatz von long in wide.\n\n    df_long &lt;- data.frame(\n      ID = c(\"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\"),\n      month = c(\"Jan\", \"Jan\", \"Jan\", \"Feb\", \"Feb\", \"Feb\", \"Mar\", \"Mar\", \"Mar\"),\n      sales = c(10, 40, 70, 20, 50, 80, 30, 60, 90))\n      \n\n    # Your Code Here\n    df_long %&gt;% pivot_wider(names_from = month, values_from = sales)"
  },
  {
    "objectID": "about/about_jan.html",
    "href": "about/about_jan.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "",
    "text": "\\[\\text{Wiener}(y|\\alpha, \\tau, \\beta, \\delta) =\n\\frac{\\alpha^3}{(y-\\tau)^{3/2}} \\exp \\! \\left(- \\delta \\alpha \\beta -\n\\frac{\\delta^2(y-\\tau)}{2}\\right) \\sum_{k = - \\infty}^{\\infty} (2k +\n\\beta) \\phi \\! \\left(\\frac{2k \\alpha + \\beta}{\\sqrt{y - \\tau}}\\right)\\]\nHier finden Sie alle n√∂tigen Informationen zum Seminar, sowie alle Skripte, Aufgaben und Links zur notwendigen Software."
  },
  {
    "objectID": "about/about_jan.html#allgemeine-materialien-f√ºr-das-seminar",
    "href": "about/about_jan.html#allgemeine-materialien-f√ºr-das-seminar",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Allgemeine Materialien f√ºr das Seminar",
    "text": "Allgemeine Materialien f√ºr das Seminar\n\nWichtige Informationen werden auf dem Teams Channel des Seminars ver√∂ffentlicht:\n\nBeitrittslink zum Teams Channel\nKursmaterialien wie Literatur, R-Skripte und Pr√§sentationen finden Sie hier auf unserer Website\n\nWann: Montags von 16:15 - 17:45 Uhr (17.04.22 - Mo. 17.07.23).\nWo: Raum 01-236 (CIP Pool) im Psychologischen Institut (Binger Str.)"
  },
  {
    "objectID": "about/about_jan.html#inhalt-des-seminars",
    "href": "about/about_jan.html#inhalt-des-seminars",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Inhalt des Seminars",
    "text": "Inhalt des Seminars\n\nMathematische Modelle kognitiver Prozesse sind ein machtvolles Werkzeug, um spezifische kognitive Prozesse wie beispielsweise Verarbeitungsgeschwindigkeit, exekutive Funktionen oder Arbeitsged√§chtniskapazit√§t genauer abzubilden. Die mathematische Formalisierung dieser Prozesse erm√∂glicht es, verbale Theorien in empirisch testbare Modelle zu √ºberf√ºhren, die eine Ableitung und √úberpr√ºfung spezifischer Hypothesen und Vorhersagen √ºber bspw. experimentelle Effekte erm√∂glichen. Ein Beispiel f√ºr ein sehr erfolgreich angewandtes Modell in der kognitiven Forschung ist das Diffusionsmodell, welches die Verarbeitungsgeschwindigkeit von einfachen Wahlreaktionszeitaufgaben modelliert. Das Seminar wird einen √úberblick √ºber die theoretischen Grundlagen und g√§ngigen Modelle zur mathematischen Modellierung verschiedener Arbeitsged√§chtnisprozesse geben. Im praktischen Teil des Seminars werden Anhand unterschiedlicher kognitiver Modelle wie des Diffusionsmodells die Modellimplementierung in R / STAN, die Sch√§tzung anhand empirischer Daten sowie die Bewertung und Interpretation der gesch√§tzten Modellparameter einge√ºbt.\n\n\nFahrplan\n\n\n\nKW\nThema Seminar\nLink\nMaterialien\n\n\n\n\n16\nOrganisation und Ablauf\nZusammenfassung\n\n\n\n17\nEinf√ºhrung I: Grundlagen der Modellierung\n\n \n\n\n19\nEinf√ºhrung II: Grundlagen der Modellierung\n\n  \n\n\n20\nParametersch√§tzung I: Diskrepanzfunktionen & Sch√§tzalgorithmen\n\n  \n\n\n21\nParametersch√§tzung II: Maximum Likelihood & Beyond\n\n \n\n\n23\nParametersch√§tzung III: Hands On in R Parameter Estimation\n\n\n\n\n24\nAdvanced R for Cognitive Modeling\n\n\n\n\n25\nMultinomial Processing Tree Models (Theorie)\n\n\n\n\n26\nAnwendung von MPT Modellen (R-Sitzung)\n\n\n\n\n27\nDrift Diffusion Models (Theory)\n\n\n\n\n28\nMemory Measurement Model (M3)\n\n\n\n\n29\nAnwendung des M3 Modells\n\n\n\n\n30\nAbgabe des kompletten Portfolios"
  },
  {
    "objectID": "about/about_jan.html#begleitendes-tutorium",
    "href": "about/about_jan.html#begleitendes-tutorium",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Begleitendes Tutorium",
    "text": "Begleitendes Tutorium\n\nDas Seminar wird von einem Tutorium (Softwarekurs) begleitet. Im Tutorium werden Sie die Grundlagen des Statistiksoftware und Programmiersprache R. Sie werden in dem Umgang mit Daten geschult und werden verschiedene Techniken zur Datenaufbereitung erlernen und ein√ºben. Dieser Teil des Tutoriums findet im ersten Teil des Semesters statt. Sie sollten f√ºr eines der beiden Tutorien angemeldet sein, entweder am Montag oder am Mittwoch. Besuchen das Tutorium! Erfahrungsgem√§√ü k√∂nnen viele Fragen und Startschwierigkeiten im Umgang mit R im Tutorium gut aufgefangen und gel√∂st werden.\nIm zweiten Teil des Semesters findet ein vertiefendes Tutorium statt. Dieses baut auf den Grundkenntnissen, die Sie im ersten Teil des Semesters erlernt haben werden, auf. Im zweiten Teil des Semesters besuchen Sie dieses Tutorium entweder bei Hr. Alanis oder Fr.¬†H√ºlsemann.Die Termine f√ºr die Tutorien √§ndern sich nicht. Sie m√ºssen das Tutorium nicht wechseln, alle R Inhalte die spezifisch f√ºr das Seminar sind, werden an einem Termin im Seminar behandelt und vertieft. Dieser Termin wir auf den Inhalten der bisherigen Tutorien aufbauen.\nWir haben eine Reihe von Materialien zusammengestellt, um Ihnen den Einstieg in die Programmiersprache R zur erleichtern. Diese k√∂nnen Sie unter dem folgenden Link erreichen:\n\n\n\nR-Kurs-Buch von der Abteilung Analyse und Modellierung komplexer Daten."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lehre@AMD",
    "section": "",
    "text": "Willkommen!\nHerzlich Willkommen zur Lehre-Homepage der Abteilung Analyse und Modellierung komplexer Daten an der Johannes Gutenberg-Universit√§t Mainz.\nAktuell finden Sie hier die Materialien f√ºr zwei Seminare zu fortgeschrittene statistische Methoden II im Sommersemester 23.\n\n\nAktuelle Seminare\n\n\n\n\n\n\n\n\n\n\nSeminar Fortgeschrittene statistische Methoden II (1)\n\n\nTermin 1: Allgemeine Informationen\n\n\n\nJos√© C. Garc√≠a Alanis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeminar Fortgeschrittene statistische Methoden II (3)\n\n\nKognitive Modellierung\n\n\n\nJan G√∂ttmann\n\n\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "mlm_seminar_sose23/hierarchische_daten.html",
    "href": "mlm_seminar_sose23/hierarchische_daten.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Achtung üöß\n\n\n\nDies ist ein ‚Äúlebendiges‚Äù Dokument. Es ist m√∂glich, dass einige Aktualisierungen und Erg√§nzungen nach der Sitzung vorgenommen werden."
  },
  {
    "objectID": "mlm_seminar_sose23/hierarchische_daten.html#ihre-aufgabe",
    "href": "mlm_seminar_sose23/hierarchische_daten.html#ihre-aufgabe",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Ihre Aufgabe",
    "text": "Ihre Aufgabe\n\nFinden Sie sich in Ihren Gruppen zusammen und √ºberlegen Sie sich ein passendes Beispiel.\n\n\n\nWo k√∂nnten hierarchische Daten in Ihrem Bereich vorkommen?\nWie w√ºrden diese aussehen?\n\nWie viele Ebenen w√§ren angemessen, um einen repr√§sentativen Eindruck der Variation zwischen den Beobachtungen zu erhalten?\n\nWelche Probleme w√ºrden sich ergeben, wenn Sie eine oder mehrere Beobachtungsebenen au√üer Acht lassen w√ºrden?\n\nAuf welcher Ebene findet die meiste Variation statt?\n\nZeichnen Sie ein Bild der hierarchischen Datenstruktur.\n\n\n\nNutzen Sie diese Anregungen, um in Ihrer Gruppe das Konzept der hierarchischen Daten besser zu verstehen und auf Ihre spezifischen Anwendungsf√§lle anzuwenden. Diskutieren Sie die verschiedenen Aspekte und identifizieren Sie m√∂gliche Herausforderungen, die sich aus der Verwendung hierarchischer Daten in Ihrem Kontext ergeben k√∂nnten.\n\n\n\n\n‚àí+\n20:00"
  }
]