[
  {
    "objectID": "scripts/ddm-workshop/DDM_Workshop_solution.html",
    "href": "scripts/ddm-workshop/DDM_Workshop_solution.html",
    "title": "Workshop Diffusion-Modeling",
    "section": "",
    "text": "Heute werden wir uns mit der praktischen Anwendung des Drift-Diffusion Models (DDM) beschäftigen. Hierzu werden wir die Modellierung von Frenken et al. (2022) replizieren. Diese haben das DDM dazu verwendet, um die dem “Shooter-Bias” zu Grunde liegenden kognitiven Prozessen zu untersuchen. Hierzu werden wir fast-dm30 (Voss & Voss, 2007) nutzen, welches es ermöglicht das DDM mit Hilfe der ML-Diskrepanzfunktion zu schätzen. Ihr findet fast-dm auch im Teams Ordner des heutigen Termins.\n\n\nWie schon in der letzten Sitzung vorgestellt handelt es sich beim beim First Person Shooter Task (FPST) um ein Paradigma, mit dem Stereotype und deren Auswirkungen auf Entscheidungen untersucht werden können. Hierbei werden üblicherweise verschiedene Ethnien (z.B. schwarze oder weiße Personen), entweder mit einer Waffe (threat) oder einem ungefährlichem Objekt (z.B. Telefon, harmless) gezeigt. Die Versuchspersonen werden instruiert unabhängig von der Hautdfarbe so schnell und korrekt wie möglich auf bewaffnete Ziele zu schiessen. Im Gegensatz dazu soll nicht auf unbewaffnete Ziele geschossen werden.\n\n\n\nFirst Person Shooter Task Paradigm\n\n\nÜblichweise findet man einen sog. Racial Bias, da die typischerweise Reaktionzeiten kürzer für bewaffnete, und länger für unbewaffnete Schwarze sind. Zudem zeigen Befunde, dass bei Schwarzen die Schwelle zum Schuss generell geringer ist, also eher liberaler entschieden wird (z.B. mehr inkorrekte Entscheidungen bei Schwarzen). Es gibt also eine signifikante Interaktion zwischen der Hautfarbe / Ethnie und dem tragen einer Waffe auf die Reaktionszeiten. Dies zeigt sich vorallem in Laienstichproben, wohingegen in Stichproben von Polizeibeamten die Befunde nicht ganz eindeutig sind.\n\n\n\nFrenken et al. (2022) nehmen an, dass dieser Racial-Bias möglichweise durch drei unterschiedliche Mechanismen zustande kommt, die sich durch das Diffusionsmodell trennen lassen.\n\nErstens durch sog. frequency stereotypes, also der Annahme, dass Schwarze beispielsweise häufiger Waffen tragen oder eine Gefahr darstellen. Dies führt in der Folge zu einer gesteigerten Bereitschaft zu Schießen. Es besteht also ein Bias in Richtung der Entscheidung zu Schießen, da eine implizite Korrelation von Schwarzen und Waffen oder Gefahren angenommen wird. Diese frequency hypothesis würde sich im FPST in einem Bias des Startpunktes z des Diffusionsmodelles auswirken, da schon eine a priori Tendenz zum Schießen bei schwarzen Targets besteht, unabhängig vom Objekt (threat vs. harmless).\nZweitens, durch die typicality hypothesis welche animmt, dass Schwarze auf einem konzeptuellen Level typischerweise als gefährlicher wahrgenommen werden. Hierbei wird die Waffe selbst und die schwarze Person unabhängig voneinander als Gefahr wahrgenommen. Dies führt zu einer Aktivierung sich überlappender konzeptueller kognitiver Cluster. Sind diese kongruent, so wird die Aufnahme von relevanten Informationen beschleunigt. Bei inkongruenten Clustern wird diese verlangsamt. Dies hätte keinen Einfluss auf einen a priori Bias, sondern direkt auf die Akkumulation taskrelevanter Informationen. Dies würde sich im Diffusionsmodell auf die Driftrate v auswirken. Hier würde man einen signifikanten Interaktionseffekt zwischen Hautfarbe und Objekt auf die Driftrate v erwarten.\nDrittens könnte der Effekt auch durch einen response execution bias erklärt werden. Hier würden sich Stereotype auf die nicht entscheidungsrelevanten Prozesse, wie die Motorreaktion, auswirken und möglichweise die Vorbereitung dieser Reaktionen initiieren. Dies könnten zum Beispiel ein festerer Druck auf den Trigger der Waffe beim Anblick einer schwarzen Person sein. Dieser Effekt würde sich als Interaktion von Hautfarbe und Objekt auf der non-decision time t0 des DDMs zeigen, da anzunehmen ist, dass eine Waffe die gleichen Prozesse initiiert.\n\n\n\nBei Diffusionmodellierungen empfiehlt sich meist die folgende Vorgehensweise, um ein komplettes Bild der Daten zu erhalten:\n\nAnalyse und Interpretation der Reaktionszeiten und Fehlerraten\nErstellen von einzelnen .dat - files für fast-dm\nSpezifizieren der control-file (Modellspezifikation) für fast-dm\nModellschätzung\nErgebnisdatensatz für die Analyse säubern und vorbereiten\nModellvergleich mit dem Baseline-Modell\nAnalyse und Interpretation der Modellparameter des Modelles\n\nIm heutigen Workshop werden wir in dieser Reihenfolge vorgehen. Zu jedem Schritt werdet ihr Code entweder selbst schreiben, oder ergänzen müssen. Wir werden zu einzelnen Punkten auch offene Fragen und Interpretationen diskutieren.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Diffusion Modeling",
      "Workshop Diffusion Modeling"
    ]
  },
  {
    "objectID": "scripts/ddm-workshop/DDM_Workshop_solution.html#der-shooter---bias",
    "href": "scripts/ddm-workshop/DDM_Workshop_solution.html#der-shooter---bias",
    "title": "Workshop Diffusion-Modeling",
    "section": "",
    "text": "Wie schon in der letzten Sitzung vorgestellt handelt es sich beim beim First Person Shooter Task (FPST) um ein Paradigma, mit dem Stereotype und deren Auswirkungen auf Entscheidungen untersucht werden können. Hierbei werden üblicherweise verschiedene Ethnien (z.B. schwarze oder weiße Personen), entweder mit einer Waffe (threat) oder einem ungefährlichem Objekt (z.B. Telefon, harmless) gezeigt. Die Versuchspersonen werden instruiert unabhängig von der Hautdfarbe so schnell und korrekt wie möglich auf bewaffnete Ziele zu schiessen. Im Gegensatz dazu soll nicht auf unbewaffnete Ziele geschossen werden.\n\n\n\nFirst Person Shooter Task Paradigm\n\n\nÜblichweise findet man einen sog. Racial Bias, da die typischerweise Reaktionzeiten kürzer für bewaffnete, und länger für unbewaffnete Schwarze sind. Zudem zeigen Befunde, dass bei Schwarzen die Schwelle zum Schuss generell geringer ist, also eher liberaler entschieden wird (z.B. mehr inkorrekte Entscheidungen bei Schwarzen). Es gibt also eine signifikante Interaktion zwischen der Hautfarbe / Ethnie und dem tragen einer Waffe auf die Reaktionszeiten. Dies zeigt sich vorallem in Laienstichproben, wohingegen in Stichproben von Polizeibeamten die Befunde nicht ganz eindeutig sind.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Diffusion Modeling",
      "Workshop Diffusion Modeling"
    ]
  },
  {
    "objectID": "scripts/ddm-workshop/DDM_Workshop_solution.html#hypothesen",
    "href": "scripts/ddm-workshop/DDM_Workshop_solution.html#hypothesen",
    "title": "Workshop Diffusion-Modeling",
    "section": "",
    "text": "Frenken et al. (2022) nehmen an, dass dieser Racial-Bias möglichweise durch drei unterschiedliche Mechanismen zustande kommt, die sich durch das Diffusionsmodell trennen lassen.\n\nErstens durch sog. frequency stereotypes, also der Annahme, dass Schwarze beispielsweise häufiger Waffen tragen oder eine Gefahr darstellen. Dies führt in der Folge zu einer gesteigerten Bereitschaft zu Schießen. Es besteht also ein Bias in Richtung der Entscheidung zu Schießen, da eine implizite Korrelation von Schwarzen und Waffen oder Gefahren angenommen wird. Diese frequency hypothesis würde sich im FPST in einem Bias des Startpunktes z des Diffusionsmodelles auswirken, da schon eine a priori Tendenz zum Schießen bei schwarzen Targets besteht, unabhängig vom Objekt (threat vs. harmless).\nZweitens, durch die typicality hypothesis welche animmt, dass Schwarze auf einem konzeptuellen Level typischerweise als gefährlicher wahrgenommen werden. Hierbei wird die Waffe selbst und die schwarze Person unabhängig voneinander als Gefahr wahrgenommen. Dies führt zu einer Aktivierung sich überlappender konzeptueller kognitiver Cluster. Sind diese kongruent, so wird die Aufnahme von relevanten Informationen beschleunigt. Bei inkongruenten Clustern wird diese verlangsamt. Dies hätte keinen Einfluss auf einen a priori Bias, sondern direkt auf die Akkumulation taskrelevanter Informationen. Dies würde sich im Diffusionsmodell auf die Driftrate v auswirken. Hier würde man einen signifikanten Interaktionseffekt zwischen Hautfarbe und Objekt auf die Driftrate v erwarten.\nDrittens könnte der Effekt auch durch einen response execution bias erklärt werden. Hier würden sich Stereotype auf die nicht entscheidungsrelevanten Prozesse, wie die Motorreaktion, auswirken und möglichweise die Vorbereitung dieser Reaktionen initiieren. Dies könnten zum Beispiel ein festerer Druck auf den Trigger der Waffe beim Anblick einer schwarzen Person sein. Dieser Effekt würde sich als Interaktion von Hautfarbe und Objekt auf der non-decision time t0 des DDMs zeigen, da anzunehmen ist, dass eine Waffe die gleichen Prozesse initiiert.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Diffusion Modeling",
      "Workshop Diffusion Modeling"
    ]
  },
  {
    "objectID": "scripts/ddm-workshop/DDM_Workshop_solution.html#generelles-vorgehen",
    "href": "scripts/ddm-workshop/DDM_Workshop_solution.html#generelles-vorgehen",
    "title": "Workshop Diffusion-Modeling",
    "section": "",
    "text": "Bei Diffusionmodellierungen empfiehlt sich meist die folgende Vorgehensweise, um ein komplettes Bild der Daten zu erhalten:\n\nAnalyse und Interpretation der Reaktionszeiten und Fehlerraten\nErstellen von einzelnen .dat - files für fast-dm\nSpezifizieren der control-file (Modellspezifikation) für fast-dm\nModellschätzung\nErgebnisdatensatz für die Analyse säubern und vorbereiten\nModellvergleich mit dem Baseline-Modell\nAnalyse und Interpretation der Modellparameter des Modelles\n\nIm heutigen Workshop werden wir in dieser Reihenfolge vorgehen. Zu jedem Schritt werdet ihr Code entweder selbst schreiben, oder ergänzen müssen. Wir werden zu einzelnen Punkten auch offene Fragen und Interpretationen diskutieren.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Diffusion Modeling",
      "Workshop Diffusion Modeling"
    ]
  },
  {
    "objectID": "scripts/ddm-workshop/DDM_Workshop_solution.html#deskriptive-analyse-der-fehlleraten-und-reaktionszeiten",
    "href": "scripts/ddm-workshop/DDM_Workshop_solution.html#deskriptive-analyse-der-fehlleraten-und-reaktionszeiten",
    "title": "Workshop Diffusion-Modeling",
    "section": "2.1 Deskriptive Analyse der Fehlleraten und Reaktionszeiten",
    "text": "2.1 Deskriptive Analyse der Fehlleraten und Reaktionszeiten\nHier seht ihr ein Balkendiagramm der Fehlerraten aus den unterschiedlichen Bedinungen:\n\n\n\nErstellt nun einen Datensatz desc_rt mit den mittleren Reaktionszeiten für jede Bedingungskonstellation von Hautfarbe und Objekt. Plottet anschließend die Werte in einem Barplot analog zu den Fehlerraten für die jeweiligen Bedingungskombinationen. Dazu kann geom_bar() verwendet werden.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Diffusion Modeling",
      "Workshop Diffusion Modeling"
    ]
  },
  {
    "objectID": "scripts/ddm-workshop/DDM_Workshop_solution.html#deskriptive-analyse-der-reaktionszeiten",
    "href": "scripts/ddm-workshop/DDM_Workshop_solution.html#deskriptive-analyse-der-reaktionszeiten",
    "title": "Workshop Diffusion-Modeling",
    "section": "2.2 Deskriptive Analyse der Reaktionszeiten",
    "text": "2.2 Deskriptive Analyse der Reaktionszeiten\n\n# Create summary dataset\n# YOUR CODE HERE\ndesc_rt &lt;- dm_final %&gt;% group_by(skin_color,object) %&gt;% \n  summarise(meanRT = mean(rt))\n# Create Bar Plot \n# RT \n\nggplot(desc_rt,aes(x=factor(object,levels = c(\"phone\",\"gun\")),y=meanRT,fill=factor(skin_color,levels = c(\"white\",\"black\")))) + \n  geom_bar(stat=\"identity\", position = position_dodge())+ \n  scale_y_continuous(breaks = waiver()) + theme_apa() + scale_fill_d3(name=\"Skin Color\") +\n  geom_errorbar(aes(ymin=meanRT - 1.96*se(meanRT),ymax=meanRT+1.96*se(meanRT),width=0.2),position = position_dodge(0.75)) + \n  labs(x=\"Object\",y=\"mean Reaction Time [ms]\")\n\n\n\n\n\n\n\n# Illustration of Interaction effect. \nfit_rt&lt;-aov_ez(data = dm_final, id = \"ID\", within = c(\"skin_color\",\"object\"), dv = \"rt\")\n\nmeans &lt;- emmeans(fit_rt, specs = ~skin_color*object)\n\npairs(means)\n\n contrast                  estimate      SE  df t.ratio p.value\n black gun - white gun      -0.0335 0.00159 136 -21.022  &lt;.0001\n black gun - black phone    -0.1198 0.00323 136 -37.049  &lt;.0001\n black gun - white phone    -0.0972 0.00288 136 -33.735  &lt;.0001\n white gun - black phone    -0.0863 0.00293 136 -29.436  &lt;.0001\n white gun - white phone    -0.0637 0.00266 136 -23.980  &lt;.0001\n black phone - white phone   0.0226 0.00214 136  10.566  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nZeigen die Reaktionszeit und Fehllerraten den erwarteten “Shooter-Bias” und wie äußert sich\ndieser?",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Diffusion Modeling",
      "Workshop Diffusion Modeling"
    ]
  },
  {
    "objectID": "scripts/ddm-workshop/DDM_Workshop_solution.html#vorbereitung-von-fast-dm",
    "href": "scripts/ddm-workshop/DDM_Workshop_solution.html#vorbereitung-von-fast-dm",
    "title": "Workshop Diffusion-Modeling",
    "section": "3.1 Vorbereitung von fast-dm",
    "text": "3.1 Vorbereitung von fast-dm\n\n3.1.1 Erstellen von einzelnen .dat - files für fast-dm\nFast-dm benötigt zum schätzen der Parameter eine Datei für jede einzelne VP, die alle Trialdaten für diese VP enthält. Um dies zu tun, nutzen wir dplyr um zunächst die nötigen Informationen aus dem Datensatz zu extrahieren und diese dann in einer neuen Datei für jede einzelnen Versuchperson zu speichern . Dazu können wir die Funktion group_split(subject) in dplyr nutzen, die für jede Beobachtung einen eigenen Datensatz erstellt und dann alle einzeln in einem Listenobjekt speichert.\n\n# Funktioniert nur, wenn ihr dm_final in entsprechender Struktur erstellt habt! \n\nvp_list &lt;- dm_final %&gt;% \n  select(\"ID\",\"skin_color\", \"object\", \"response\", \"rt\") %&gt;% \n  group_split(ID) \nmapply(write.table, vp_list, row.names = FALSE ,\n       col.names=FALSE,USE.NAMES = TRUE,\n       file=paste0(seq(1:length(unique(dm_final$ID))), '.dat'))\n\n\n\n3.1.2 Spezifizieren der Control-File in fast-dm\nBevor wir das Modell nun fitten können, müssen wir ein controle-file erstellen, die das Modell spezifiziert das gefittet werden soll. Das umfasst die Definition der\n\nfreien Parameter\nfixierten Parameter\nSchätzmethode (Diskrepanzfuntion)\nDatenformat\nVP - Datenfiles\nOutputfiles\nprecision (wichtiger Parameter, default Wert ist 3, sollte aber mindestens auf 4 stehen !)\n\nFür die Diskrepanzfunktion können neben Maximum-Likelihood (ML) auch Chi-Square oder Kolmogorov-Smirnov gewählt werden. Diese haben andere statistische Eigenschaften, erfordern jedoch mehr Trials. Generell wird ML bei wenigen Trials als Schätzmethode empfohlen, zudem ermöglicht ML die Berechnung der unterschiedlichen Informationskriterien zur Bewertung des relativen Modelfits (Akaike Information Criterion, Bayesian Information Criterion).\nFolgender Code dient für euch als Vorlage, zum erstellen einer Control-File, wenn ihr die entsprechenden Modifikationen vorgenommen habt, könnt ihr hiermit die Datei in eurem Working Directory erstellen!\n\ncat(c(\n  \"precision \",3, \"\\n\", #Precision of Parameter Estimation \n  \"method \", \"\", \"\\n\", # Optimization Criterion (Maximum Likelihood)\n  \"set \", \"\\n\",\n  \"set\", \"\\n\",\n  \"set\", \"\\n\",\n  \"set\", \"\\n\",#Fixation of Parameters to fixed values\n  \"depends \",\"\\n\",\n  \"depends \" ,\"\\n\",\n  \"depends  \",\"\\n\",\n  \"depends \" ,\"\\n\",\n  \"format \", \"\\n\",\n  \"load *.dat\", \"\\n\",\n  \"log \", \"\\n\"),\n  file=paste(\"shooter_dm.ctl\",sep=\"\"), sep=\"\", fill=F)\n\ncat(c(\n  \"precision \", 3, \"\\n\", #Precision of Parameter Estimation \n  \"method \", \"ml\", \"\\n\", # Optimization Criterion (Maximum Likelihood)\n  \"set \", \"\\n\",\n  \"set \", \"\\n\",\n  \"set \", \"\\n\",\n  \"set\", \"\\n\",#Fixation of Parameters to fixed values\n# Vary over which conditions (one line for each parameter)\n  \"format \", \"\\n\",\n  \"load *.dat\", \"\\n\",\n  \"log \", \"\\n\"),\n  file=paste(\"shooter_dm_baseline.ctl\",sep=\"\"), sep=\"\", fill=F)\n\nBitte schaut euch die unten stehende Tabelle an und sepzifiziert zwei Modelle:\n\nEin Modell, welches die Parameter über unterschiedliche experimentelle Manipulationen variieren lässt:\n\n\nSchätzen der Drift-Rate (v) und non-decision time in Abhängigkeit von Hautfarbe & Objekt\nSchätzen des Startpunktes (z) und der threshold separation (a) in Abhängigkeit der Hautfarbe\nFixiert die parameter szr, sv, d, p auf 0\nspeichert die Ergebnisse in der Datei “shooter_dm.txt”\nspeichtert die ctl-Datei unter “shooter_dm.ctl”\n\n\nEin Modell, welches die Parameter unabhängig von den experimentellen Manipulationen schätzt. Dies entspricht einem Modell, welches keine Effekte der Manipulationen annimmt und die Parameter unabhängig von diesen schätzt. Tip: Hier müssen im Vergleich zum vollen Modell nur Zeilen weggelassen werden.\n\n\nspeichtert die ctl-Datei unter shooter_dm.ctl\nspeichert die Ergebnisse in der Datei shooter_dm_baseline.txt\n\nHier findet ihr nochmals eine Erläuterung der unterschiedlichen Optionen, die in der .ctl file gesetzt werden müssen:\n\n\n\nControl File Spezifikationen",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Diffusion Modeling",
      "Workshop Diffusion Modeling"
    ]
  },
  {
    "objectID": "scripts/ddm-workshop/DDM_Workshop_solution.html#modellschätzung",
    "href": "scripts/ddm-workshop/DDM_Workshop_solution.html#modellschätzung",
    "title": "Workshop Diffusion-Modeling",
    "section": "3.2 Modellschätzung",
    "text": "3.2 Modellschätzung\nWenn alles korrekt spezifiziert ist, können wir die Modelle nun schätzen. Hierzu müssen die einzelnen .dat- files der Versuchpersonen im Working Directory sein. Ist das der Fall, könnt ihr fast-dm30.2 wie folgt aus über die R-Console aufrufen:\n\n# Run fast-dm and estimate parameters\n# Check fast-dm directory beforehand\n\n# Windows\n# system(\"./fast-dm30.2/fast-dm shooter_dm.ctl\")\n# system(\"./fast-dm30.2/fast-dm shooter_dm_baseline.ctl\")\n\n#MacOS\nsystem(\"./fast-dm-mac/fast-dm shooter_dm.ctl\")\nsystem(\"./fast-dm-mac/fast-dm shooter_dm_baseline.ctl\")\n\nFunktioniert alles, solltet einen solchen Output in der Konsole sehen:\n\n a, 0.5, v_3, t0, 0, 0, 0, st0, 0 (72+0 samples)\n  ... -LL = 118.706\n  ... -LL = 113.164\n  ... -LL = 113.163\n  -&gt; a = 2.270632\n  -&gt; v_7 = 1.066865\n  -&gt; t0 = 0.212743\n  -&gt; st0 = 0.000081\n  -&gt; v_5 = 1.484960\n  -&gt; v_3 = 2.230094\ndataset 26.dat:\n  a, 0.5, v_7, t0, 0, 0, 0, st0, 0 (50+17 samples)\n  a, 0.5, v_3, t0, 0, 0, 0, st0, 0 (69+3 samples)\n  a, 0.5, v_5, t0, 0, 0, 0, st0, 0 (66+3 samples)\n  ... -LL = 147.232\n  ... -LL = 137.997\n  ... -LL = 137.997\n  -&gt; a = 2.065754\n  -&gt; v_7 = 0.421251\n  -&gt; t0 = 0.188052\n  -&gt; st0 = 0.000299\n  -&gt; v_3 = 2.409830\n  -&gt; v_5 = 1.633827",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Diffusion Modeling",
      "Workshop Diffusion Modeling"
    ]
  },
  {
    "objectID": "scripts/ddm-workshop/DDM_Workshop_solution.html#modellvergleich-mit-dem-baseline-modell",
    "href": "scripts/ddm-workshop/DDM_Workshop_solution.html#modellvergleich-mit-dem-baseline-modell",
    "title": "Workshop Diffusion-Modeling",
    "section": "4.1 Modellvergleich mit dem Baseline-Modell",
    "text": "4.1 Modellvergleich mit dem Baseline-Modell\nBevor wir mit der Anaylse beginnen, müssen wir zunächst prüfen, ob das Modell dass die Parameter über die experimentallen Bedingungen variiert, einen besseren Fit aufweißt, als das Baseline-Modell. Dieses Vorgehen dient hier als “Sanity-Check”. Solltet ihr mehrere Modelle gegeneinander testen, ist das Vorgehen das gleiche !\nBei explorativem Vorgehen sollten immer mehrere Modelle, die unterschiedliche Parameter variieren lassen, geschätzt und gegenübergestellt werden (model comparison approach). Geht man so vor, berechnet man den relativen Modelfit. Dieser wird üblicherweise mit dem Akaike- (AIC) und Baysian Information Criterium (BIC) berechnet (Akaike,1974; Schwarz, 1978).\nWenn hypothesengeleitet ein bestimmtes Modell angenommen und nur dieses gefittet wird, dann berechnet man den absoluten Modelfit, indem man Daten aus den geschätzen Parametern simuliert und diese mit den emprischen Daten korreliert. Das haben die Autoren getan, ich habe mich hier aber dazu entschieden, einen relativen Modelfit zu berechnen, um euch dieses - sehr häufige - Vorgehen zu erläutern.\nWichtig beim relativen Modelfit ist, dass es stets auch ein Baseline-Modell gibt, in dem alle Parameter frei geschätzt werden. Dies dient als Referenzmodell, welches annimmt, das die experimentellen Manipulationen keinen Effekt auf die Parameter haben. Ich stelle euch hier Funktionen bereit, mit denen AIC & BIC berechnet werden können. Diese liefern relative Werte über -grob gesagt- den Informationsverlust eurer Modelle und sind dimensionslose Werte, welche nur im Vergleich mit anderen Modellen interpretiert werden können.\nWie wir schon in der Sitzung über Parameterschätzung besprochen haben, kann die maximierte Likelihoodfunktion in die Deviance die Abweichung des Modelles zu den Daten transformiert werden:\n\\[ Deviance = -2 \\cdot LL \\]\nDer AIC und der BIC quantifizieren nun diese Abweichung, indem sie die Komplexität des Modelles (Anzahl der Parameter k), sowie die Stichprobengröße (N; nur BIC) mit in die Gleichung einbeziehen:\n\\[ AIC = -2 \\cdot LL + 2 \\cdot k\\] \\[BIC = -2 \\cdot LL + k \\cdot log(N)\\]\nDer BIC zieht also in Betracht, dass die Größe der Stichprobe den Fit künstlich erhöht und korrigiert daher für die Stichprobengröße im Strafterm. Über beide Kennwerte können später AIC / BIC - Weights (Wagenmakers & Farell, 2004) berechnet werden:\n\\[\nw_{i}(AIC) = \\frac{exp\\{-\\frac{1}{2}\\Delta_i(AIC)\\}}{\\sum_{k=1}^K exp\\{-\\frac{1}{2}\\Delta_k(AIC)\\}}\\]\nmit deren Hilfe auch eine Evidence-Ratio berechnet werden kann (hier mit Zahlen Beispielen):\n\\[ Evidence Ratio_{A/B} = \\frac{w_A(AIC)}{w_B(AIC)} = \\frac{.61}{.22} \\approx 2.7 \\]\nDie berechneten AIC/BIC-Weights können auch mit Hilfe von Luces Choice Rule zu Wahrscheinlichkeiten normalisiert werden. Es kann also für jedes Modell zusätzlich zur Evidence Ratio die Wahrscheinlichkeit der Gültigkeit über die jeweiligen Konkurrenzmodelle berechnet werden:\n\\[ p_{A/B} = \\frac{w_{A}(AIC)}{w_{A}(AIC) + w_{B}(AIC)} = \\frac{.61}{.61 + .22} \\approx .73\\]\nFast-dm bietet aber auch die Möglichkeit, aus den geschätzten Modellen Reaktionszeitverteilungen für korrekte und inkorrekte Trials zu simulieren. Diese werden dann pro Quantil (meist für das 1. Quantil) mit den empirischen Reaktionszeiten korreliert und bilden ein Maß für den absoluten Modelfit. Wir werden uns die Berechnung des absoluten Modelfits in einer anderen Sitzung genauer ansehen!\nNun extrahieren wir aus dem Datensatz die Werte für die LogLikelihood (LL). Dabei ist zu beachten, dass fast-dm immer bereits die negative logLikelihood ausgibt, wir müssen diese also nicht mehr flektieren.\n\n# Evaluate Fit\n\n# Conducted Trials for every ID\n\nnTrials &lt;- dm_final %&gt;% \n  group_by(ID) %&gt;% \n  summarise(nTrials = n()) %&gt;% \n  rename(dataset=ID)\n\nmodel_a &lt;- shooter_dm_baseline %&gt;%\n  group_by(dataset) %&gt;% \n  select(fit) %&gt;% \n  inner_join(nTrials,by= \"dataset\") %&gt;% \n  mutate(Model = \"Model A\")\n\nmodel_b &lt;- shooter_dm %&gt;% \n  group_by(dataset) %&gt;% \n  select(fit) %&gt;% \n  inner_join(nTrials,by= \"dataset\") %&gt;%\n  mutate(Model = \"Model B\")\n\nErgänzt nun den unten stehenden Code, um zunächst den AIC zu berechnen, nutzt dazu die bereits vorhanden Funktionen. Um den AIC zu berechnen, muss die Anzahl der geschätzten Parameter im Modell herangezogen werden. Dazu müsst ihr auszählen, wie viele unterschiedliche Parameter pro Person geschätzt werden. Diese ergeben sich aus der Anzahl der Bedingungen, sowie der pro Bedingung variierten Parameter.\nBeispiel: In Bedingung A (2-Level) werden 3 Parameter variiert. Das heisst alle 3 Parameter werden über die 2-level der Bedinungen A geschätzt, was insgesamt 6 freie Parameter ergibt !\nWie viele Parameter werden also im “Baseline” Model und im “Restricted” Model frei geschätzt?\n\n# Extract -LL from results - fast-dm saves always the negative LogLikelihood \n\nLL_A &lt;- model_a %&gt;% \n  select(dataset,Model,fit,nTrials)\n\nLL_B &lt;- model_b %&gt;% \n  select(dataset,Model,fit,nTrials)\n\n# Remove subjects which did not converge in model b -  Manche Subjects sind im komplexeren Modell nicht konvergiert, das zu wenig Daten vorhanden waren. Diese müssen wir aus dem Datensatz entfehrnen, damit beide gleich groß sind.\n\nLL_A &lt;- model_a %&gt;% \n  filter(dataset %in% LL_B$dataset)\n\n# Modelltests  ----\n# Calculating AIC for each Model \n# Settings (k= Number of est. Parameters, M = Number of Observations)\n#           Baseline Model A: k = ?, M = Conducted Trials, LL = negative Likelihood-Values for each Person\n#           Restricted Model B (v,t0,a,zr): k = ?, M = Conducted Trials, LL = negative Likelihood-Values for each Person\n\nA_AIC &lt;- function.aic(k=4,\n                      LL_A$fit) \n\n[1] -130.3094\n\nB_AIC &lt;- function.aic(k=12,\n                      LL_B$fit)\n\n[1] -260.5301\n\nAIC &lt;- cbind(A_AIC,B_AIC)\n\n\n# Melt AIC\n\nModel_AICs&lt;-melt(AIC, id.vars = c(\"dataset\", \"nTrials\"),\n                 varnames = c(\"ID\",\"Model\"), \n                 value.name = \"AIC\") \nModel_AICs$Model &lt;- as.character(Model_AICs$Model)\nModel_AICs &lt;-Model_AICs %&gt;% \n  mutate(Model=stringr::str_remove(Model, pattern = \"_AIC\"))\n\nAus den berechneten AICs könnenw wir nun die Unterschiede der Modelle bezüglich des AIC berechnen. Dies ist der sogennante \\(\\Delta\\)AIC-Wert, dieser kann dann zu Berechnung der jeweiligen AIC-weights genutzt werden. Hierzu muss der mittlere Fit nach Modell, sowie der \\(\\Delta\\)AIC als neue Spalte berechnet werden. Hierzu muss das “kleinere” vom größeren Modell abgezogen wird. Nutzt hierfür dplyr (mit summarise() & mutate())! Berechnet anschließend mit Hilfe der aic.weight() Funktion das jeweilige AIC-weight beider Modelle!\nAnschließend könnt ihr mit Hilfe der berechneten Weights sowohl die relative Evidenz, als auch die Wahrscheinlichkeit der Modelle (gegeben aller anderen Modelle) berechnen, orientiert euch hierfür an den oben stehenden Formeln!\n\n# Berechnet nun den dAIC und dBIC \n\ndAIC&lt;-Model_AICs %&gt;% group_by(Model) %&gt;%\n  summarise(AIC = mean(AIC)) %&gt;% \n  mutate(dAIC =  AIC - min(AIC))\n\n\n# Calculate AIC and BIC weights\naic_weights &lt;- aic.weight(d_aics = dAIC$dAIC) #Vector of dAIC\n\n# Calculate Relative Evidence of Model B over Model A \n# (hint: do not round)\n\naic_weights[2] / aic_weights[1]\n\n[1] 1.892574e+28\n\n# Calulate the probability of Model B over Model A   \n# Probability Model B over Model A \n\naic_weights[2] / sum(aic_weights)\n\n[1] 1\n\n\nWelches Modell fittet besser ?\n\n4.1.1 Ergebnisdatensatz für die Analyse säubern und vorbereiten\nNun müssen wir die Daten aus fast-dm zunächst säubern und in ein tidy-Format bringen. Hierzu erfordert es einige fortgeschrittener dplyr-Funktionen, daher werden wir dies Zeile für Zeile zusammen durchgehen:\n\n# Tidy Up Results for Analysis\n# Select only parameter estimates\n\n# Recode Driftrate to positive only\nshooter_est_v_t0&lt;- shooter_dm %&gt;%\n  select(dataset, starts_with(c(\"t0\", \"v\"))) %&gt;% \n  mutate(v_black_gun = case_when(v_black_gun &lt; 0 ~ v_black_gun * -1,\n                                 TRUE ~ v_black_gun),\n         v_white_gun = case_when(v_white_gun &lt; 0 ~ v_white_gun * -1,\n                                 TRUE ~ v_white_gun))\n\n# Create different Datasets for zr & a and t0 & v \n# and bring in long format for analysis\n\nshooter_est_z_a &lt;- shooter_dm %&gt;%\n  select(dataset, starts_with(c(\"z\", \"a\")))\n\ntheta_col_obj&lt;-pivot_longer(shooter_est_v_t0, \n             cols = starts_with(c(\"t0\",\"v\")),\n             names_to =c(\"theta\",\"color\",\"object\"),\n             values_to = \"estimate\",\n             names_sep = \"_\")\n\ntheta_col &lt;- pivot_longer(shooter_est_z_a, \n                          cols = starts_with(c(\"z\",\"a\")),\n                          names_to =c(\"theta\",\"color\"),\n                          values_to = \"estimate\", names_sep = \"_\")",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Diffusion Modeling",
      "Workshop Diffusion Modeling"
    ]
  },
  {
    "objectID": "scripts/ddm-workshop/DDM_Workshop_solution.html#varianzanalysen-und-kontraste",
    "href": "scripts/ddm-workshop/DDM_Workshop_solution.html#varianzanalysen-und-kontraste",
    "title": "Workshop Diffusion-Modeling",
    "section": "4.2 Varianzanalysen und Kontraste",
    "text": "4.2 Varianzanalysen und Kontraste\nNun führen wir Varianzanalysen durch, um die Effekte der experimentellen Manipulationen auf die Parameter zu prüfen. Hierzu müssen wir zunächst für jeden Parameter einen eigenen Datensatz erstellen. Nutzt hierzu dplyr, um aus den Datensätzen theta_col und theta_col_obj neue Datzensätze zu erstellen.\nDann nutzen wir das Paket afex um ANOVAs zu berechnen, hierzu habe ich euch den Code bereits geschrieben. Welche Effekte und welche Interpretationen lassen sich aus den Ergebnissen ableiten ? Welche Hypothese der Autoren wird hierdurch bestätigt, welche widerlegt?\nErstellt dann aus den neuen Datensätzen zusätzliche summary-Datensätze, in welchen ihr je nach Parameter und experimenteller Manipulation die entsprechenden Mittelwerte und Standardfehler berechnet.\n\n# Run ANOVA with afex to check group differences and estimate marginal means from results\n# Create Datasets for every Parameter.\n# Sample Size\nSubject = unique(length(shooter_dm$dataset))\n\n\naov_zr &lt;- filter(theta_col, theta==\"zr\")\naov_a &lt;- filter(theta_col, theta==\"a\")\naov_t0 &lt;- filter(theta_col_obj, theta==\"t0\")\naov_v &lt;- filter(theta_col_obj, theta==\"v\")\n\nzr_result &lt;-afex::aov_ez(id = \"dataset\",dv =\"estimate\",data = aov_zr, within = \"color\")\n# Estimate means and calculate pairwise contrasts\nmeans_zr&lt;-emmeans(zr_result,specs = \"color\")\npairs(means_zr,reverse = T)\n\n contrast      estimate     SE  df t.ratio p.value\n white - black    0.035 0.0228 129   1.537  0.1267\n\n# Create Summary Dataset\nsummary_z &lt;- aov_zr %&gt;% group_by(color) %&gt;% \n  summarise(mean_z = mean(estimate)-0.5, \n            sd_z = sd(estimate),\n            se_z = sd_z/sqrt(Subject))\n\na_result &lt;- afex::aov_ez(id = \"dataset\",\n                         dv =\"estimate\",\n                         data = aov_a, within = \"color\")\n\n# # Estimate means and calculate pairwise contrasts\nmeans_a&lt;-emmeans(a_result,specs = \"color\")\npairs(means_a,reverse = T)\n\n contrast      estimate    SE  df t.ratio p.value\n white - black   -0.274 0.108 129  -2.525  0.0128\n\n# # Create Summary Dataset\nsummary_a &lt;- aov_a %&gt;% group_by(color) %&gt;% \n  summarise(mean_a = mean(estimate), \n            sd_a = sd(estimate),\n            se_a = sd_a/sqrt(Subject))\n\nt0_result &lt;- afex::aov_ez(id = \"dataset\",\n                          dv =\"estimate\",\n                          data = aov_t0, \n                          within = c(\"color\",\"object\"), \n                          fun_aggregate = mean)\n\n# Estimate means and calculate pairwise contrasts\nmeans_t0&lt;-emmeans(t0_result,specs=c(\"color\",\"object\"))\npairs(means_t0,reverse = T,adjust=\"holm\")\n\n contrast                  estimate      SE  df t.ratio p.value\n white gun - black gun       0.0323 0.00544 129   5.943  &lt;.0001\n black phone - black gun     0.1042 0.00673 129  15.488  &lt;.0001\n black phone - white gun     0.0719 0.00678 129  10.610  &lt;.0001\n white phone - black gun     0.0837 0.00573 129  14.626  &lt;.0001\n white phone - white gun     0.0514 0.00477 129  10.791  &lt;.0001\n white phone - black phone  -0.0205 0.00641 129  -3.200  0.0017\n\nP value adjustment: holm method for 6 tests \n\n# Create Summary Dataset\n\nsummary_t0 &lt;- aov_t0 %&gt;% group_by(color,object) %&gt;% \n  summarise(mean_t0 = mean(estimate), \n            sd_t0 = sd(estimate),\n            se_t0 = sd_t0/sqrt(Subject))\n\n\nv_result &lt;- afex::aov_ez(id = \"dataset\",\n                         dv =\"estimate\",\n                         data = aov_v,\n                         within = c(\"color\",\"object\"), \n                         fun_aggregate = mean)\n\n# Estimate means and calculate pairwise contrasts\nmeans_v&lt;-emmeans(v_result,specs=c(\"color\",\"object\"))\npairs(means_v,reverse = T,adjust=\"holm\")\n\n contrast                  estimate    SE  df t.ratio p.value\n white gun - black gun       -2.266 0.593 129  -3.819  0.0010\n black phone - black gun      2.870 1.136 129   2.527  0.0255\n black phone - white gun      5.137 0.982 129   5.233  &lt;.0001\n white phone - black gun     -0.232 0.856 129  -0.271  0.7868\n white phone - white gun      2.034 0.702 129   2.899  0.0176\n white phone - black phone   -3.102 1.103 129  -2.813  0.0176\n\nP value adjustment: holm method for 6 tests \n\n# Create Summary Dataset\nsummary_v &lt;- aov_v %&gt;% group_by(color,object) %&gt;% \n  summarise(mean_v = mean(estimate), \n            sd_v = sd(estimate),\n            se_v = sd_v/sqrt(Subject))",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Diffusion Modeling",
      "Workshop Diffusion Modeling"
    ]
  },
  {
    "objectID": "scripts/ddm-workshop/DDM_Workshop_solution.html#plotten-der-ergebnisse-und-interpretation",
    "href": "scripts/ddm-workshop/DDM_Workshop_solution.html#plotten-der-ergebnisse-und-interpretation",
    "title": "Workshop Diffusion-Modeling",
    "section": "4.3 Plotten der Ergebnisse und Interpretation",
    "text": "4.3 Plotten der Ergebnisse und Interpretation\nIm letzten Schritt bietet es sich an, die Ergebnisse mit Hilfe von ggplot zu visualisieren. Verwendet hierzu die von euch erstellten summary-Datensätze. Nutz hierzu wieder ggplot und geom_bar(), sowie geom_errorbar(). Erstellt für jeden Parameter einen eigenen Plot. Ihr könnt alle Plots in einem Grid zu einem Plot zusammenführen, wenn ihr aus dem Package gridExtra den Befehl grid.arrange(), verwendet.\nBeschriftet die Plots, als würden sie in einer Publikation erscheinen (keine Variablennamen aus R, Legede, gut erkennbare Farben etc.)\n\n# For z\nz &lt;- ggplot(summary_z,aes(x=factor(color,levels = c(\"white\",\"black\")),y=mean_z,fill=color)) + \n  scale_y_continuous(breaks = seq(-0.2,0.2,0.05), limits = c(-0.2,0.2)) + \n  geom_bar(stat=\"identity\",width=0.4, position = position_dodge(0.4)) + \n  scale_fill_d3(name=\"Skin Color\") + theme_apa() +     geom_errorbar(aes(ymin=mean_z-1.96*se_z,ymax=mean_z+1.96*se_z,width=0.1),position=position_dodge(0.4)) + \n  labs(title = \"Effect of skin color on starting bias z\", \n       x= \"Skin Color\", y=\"Estimated deviation from 0.5 of z\") +theme(legend.position = \"none\") \n\n\n# For a\na &lt;- ggplot(summary_a,aes(x=factor(color,levels = c(\"white\",\"black\")),y=mean_a,fill=color)) + \n  geom_bar(stat=\"identity\",width=0.4) + scale_fill_d3(name=\"Skin Color\")+ theme_apa() + \n  geom_errorbar(aes(ymin=mean_a-1.96*se_a,ymax=mean_a+1.96*se_a, group=color, width=0.1))+ \n  labs(title = \"Effect of skin color on threshold separation a\",  x= \"Skin Color\",\n       y=\"Estimated marginal mean for a\")+\n  theme(legend.position = \"none\")\n\n# For t0\nt0 &lt;- ggplot(summary_t0,aes(x=factor(object,levels = c(\"phone\",\"gun\")),y=mean_t0,fill=factor(color,levels = c(\"white\",\"black\")))) + geom_bar(stat=\"identity\",width=0.4, position = position_dodge()) + \n  scale_fill_d3(name=\"Skin Color\") + theme_apa() + \n  geom_errorbar(aes(ymin=mean_t0-1.96*se_t0,ymax=mean_t0+1.96*se_t0,width=0.1),\n                position=position_dodge(0.4)) +\n  labs(title = \"Effect of skin color and object on non-decision time t0\", x= \"Object\",\n       y=\"Estimated Marginal mean for t0\")\n\n\n\n# For v\nv &lt;-ggplot(summary_v,aes(x=factor(object,levels = c(\"phone\",\"gun\")),y=mean_v,fill=factor(color,levels = c(\"white\",\"black\"))))  + geom_bar(stat=\"identity\",width=0.4, position = position_dodge()) + \n  scale_fill_d3(name=\"Skin Color\") + theme_apa() + \n  geom_errorbar(aes(ymin=mean_v-1.96*se_v,ymax=mean_v+1.96*se_v,width=0.1),position=position_dodge(0.4))  + \n  labs(title = \"Effect of skin color and object on drift rate v\", x= \"Object\", \n       y=\"Estimated Marginal mean for v\")\n\n\n# Arrange Grid with gridExtra\n\nggarrange(a,z,v,t0, ncol=2, nrow=2, common.legend = TRUE, legend=\"bottom\")",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Diffusion Modeling",
      "Workshop Diffusion Modeling"
    ]
  },
  {
    "objectID": "scripts/advanced-r/Advanced_R_SS23.html",
    "href": "scripts/advanced-r/Advanced_R_SS23.html",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "scripts/advanced-r/Advanced_R_SS23.html#fortgeschrittene-dplyr-funktionen",
    "href": "scripts/advanced-r/Advanced_R_SS23.html#fortgeschrittene-dplyr-funktionen",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "scripts/advanced-r/Advanced_R_SS23.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "href": "scripts/advanced-r/Advanced_R_SS23.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "title": "Advanced Dplyr",
    "section": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen",
    "text": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen\nDie case_when Funktion in dplyr ermöglicht es, basierend auf bestimmten Bedingungen verschiedene Werte für eine Spalte in einem Datensatz auszuwählen und abhängig vom Variablenwert einen neue Variable zu erstellen. Es ist eine sehr einfache Möglichkeit, if-else Statements in eine dplyr-Pipeline zu integrieren.\nDie Syntax eines case_when() Befehles entspricht einer einfachen if oder if-else Verknüpfung. Eine if-else Verknüpfung prüft eine Bedingung und führt wenn diese Erfüllt ist einen definierten Befehl aus. Ist die Bedingung nicht erfüllt, wird andernfalls (else) ein anderer Befehl ausgeführt:\n\n\n\nCode\n# Vektor von Zahlen darauf testen, ob sie gerade oder ungerade sind:\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei geteilt keinen Rest hat, dann schreibe Gerade, ansonsten Ungerade.\n\nifelse(zahlen %% 2 == 0,\"Gerade\",\"Ungerade\")\n\n\ncase_when() funktioniert nach der Gleichen Logik:\n\n\n\nDabei wird immer zuerst die if-Bedinungen und deren Output definiert und anschließend mit TRUE die else Bedingung, welche angibt was passiert wenn die if - Bedingung nicht zutrifft:\n\n\n\n\nCode\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei\n# geteilt keinen Rest hat, dann schreibe Gerade, \n# ansonsten Ungerade.\n\ncase_when(zahlen %% 2 == 0 ~ \"Gerade\",\n          TRUE ~\"Ungerade\")\n\n\nDer Unterschied von case_when() und ifelse() ist, dass mit case_when() auch mehrere Bedingungen definiert werden können (z.B. wenn eine Variable in mehrere Kategorien eingeteilt werden soll):\n\nHier gibt es auch ein finales else Statement, dieses muss aber nicht sein, solange die Kategorien alle Fälle abdecken!\n\n2.0.1 case_when(): Beispiele\nDas folgende ist ein einfaches Beispiel, bei dem case_when() verwendet wird, um in einem Datensatz eine neue Spalte Alter_Kategorie anhand des Werts in der Spalte Alter zu kategorisieren. Wenn eine neue Spalte erstellt werden soll, muss case_when() immer mit mutate() kombiniert werden:\n\n\nCode\n# Beispiel Datensatz erstellen\ndf &lt;- data.frame(Name = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45))\nhead(df)\n\n# mutate und case_when verwenden, um neue Variable in Abhängigkeit vom Alter zu erstellen:\ndf %&gt;% \n  mutate(Alter_Kategorie = case_when(\n    Alter &lt;= 25 ~ \"jung\",\n    Alter &gt; 25 & Alter &lt;= 35 ~ \"mittel\",\n    Alter &gt; 35 ~ \"alt\"\n  ))"
  },
  {
    "objectID": "scripts/advanced-r/Advanced_R_SS23.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "href": "scripts/advanced-r/Advanced_R_SS23.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "title": "Advanced Dplyr",
    "section": "3 Long- und Wide-Datenformat - Datentransformation in dplyr",
    "text": "3 Long- und Wide-Datenformat - Datentransformation in dplyr\nNun haben wir die Daten von Frenken et al. neu kodiert und die nötigen Informationen im Datensatz, um mit MPTinR zu arbeiten. Allerdings liegen die Daten noch im sogenannten long - Format vor. Oftmals ist es jedoch notwendig, Daten entweder von einem long in ein wide Format oder umgekehrt zu transfomieren. Im Kontext von Datenanalyse und statistischer Modellierung gibt es in der Regel zwei Haupttypen von Datenformaten: wide-Format (breit-Format) und long-Format (lang-Format).\n\n3.1 Wide-Format\n\nIn einem wide Format sind die verschiedenen Merkmale einer einzigen Beobachtung in separaten Spalten dargestellt.\nEine einzige Zeile in einem breiten Datensatz repräsentiert eine Beobachtung.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, wäre in einem breiten Format dargestellt, wobei jede Spalte einem bestimmten Fach entspricht.\nVorteil: Es ist einfach, schnelle Übersichten über große Datenmengen zu erhalten.\n\nBeispiel:\n\n\nCode\nwide_df &lt;- data.frame(Schüler_ID = c(1, 2, 3), \n                      Mathe = c(89, 76, 92), \n                      Englisch = c(92, 88, 95), \n                      Wissenschaft = c(88, 72, 98))\nwide_df\n##   Schüler_ID Mathe Englisch Wissenschaft\n## 1          1    89       92           88\n## 2          2    76       88           72\n## 3          3    92       95           98\n\n\n\n\n3.2 Long-Format\n\nIm long Format sind alle Merkmale einer einzigen Beobachtung in einer Zeile dargestellt.\nEine einzige Spalte in einem long Datensatz repräsentiert ein bestimmtes Merkmal.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, könnte in einem langen Format dargestellt werden, wobei jede Zeile einer bestimmten Schüler-Fach-Kombination entspricht.\nVorteil: Es ist einfach, bestimmte Merkmale für verschiedene Beobachtungen zu vergleichen oder zu analysieren. Außerdem ist es für manche statistische Methoden, wie lineare Regression oder einer ANOVA, das bevorzugte Format.\n\nBeispiel:\n\n\nCode\nlong_df &lt;- data.frame(Schüler_ID = c(rep(1, 3), rep(2, 3), rep(3, 3)), \n                      Fach = c(rep(\"Mathe\", 3), rep(\"Englisch\", 3), rep(\"Wissenschaft\", 3)), \n                      Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\nlong_df\n##   Schüler_ID         Fach Note\n## 1          1        Mathe   89\n## 2          1        Mathe   92\n## 3          1        Mathe   88\n## 4          2     Englisch   76\n## 5          2     Englisch   88\n## 6          2     Englisch   72\n## 7          3 Wissenschaft   92\n## 8          3 Wissenschaft   95\n## 9          3 Wissenschaft   98\n\n\nEs ist wichtig, das Konzept des wide- und long-Formats zu verstehen, da es bei der Datenaufbereitung und Analyse eine wichtige Rolle spielt. Zum Beispiel kann ein wide -Datensatz schwer zu analysieren sein, wenn man Vergleiche zwischen bestimmten Merkmalen über mehrere Beobachtungen hinweg machen möchte (z.b. bei Varianzanalysen). Hier ist es oft besser, den Datensatz in ein long-Format zu bringen.Umgekehrt kann es der Fall sein, das eine bestimmte Analyseform oder Modellierung die Daten im wide-Format\n\n\n3.3 pivot - Funktionen in dplyr\nDie Funktionen pivot_wider und pivot_longer gehören zu den Funktionen von dplyr und dienen dazu, Datensätze zu transformieren.\n\npivot_wider verwandelt einen long-Format Datensatz in einen wide-Format Datensatz, indem es die Werte einer bestimmten Spalte zu neuen Spalten umbenennt. Dies kann manuell durchgeführt werden, aber pivot_wider macht dies automatisch und erleichtert so die Datentransformation:\npivot_longer verwandelt einen wide-Format Datensatz in einen long-Format Datensatz, indem es die Spalten mit bestimmten Werten in einer neuen Spalte zusammenfasst.\n\nEin Beispiel für den Einsatz von pivot_wider:\n\n\nCode\n# long data example\nlong_data &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\", \"John\", \"Jane\", \"Jim\"),\n  subject = c(\"physics\", \"physics\", \"physics\", \"math\", \"math\", \"math\"),\n  score = c(85, 90, 80, 75, 80, 70)\n)\n\nlong_data\n## # A tibble: 6 × 3\n##   name  subject score\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 John  physics    85\n## 2 Jane  physics    90\n## 3 Jim   physics    80\n## 4 John  math       75\n## 5 Jane  math       80\n## 6 Jim   math       70\n\n\n# pivot to wide format\npivot_wider(long_data, names_from = subject, values_from = score)\n## # A tibble: 3 × 3\n##   name  physics  math\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n## 1 John       85    75\n## 2 Jane       90    80\n## 3 Jim        80    70\n\n\nEin Beispiel für den Einsatz von pivot_longer:\n\n\nCode\n# wide data example\nwide_df &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\"),\n  Mathe_Note = c(85, 90, 80),\n  Englisch_Note = c(75, 80, 70)\n)\n\n\nwide_df\n## # A tibble: 3 × 3\n##   name  Mathe_Note Englisch_Note\n##   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n## 1 John          85            75\n## 2 Jane          90            80\n## 3 Jim           80            70\n\n\n# pivot to long format\npivot_longer(wide_df, cols = c(Mathe_Note, Englisch_Note), \n             names_to = \"Schüler\", values_to = \"Note\")\n## # A tibble: 6 × 3\n##   name  Schüler        Note\n##   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n## 1 John  Mathe_Note       85\n## 2 John  Englisch_Note    75\n## 3 Jane  Mathe_Note       90\n## 4 Jane  Englisch_Note    80\n## 5 Jim   Mathe_Note       80\n## 6 Jim   Englisch_Note    70\n\n\n\n\n3.4 Argumente für pivot_longer und pivot_wider\npivot_longer benötigt mindestens zwei Argumente:\n\ncols: Dies ist ein Zeichenvektor, der angibt, welche Spalten im Datensatz zusammengefasst werden sollen. -Schüler_ID bedeutet zum Beispiel, dass alle Spalten außer Schüler_ID zusammengefasst werden sollen.\nnames_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die zusammengefassten Werte gespeichert werden.\nvalues_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die Werte gespeichert werden, die aus den zusammengefassten Spalten stammen.\n\n\npivot_wider benötigt mindestens zwei Argumente:\n\nnames_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Namen für die neuen Spalten verwendet werden soll.\nvalues_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Werte für die neuen Spalten verwendet werden soll.\n\n\nBeide Funktionen haben auch weitere optionale Argumente wie values_fill und names_prefix oder names_sep()um die Daten bei Bedarf weiter anzupassen."
  },
  {
    "objectID": "scripts/advanced-r/Advanced_R_SS23.html#praktisches-beispiel",
    "href": "scripts/advanced-r/Advanced_R_SS23.html#praktisches-beispiel",
    "title": "Advanced Dplyr",
    "section": "4 Praktisches Beispiel",
    "text": "4 Praktisches Beispiel\nIm nächstes Beipsiel werden wir einen Datensatz umformen, mit dem wir uns im nächsten Workshop zu MPT-Modellen beschäftigen werden. Der Datensatz enthält fünf Spalten, die die Daten aus einem Priming-Experiment codieren. Insgesamt hat jede Versuchsperson 4 Bedingungen durchlaufen, die in der Spalte stim codiert sind (bg,bp,wg,wp). In jeder dieser Bedingungen mussten die VP eine binäre Entscheidungsaufgabe bearbeiten. in den Spalten hits und miss ist codiert, ob die VP die richtige oder falsche Entscheidung getroffen haben. Die Spalte ntrials gibt die Gesamtzahl der Versuche einer Person pro Bedingung an.\n\n\nCode\nhead(freq_dat,5)\n\n\n# A tibble: 5 × 5\n# Groups:   subj_idx [2]\n  subj_idx stim   hits ntrials  miss\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n1        0 bg       30      31     1\n2        0 bp       28      29     1\n3        0 wg       23      27     4\n4        0 wp       28      33     5\n5        1 bg       28      30     2\n\n\nUm in der Praxis mit dem Datensatz arbeiten zu können, müssen wir diesen jedoch zunächst in das richtige Format bringen. Der Datensatz liegt momentan im long-Format vor, für bestimmte Analysen muss dieser jedoch in das wide-Format umgewandelt werden. Das führt dazu, dass es für jede Bedingung je eine Spalte für hits und misses geben wird, also z.B. hits_bg und misses_bg.\nNach der Umformung, sieht der Datensatz dann wie folgt aus:\n\n\nCode\n#| echo: true\n#| output: true\n#| warning: false\n#| code-overflow: wrap\n#| collapse: true\n\nhead(freq_dat %&gt;% pivot_wider(names_from = c(\"stim\"),\n                         values_from = c(\"hits\",\"miss\"),\n                         id_cols=\"subj_idx\"),5)\n\n\n# A tibble: 5 × 9\n# Groups:   subj_idx [5]\n  subj_idx hits_bg hits_bp hits_wg hits_wp miss_bg miss_bp miss_wg miss_wp\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1        0      30      28      23      28       1       1       4       5\n2        1      28      16      27      11       2       2       2       1\n3        3      25      10      33      23       0       0       1       1\n4        4      35      24      27      33       1       0       0       0\n5        5      31      28      25      23       1       2       4       4\n\n\nWas passiert hier genau ? Schritt für Schritt:\n\nnames_from = c(stim) - Die neuen Spalten sollen aus der Spalte stim benannt werden - (bg,bp,wp,wg)\nvalues_from = c(\"hits\",\"miss\") - in den neuen Spalten sollen die Werte der Hits und Miss Spalten stehen - durch den ersten und zweiten Schritt entstehen also Spalten, die jeweils hits_bg etc. enthalten. Also die Hits aus der Bedingung “bg” usw.\nid_cols - dies soll für jedes Subject einzeln geschehen."
  },
  {
    "objectID": "scripts/advanced-r/Advanced_R_SS23.html#übungen",
    "href": "scripts/advanced-r/Advanced_R_SS23.html#übungen",
    "title": "Advanced Dplyr",
    "section": "5 Übungen",
    "text": "5 Übungen\n\n5.1 case_when()\n1.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Note_Kategorie” zu erstellen, die “Sehr gut” für Noten über 90, “Gut” für Noten zwischen 80 und 90 und “Schlecht” für Noten unter 80 angibt.\n\n\nCode\ndf &lt;- data.frame(Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\n# Your Code Here\n\n\n2.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Bestanden” zu erstellen, um zu Prüfen ob ein Schüler einer bestimmten Schulform eine Prüfung bestanden hat. Die Bestehensgrenzen sind wie folgt.\n-   Für die Hauptschule liegt die Bestehensgrenze bei 50 %\n-   Für die Realschule liegt die Bestehensgrenze bei 60 %\n-   Für das Gymnasium liegt die Bestehensgrenze bei 70 %\nKodieren Sie das bestehen entweder mit “Pass” oder “Fail”.\n\n\nCode\ndf &lt;- data.frame(Note = runif(100,min=0, max=100),\n                 Schulform = sample(c(\"Gymnasium\",\"Realschule\", \"Hauptschule\"),\n                                    size = 100, replace = T))\n# Your Code here\n\n\n3.) Nutzen Sie die Funktion case_when und die dplyr-Library in R, um eine neue Spalte in dem Datensatz “df” zu erstellen, die die Einkommenskategorie jeder Person basierend auf ihrem Berufsstatus und ihrem Einkommen kategorisiert. Die Einkommenskategorien sollten wie folgt sein:\n\nFür Angestellte mit einem Einkommen von bis zu 50.000: “niedrig”\nFür Angestellte mit einem Einkommen zwischen 50.000 und 75.000: “mittel”\nFür Angestellte mit einem Einkommen über 75.000: “hoch”\nFür Freiberufler mit einem Einkommen von bis zu 60.000: “niedrig”\nFür Freiberufler mit einem Einkommen zwischen 60.000 und 100.000: “mittel”\nFür Freiberufler mit einem Einkommen über 100.000: “hoch”\nFür Ruheständler mit einem Einkommen von bis zu 30.000: “niedrig”\nFür Ruheständler mit einem Einkommen über 30.000: “mittel_hoch”\n\n\n\nCode\ndf &lt;- data.frame(ID = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45),\n                 Berufsstatus = c(\"Angestellter\", \"Freiberufler\", \"Ruheständler\"),\n                 Einkommen = c(45000, 75000, 32000))\n\n\n# Your Code here\n\n\n\n\n5.2 Pivotting\n1.) Konvertieren Sie die folgenden Datensatz von wide to long. Erstellen Sie aus den Spalten zwei neue Spalten mit den Namen (month und index). Nutzen Sie dazu das Argument names_sep = \"_\". Die Werte sollen in die Spalte “N” geschrieben werden.\nTip: Sie müssen bei names_to einen Vektor mit den Namen der neuen Spalten angeben.\n\n\nCode\ndf_wide &lt;- data.frame(\n  Jan_sales = c(10, 40, 70),\n  Feb_sales = c(20, 50, 80),\n  Mar_sales = c(30, 60, 90)\n)\n\n# Your Code here\n\n\n2.) Konvertieren Sie die folgendn Datensatz von long in wide.\n\n\nCode\n    df_long &lt;- data.frame(\n      ID = c(\"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\"),\n      month = c(\"Jan\", \"Jan\", \"Jan\", \"Feb\", \"Feb\", \"Feb\", \"Mar\", \"Mar\", \"Mar\"),\n      sales = c(10, 40, 70, 20, 50, 80, 30, 60, 90))\n      \n\n    # Your Code Here"
  },
  {
    "objectID": "scripts/mpt-workshop/mpt-workshop.html",
    "href": "scripts/mpt-workshop/mpt-workshop.html",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Abbildung 1: Step by Step Tutorial für den MPT-Workshop",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Multinomial Processing Tree Models",
      "MPT Workshop"
    ]
  },
  {
    "objectID": "scripts/mpt-workshop/mpt-workshop.html#einführung",
    "href": "scripts/mpt-workshop/mpt-workshop.html#einführung",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "1 Einführung",
    "text": "1 Einführung\nHeute Beschäftigen wir uns mit der Anwendung von MPT Modellen in R. Hierzu nutzen wir einen Datensatz von Frenken et al. (2022), der sich mit der Verarbeitung von Stereotypen befasst. Zunächst müssen wir aber die entsprechenden Packages installieren. Wir nutzen das Package (MPTinR), das viele Funktionalitäten für das MPT Modelling mit sich bringt ! Zusätzlich installieren wir noch ggpubr und effectsize um später einige nette Features beim Plotten und Analysieren der Ergebnisse zu haben:\n\n# Install MPT Package\n# install.packages(\"MPTinR\")\n# install.packages(\"ggpubr\")\n# install.packages(\"effectsize\")\n\nNun laden wir die Pakete die wir für den weiteren Verlauf brauchen werden sowie unseren Datensatz ein:\n\nlibrary(MPTinR)\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(effectsize)\n\n\nStudy_2_dm &lt;- read_csv(\"Study_2_dm.csv\", \n                       col_types = cols(...1 = col_skip(), \n                                        stimulus = col_factor(levels = c(\"gun\",\"phone\")), \n                                        response = col_integer(), \n                                        condition = col_factor(levels = c(\"black\",\"white\"))))",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Multinomial Processing Tree Models",
      "MPT Workshop"
    ]
  },
  {
    "objectID": "scripts/mpt-workshop/mpt-workshop.html#der-shooter---bias",
    "href": "scripts/mpt-workshop/mpt-workshop.html#der-shooter---bias",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "2 Der Shooter - Bias",
    "text": "2 Der Shooter - Bias\nWie schon in der letzten Sitzung vorgestellt handelt es sich beim beim First Person Shooter Task (FPST) um ein Paradigma, mit dem Stereotype und deren Auswirkungen auf Entscheidungen untersucht werden können. Hierbei werden üblicherweise verschiedene Ethnien (z.B. schwarze oder weiße Personen), entweder mit einer Waffe (threat) oder einem ungefährlichem Objekt (z.B. Telefon, harmless) gezeigt. Die Versuchspersonen werden instruiert unabhängig von der Hautdfarbe so schnell und korrekt wie möglich auf bewaffnete Ziele zu schiessen. Im Gegensatz dazu soll nicht auf unbewaffnete Ziele geschossen werden.\n\n\n\nVerschiedene Paradigmen des FPST",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Multinomial Processing Tree Models",
      "MPT Workshop"
    ]
  },
  {
    "objectID": "scripts/mpt-workshop/mpt-workshop.html#model-definition",
    "href": "scripts/mpt-workshop/mpt-workshop.html#model-definition",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "3 Model Definition",
    "text": "3 Model Definition\nWir werden heute versuchen, die Daten von Frenken et al. mit dem Process Dissociation Model zu fitten. Dieses müssen wir aber zunächst in R definieren. Die Modellgleichungen leiten sich aus folgendem zugrundeliegendem Wahrscheinlichkeitsbaum ab:\n\n\n\nProcess Dissociation Model (Payne,2001)\n\n\nDas Modell hat nur zwei freie Parameter, zum einen c , der die Wahrscheinlichkeit angibt das kontrollierte Verarbeitungsprozesse aktiviert werden und zum anderen a der die Wahrscheinlichkeit angibt, dass diese Kontrolle fehlschlägt und eine stereotype Verarbeitung aktiviert wird.\n\nZur Erinnerung: das experimentelle Paradigma in Payne et al. ist ein Sequential Priming Task, bei dem die Versuchspersonen ein schwarzes oder weißes Gesicht gesehen haben und dann entscheiden sollten, ob es sich bei dem anschließend gezeigten Objekt um einen Waffe oder ein Werkzeug handelt:\n\n\nIn unserem Fall ist das Paradigma also ähnlich, die Versuchspersonen sehen entweder einen Schwarzen oder einen Weißen (unbewaffnet vs. bewaffnet) und sollen dann entscheiden, ob geschossen werden soll oder nicht. Das heisst, wir können das Modell im Prinzip ekaxt so wie es ist, auf die Daten von Frenken et al. übertragen.\n\nNun müssen wir das Modell allerdings definieren. Hierzu müssen die Gleichungen für jeden Wahrscheinlichkeitsbaum aufgestellt werden. Jeder Baum repräsentiert eine Zielkategorie:\n\n\n\n\nHautfarbe\nObjekt\n\n\n\n\nSchwarz\nWaffe\n\n\nWeiß\nWaffe\n\n\nSchwarz\nTelefon\n\n\nWeiß\nTelefon\n\n\n\nEs müssen nun für jede dieser Outcome Kategorien Gleichungen für die unterschiedlichen Pfade definiert werden, welche zu den jeweiligen “Hits” und “Misses” führen (“+” bzw. “-”auf der Abbildung). Zusätzlichmüssen insgesamt 4 Parameter definiert werden. A und C jeweils für schwarze und weiße Hautfarbe, um Unterschiede in den Parametern, die auf die Hautfarbe zurückgehen, identifizieren zu können:\n\n\nFür MPTinR können die Gleichungen in einem einfachen “String” definiert werden. Hier ist die erste Gleichung für die Kategorie “White/ Phone” bereits eingetragen. In der Kategorie “White/Phone” führt der Pfad \\[c\\] und der Pfad \\[(1-c) \\cdot a\\] zu einem “Hit”.\nIm Gegensatz dazu führt \\[(1-c) \\cdot (1-a)\\] zu einem Miss. Tragt immer zunüchst die Gleichung für einen Hit und dann die Gleichungen für einen Miss ein. Da zwischen Pfaden die Wahrscheinlichkeiten addiert werden können, ist also die Gesamtgleichung eines “Hits” für die Kategorie “White/Phone”:\n\\[c + (1-c) \\cdot a\\] und für einen Miss\n\\[ (1-c) \\cdot (1-a)\\]\nda es nur einen Pfad zu einem “Miss” gibt ! Vervollständigt nun die Gleichungen für die restlichen Kategorien. Denkt daran das Ihr die Parameter für die schwarze und weiße Hautfarbe unterschiedlich benennt!\n\n\n# Parameters for Black and White Skin Color! \n# c_w = c white, a_w = a white c_b = black, a_b = a black\n\npd &lt;- \"\n# WT\nc_w + (1-c_w)*a_w\n(1-c_w)*(1-a_w)\n\n\n# WG\nc_w + (1-c_w) * (1-a_w)\n(1-c_w) * a_w\n\n# BT\nc_b + (1-c_b)*(1-a_b)\n(1-c_b) * a_b\n\n\n# BG\nc_b + (1-c_b)*a_b\n(1-c_b)*(1-a_b)\n\"\n\n\nWenn ihr die Gleichungen alle definiert habt, könnt ihr das Modell mit MPTinR auf Korrektheit überprüfen lassen:\n\n\ncheck.mpt(textConnection(pd))\n\n$probabilities.eq.1\n[1] TRUE\n\n$n.trees\n[1] 4\n\n$n.model.categories\n[1] 8\n\n$n.independent.categories\n[1] 4\n\n$n.params\n[1] 4\n\n$parameters\n[1] \"a_b\" \"a_w\" \"c_b\" \"c_w\"\n\n\nWir sehen hier einen Test ob alle Wahrscheinlichkeiten sich zu 1 addieren, die Anzahl der Bäume, der gesamten und unabhängigen Kategorien, sowie der Parameter. In den Daten müssen immer exakt so viele Kategorien vorhanden sein, wie im Modell definiert ! Zusätzlich darf die Anzahl Parameter nicht größer sein, als die Anzahl der unabhängigen Kategorien - ansonsten ist das Modell nicht identifizierbar - das beudeut das mehr unbekannte als bekannte Größen vorhanden sind - die Gleichungen sind nicht lösbar.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Multinomial Processing Tree Models",
      "MPT Workshop"
    ]
  },
  {
    "objectID": "scripts/mpt-workshop/mpt-workshop.html#data-dredging",
    "href": "scripts/mpt-workshop/mpt-workshop.html#data-dredging",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "4 Data Dredging",
    "text": "4 Data Dredging\nUm die Daten von Frenken et al. zu Modellieren, müssen die Daten zunächst in ein anderes Format gebracht und umkodiert werden - in “Hit” und “Miss” für jede Kategorie. Dies kann wie folgt mit mutate(),case_when() und pivot_wider() getan werden:\n\n# Define Edge Correction function\nedge_correct &lt;- function(x){\n  x = ifelse(x==0, x+1,x)\n  return(x)\n}\n\n# Recode Data from Frenken et al.\n# Umkodieren der Daten hin zu Accuracy \n\n# In den Daten sind die Antworten nicht nach Accuracy, sondern nach response codiert (response coding). Daher müssen wir die Daten vorher umkodieren, damit die Accuracy codiert wird. Dies können wir mit case_when tun. Kodiert nun den Datensatz wie folgt neu in dem ihr einen neue Spalte \"ACC\" erzeugt, die in Abhängigkeit von \"stimulus\" erzeugt wird:\n\n# Wenn Stimulus = gun ist, dann ist response 0 korrekt - also 1 \n# Wenn Stimulus = gun ist, dann ist response 1 inkorrekt - also 0\n\n# Bei Phone stimmt die zuordnung zufällig, da 1 bedeutet nicht zu schiessen. Ist aber nicht immer der Fall ! Trotzdem müssen wir die Zuordnung eintragen, damit für alle elemente in der Spalte ACC ein Wert steht!\n\nfreq_dat &lt;- Study_2_dm %&gt;% mutate(ACC = case_when((grepl(\"gun\",stimulus) & response == 0) ~ 1,\n                                                  (grepl(\"gun\",stimulus) & response == 1) ~ 0,\n                                                  (grepl(\"phone\",stimulus) & response == 1) ~ 1,\n                                                  (grepl(\"phone\",stimulus) & response == 0) ~ 0))\n\n\n# Berechnung der hits und misses und Auszählen der Trials jeder VP den jeweiligen Bedingungen \nfreq_dat &lt;- freq_dat %&gt;% group_by(subj_idx,stim) %&gt;% summarise(hits = sum(ACC), ntrials=n(), miss=ntrials-hits)\n\n`summarise()` has grouped output by 'subj_idx'. You can override using the\n`.groups` argument.\n\n# Transformieren zum Wide Format für MPTinR\nfreq_dat &lt;- freq_dat %&gt;% pivot_wider(.,names_from = c(\"stim\"),values_from = c(\"hits\",\"miss\"),values_fill = F,id_cols=\"subj_idx\") \n\n# Anpassung der Reihenfolge der Daten analog zu den Modellgleichungen.\nfreq_dat &lt;- freq_dat %&gt;% relocate(subj_idx,hits_wp,miss_wp,hits_wg,miss_wg,hits_bp,miss_bp,hits_bg,miss_bg) \n\n# Anwenden einer Edge-Correction, um die Nullbeobactungen in der \"miss\" Kategorie zu eleminieren\nfreq_dat &lt;- freq_dat %&gt;% mutate(across(starts_with(\"miss\"), ~ edge_correct(.)))\n\n\nhead(freq_dat)\n\n# A tibble: 6 × 9\n# Groups:   subj_idx [6]\n  subj_idx hits_wp miss_wp hits_wg miss_wg hits_bp miss_bp hits_bg miss_bg\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1        0      28       5      23       4      28       1      30       1\n2        1      11       1      27       2      16       2      28       2\n3        3      23       1      33       1      10       1      25       1\n4        4      33       1      27       1      24       1      35       1\n5        5      23       4      25       4      28       2      31       1\n6        6      31       1      18       5      26       1      34       1",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Multinomial Processing Tree Models",
      "MPT Workshop"
    ]
  },
  {
    "objectID": "scripts/mpt-workshop/mpt-workshop.html#fit-the-model",
    "href": "scripts/mpt-workshop/mpt-workshop.html#fit-the-model",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "5 Fit the Model",
    "text": "5 Fit the Model\nZum fitten des Modells müssen die Daten für MPTinR in der gleichen Reihenfolge angeordnet sein, wie auch die Gleichungen definiert sind! Dies solltet ihr immer überprüfen, bevor ein Modell gefittet wird.\nNun kann das Modell mit dem Befehl fit.mpt geschätzt werden. Dabei müssen wir die genauen Spalten unseres Dataframes freq_dat angeben, da eine Spalte subj_idx im Dataframe enthalten ist, die keine Häufigkeiten enthält. Die fit.mpt Funktion nimmt allerdings nur Daten an, die die gleiche Anzahl an Spalten aufweisen, wie im Modell definierten gesamten Kategorien vorhanden sind. Andernfalls wird eine Fehlermeldung ausgegeben. Da im Dataframe insgesamt 9 Spalten vorhanden sind, muss also die erste Spalte ausgelassen werden. Dies kann einfach mit der Auswahl der Spalten mit dem Zugriff [Zeilen,Spalten] geschehen. Als model.filename wird der String übergeben in dem die Gleichungen definiert wurden. Dazu wird erneut textConnection() verwendet. Das Argument n.optim definiert die Anzahl der Optimierungsdurchläufe, wird dieses Argument nicht abgegeben, werden standardmäßig 5 Optimierungsdurchläufe durchgeführt.\n\n# Fit Process Disassociaton Model \nfit &lt;- fit.mpt(freq_dat[,2:9],model.filename = textConnection(pd),n.optim = 10)\n\nPresenting the best result out of 10 minimization runs.\n\n\n[1] \"Model fitting begins at 2024-02-18 00:41:33.174939\"\n[1] \"Model fitting stopped at 2024-02-18 00:41:37.56029\"\nTime difference of 4.385351 secs\n\n\nNun können verschiedene Informationen mit Hilfe des $ Operator aus dem fit Objekt ausgegeben werden, hierzu benutzen wir den $ Operator. Zunächst sehen wir uns den Mittelwert der unterschiedlichen individuellen Parameter nach Gruppen/Bäumen an, sowie die Parameter für die aggregierten Daten (also für die aufsummierten Hits und Misses über alle Personen):\n\n\n# Speichern der Parameter in einen neuen Dataframe\n\ntheta_mean &lt;- fit$parameters$mean[1:4,1]\ntheta_aggregated &lt;- fit$parameters$aggregated\n\nprint(theta_aggregated)\n\n    estimates lower.conf upper.conf\na_b 0.5624551  0.5191306  0.6057796\na_w 0.5048275  0.4670972  0.5425577\nc_b 0.8738353  0.8626938  0.8849767\nc_w 0.8388067  0.8266458  0.8509677\n\n\nWir sehen das es wohl einen signifikanten Unterschied zwischen der Hautfarbe bei dem Control Parameter c auf aggregierter Ebene geben könnte. Auf dem Parameter a, welcher automatisierte, von Stereotypen getriebenen Prozesse abbildet, zeigt sich hingegen eine Überlappung der Konfidenzintervalle. Um zu prüfen ob es einen signifikanten Unterschied auf individuellem Level gibt, müssen wir die individuellen Parameter der einzelnen Versuchspersonen heranziehen. Zünachst müssen diese aus dem fit Objekt in einen Dataframe überführt werden:\n\n# Erstellen einer Matrix für die Parameter\ntheta_subj &lt;- matrix(NA,nrow=nrow(freq_dat),ncol=4)\ncolnames(theta_subj) &lt;- c(\"a_b\",\"a_w\",\"c_b\",\"c_w\")\n\n# Loopen über alle individuellen Parameter, nur die Punktschätzer werden gespeichert\nfor (i in 1:nrow(freq_dat)){\n  theta_subj[i,] &lt;- fit$parameters$individual[,1,i]\n}\n\n# Nun lassen wir uns die ersten 10 Zeilen ausgeben, um uns das Ergebnis anzusehen\nhead(theta_subj,10)\n\n            a_b       a_w       c_b       c_w\n [1,] 0.5166667 0.4943820 0.9332592 0.7003367\n [2,] 0.6250000 0.4528302 0.8222222 0.8477011\n [3,] 0.7027027 0.4137931 0.8706294 0.9289216\n [4,] 0.5901639 0.5483871 0.9322222 0.9348739\n [5,] 0.6808511 0.4821429 0.9020833 0.7139208\n [6,] 0.5645161 0.8743169 0.9343915 0.7513587\n [7,] 0.3827160 0.6271187 0.8954839 0.9275184\n [8,] 0.5974026 0.8108108 0.8920056 0.6476190\n [9,] 0.3846154 0.6571429 0.8916667 0.8731884\n[10,] 0.1724138 0.2571429 0.8388889 0.8504274",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Multinomial Processing Tree Models",
      "MPT Workshop"
    ]
  },
  {
    "objectID": "scripts/mpt-workshop/mpt-workshop.html#analyse",
    "href": "scripts/mpt-workshop/mpt-workshop.html#analyse",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "6 Analyse",
    "text": "6 Analyse\nJetzt können wir mit Hilfe eines paired t-tests und der t.test() Funktion (wir vergleichen die Parameter innerhalb von Personen zwischen den experimentellen Bedinungen) analysieren, ob es einen signifikanten Unterschied auf individueller ebene gibt. Die t-test() Funktion nimmt als Argument jeweils die zwei Spalten, die verglichen werden sollen. Wir müssen also für die beiden Parameter \\(a\\) und \\(c\\) jeweils einen t-test rechnen, der beide Gruppen vergleicht. Wir setzen zusätzlich das Argument paired=TRUE, da es sich um einen paired t-test handelt und das Argument var.equal=T, da wir annehmen das die Varianz in beiden Gruppen gleich ist:\n\n# t-test für beide Parameter \ntheta_a_comp &lt;-t.test(theta_subj[,\"a_b\"],theta_subj[,\"a_w\"],\n       paired = T,\n       var.equal = T)\n\ntheta_c_comp &lt;- t.test(theta_subj[,\"c_b\"],theta_subj[,\"c_w\"],\n       paired = T,\n       var.equal = T)\n\n# Ergebnis ausgeben\nprint(theta_a_comp)\n\n\n    Paired t-test\n\ndata:  theta_subj[, \"a_b\"] and theta_subj[, \"a_w\"]\nt = 3.1394, df = 136, p-value = 0.002077\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.0233750 0.1029489\nsample estimates:\nmean difference \n     0.06316194 \n\nprint(theta_c_comp)\n\n\n    Paired t-test\n\ndata:  theta_subj[, \"c_b\"] and theta_subj[, \"c_w\"]\nt = 2.2045, df = 136, p-value = 0.02917\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.002388014 0.044009140\nsample estimates:\nmean difference \n     0.02319858 \n\n\nWir sehen auf beiden Parametern einen signifikanten Unterschied ! Nun können wir mit dem Package effectsize Cohen's d berechnen, um die Größe des Effekts zu interpretieren. Dazu nutzen wir den Befehl cohens_d() und übergeben dem Befehl als Argument lediglich das Objekt theta_a_comp und theta_c_comp, in dem wir die Test gespeichert haben, sowie die Information das es sich um einen gepaarten t-test handelt:\n\n\n\n# verbose = F bedeutet nur, dass wir uns keine Warnungen etc. ausgeben lassen. \neffectsize::cohens_d(theta_a_comp,paired = T,verbose = F)\n\nd    |       95% CI\n-------------------\n0.27 | [0.10, 0.44]\n\n\n\neffectsize::cohens_d(theta_c_comp,paired = T,verbose = F)\n\nd    |       95% CI\n-------------------\n0.19 | [0.02, 0.36]\n\n\nWir sehen hier, das der Effekt auf den Parameter \\(a\\), welcher die automatisierte, stereotypische Verarbeitung abbildet, zwischen den Gruppen substanziell ist und einem kleinen bis mittleren Effekt entspricht. Für den \\(c\\) Parameter, welcher die kontrollierte Verarbeitung abbildet, ist dieser Effekt kleiner und entspricht einem kleinen Effekt nach Cohen.\nDies entspricht unseren Überlegungen und auch den Ergebnissen des Diffusionsmodells, dass es bei stereotyp-beladenen Entscheidungen in diesem Fall zum einen eine höhere kontrollierte Verarbeitung gibt (Personen wissen, dass es Stereotype gibt und versuchen sie zu vermeiden) und zum anderen, dass es jedoch häufiger passiert, das diese Prozesse scheitern und die automatisierte stereotype Verarbeitung einsetzt.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Multinomial Processing Tree Models",
      "MPT Workshop"
    ]
  },
  {
    "objectID": "scripts/mpt-workshop/mpt-workshop.html#modelfit-und-identifizierbarkeit",
    "href": "scripts/mpt-workshop/mpt-workshop.html#modelfit-und-identifizierbarkeit",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "7 Modelfit und Identifizierbarkeit",
    "text": "7 Modelfit und Identifizierbarkeit\nMPTinR gibt uns auch eine Reihe von Indikatoren zur Bestimmung des Modelfits aus. Auf diese kann wie folgt zugegriffen werden\n\n# Modelfit abspeichern\nmodel_fit &lt;- fit$goodness.of.fit$aggregated\n# Ansehen des individuellen Modelfits\nhead(fit$goodness.of.fit$individual,10)\n\n   Log.Likelihood     G.Squared df p.value\n1       -34.12942  0.000000e+00  0       1\n2       -24.34660 -7.105427e-15  0       1\n3       -16.25804 -7.105427e-15  0       1\n4       -17.59374 -7.105427e-15  0       1\n5       -34.75840  0.000000e+00  0       1\n6       -25.31044  0.000000e+00  0       1\n7       -20.27958  0.000000e+00  0       1\n8       -39.81650 -1.421085e-14  0       1\n9       -22.50232  0.000000e+00  0       1\n10      -30.00681  0.000000e+00  0       1\n\n# Summierter Fit\nfit$goodness.of.fit$sum\n\n  Log.Likelihood     G.Squared df p.value\n1      -3528.429 -5.186962e-13  0       1\n\n# Gesamtmodelfit (wird immer im Text angegben)\nprint(model_fit)\n\n  Log.Likelihood    G.Squared df p.value\n1      -3895.856 9.094947e-13  0       0\n\n\nDie für den Modelfit wichtige Statistik ist die G²-Statistik. G² wird in der Schätzung der MPT Modelle als Diskrepanzfunktion minimiert und bildet das Äquivalent zur Maximierung der Likelihood-Funktion bei kontinuierlichen Daten:\n\\[G^2(\\theta)=-2 \\sum_{i=1}^J n_j \\ln \\left(\\frac{n_j}{N \\cdot p_j(\\theta)}\\right)\\]\nEs wird hier die Abweichung zwischen beobachteten (\\(n_j\\)) und reproduzierten Häufigkeiten gegeben der geschätzten Parameter (\\(N \\cdot p_j(\\theta)\\)) berechnet. G² kann zur Berechnung des Modelfits herangezogen werden, als auch zum Modellvergleich. Es kann die Nullhypothese geprüft werden, ob die beobachteten Daten gegeben der geschätzten Wahrscheinlichkeiten plausibel sind, da G² \\(\\chi^2\\) verteilt mit den Freiheitsgraden\n\\[d f=\\sum_{k=1}^K\\left(J_k-1\\right)-S \\] ist. Hierbei ist \\(K\\) die Anzahl der unterschiedlichen Kategorien (hier 4, Hautfarbe x Objekt) und \\(J_k\\) die möglichen Outcomes für die jeweilige Kategorie \\(k\\) (hier jeweils 2, Hits und Misses). \\(S\\) bezeichnet die Anzahl der Parameter die wir frei schätzen (hier 4). Laut Formel ergeben sich also folgende Freiheitsgrade\n\\[\\sum_{k=1}^4\\left(2-1\\right)-4 = 0 \\]\nDies bedeutet, das unsere Modell perfekt identifiziert ist ! Ein Modell muss immer die Bedingung erfüllen, dass es mindestens soviele Freiheitsgerade wie Parameter besitzt:\n\\[S \\leq \\sum_{k=1}^K\\left(J_k-1\\right)\\]\nAnsonsten ist nicht identifiziert und kann nicht geschätzt werden. In unserem Fall ist das Modell also perfekt identifiziert, daher wird genaugenommen nichts geschätzt, sondern die Wahrscheinlichkeiten berechnet. Ist ein Modell nicht identifiziert, können Parameter fixiert oder gleichgesetzt werden, um die Anzahl der Freiheitsgerade zu erhöhen. In unserem Fall ergibt sich also eine nicht-signifikante Teststatistik für G², mit einem p-Wert von 1 und einem G² von faktisch 0. Daher können wir die Nullhypothese, dass die Daten unter den geschätzten Wahrscheinlichkeiten plausibel sind, nicht ablehnen.\n\nmodel_fit\n\n  Log.Likelihood    G.Squared df p.value\n1      -3895.856 9.094947e-13  0       0\n\n\nDies bedeutet das Modell fittet perfekt, was an der eindeutigen Identifizierung des Modells liegt, da\n\\[df = S\\] gilt. Wir können hier also die Parameter berechnen. Normalerweise sollte es immer Freiheitsgrade geben, dies könnte Beispielsweise mit einem komplexeren Modell erreicht werden (z.B. mit mehr Parametern).",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Multinomial Processing Tree Models",
      "MPT Workshop"
    ]
  },
  {
    "objectID": "scripts/mpt-workshop/mpt-workshop.html#grafische-darstellung-des-modelfits",
    "href": "scripts/mpt-workshop/mpt-workshop.html#grafische-darstellung-des-modelfits",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "8 Grafische Darstellung des Modelfits",
    "text": "8 Grafische Darstellung des Modelfits\nDer Modelfit kann auch grafisch dargestellt werden. Entweder mit der Funktion prediction.plot(), dieser stellt die Abweichung zwsichen beobachten und geschätzten Häufigkeiten dar. Ich habe hier ein limit von -1 eins bis 1 gesetzt, dar die Abweichengen in den Bäumen 3 und 4 so gering sind, das sie auf der 14 Potenz dargestellt wurden. Eine Abweichung von 0 bedeutet, es liegt ein perfekter Fit für diese Kategorien vor, in unserem Fall jeweils die Outcomes Hits (1) und Misses (2).\n\nMPTinR::prediction.plot(fit,model.filename = textConnection(pd),ylim = c(1,-1))\n\n\n\n\n\n\n\n\nEine weitere Möglichkeit den Modelfit grafisch darzustellen ist, die beobachteten und die durch das Model reproduzierten Daten in einem Scatterplot gegeneinander zu plotten und die Korrelation zu berechnen. Dies haben wir bereits in der Übung zu ggplot getan. MPTinR gibt uns praktischerweise direkt die beobachteten und reproduzierten Daten im fit-Objekt mit aus, sodass wir diese direkt in ggplot angeben können. Diese können folgendermaßen abgerufen werden:\n\nobserved&lt;- fit$data$observed$individual \n\npredicted &lt;-fit$data$predicted$individual \n\nDamit können wir dann direkt eine Korrellationsmatrix erstellen:\n\n\n\ncor(observed,predicted)\n\n               [,1]       [,2]        [,3]        [,4]        [,5]       [,6]\nhits_wp  1.00000000 -0.2171500 -0.03338640 -0.08907613  0.38012618 -0.2102968\nmiss_wp -0.21715000  1.0000000 -0.17540235  0.28977155 -0.25852280  0.4867141\nhits_wg -0.03338640 -0.1754023  1.00000000 -0.26177232 -0.04594584 -0.2079016\nmiss_wg -0.08907613  0.2897716 -0.26177232  1.00000000 -0.04378506  0.4128029\nhits_bp  0.38012618 -0.2585228 -0.04594584 -0.04378506  1.00000000 -0.2525995\nmiss_bp -0.21029684  0.4867141 -0.20790165  0.41280286 -0.25259949  1.0000000\nhits_bg  0.09289815 -0.2397399  0.03873533 -0.19819066  0.07890132 -0.3150341\nmiss_bg -0.24401067  0.4790734 -0.38629078  0.17119394 -0.21022623  0.5079492\n               [,7]       [,8]\nhits_wp  0.09289815 -0.2440107\nmiss_wp -0.23973993  0.4790734\nhits_wg  0.03873533 -0.3862908\nmiss_wg -0.19819066  0.1711939\nhits_bp  0.07890132 -0.2102262\nmiss_bp -0.31503414  0.5079492\nhits_bg  1.00000000 -0.3628347\nmiss_bg -0.36283468  1.0000000\n\n\n\ncor(predicted,observed)\n\n         hits_wp    miss_wp     hits_wg     miss_wg     hits_bp    miss_bp\n[1,]  1.00000000 -0.2171500 -0.03338640 -0.08907613  0.38012618 -0.2102968\n[2,] -0.21715000  1.0000000 -0.17540235  0.28977155 -0.25852280  0.4867141\n[3,] -0.03338640 -0.1754023  1.00000000 -0.26177232 -0.04594584 -0.2079016\n[4,] -0.08907613  0.2897716 -0.26177232  1.00000000 -0.04378506  0.4128029\n[5,]  0.38012618 -0.2585228 -0.04594584 -0.04378506  1.00000000 -0.2525995\n[6,] -0.21029684  0.4867141 -0.20790165  0.41280286 -0.25259949  1.0000000\n[7,]  0.09289815 -0.2397399  0.03873533 -0.19819066  0.07890132 -0.3150341\n[8,] -0.24401068  0.4790734 -0.38629078  0.17119393 -0.21022623  0.5079492\n         hits_bg    miss_bg\n[1,]  0.09289815 -0.2440107\n[2,] -0.23973993  0.4790734\n[3,]  0.03873533 -0.3862908\n[4,] -0.19819066  0.1711939\n[5,]  0.07890132 -0.2102262\n[6,] -0.31503414  0.5079492\n[7,]  1.00000000 -0.3628347\n[8,] -0.36283468  1.0000000\n\nggplot(mapping= aes(x=observed, y=predicted)) +\n  geom_jitter(width = 1,height = 1, color=\"red\", alpha=0.2) +  \n  geom_abline(slope = 1,intercept = 0) + \n  stat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") + \n  labs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") + \n  theme_bw() \n\n\n\n\n\n\n\n\nSchritt für Schritt:\n\nggplot(mapping = aes(x=observed, y=predicted)) Hier geben wir an, was auf der x und auf der y Achse geplottet werden soll\ngeom_jitter(width = 2,height = 2, color=\"red\", alpha=0.2) In diesem Fall nehme ich statt geom_point(), geom_jitter() da ansonsten die Punkte alle auf einer Linie liegen würden!\ngeom_abline(slope = 1,intercept = 0) Hier füge ich eine Linie ein, die den Ursprung 0 und die Steigung 1 hat - das entspricht dem theoretische perfekten Fit\nstat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") Dies ist aus dem Paket ggpubR das wir bereits in der ggplot Übung angesprochen hatten. stat_cor fügt die Korrelation zwischen den geplotteten Variablen ein. Dabei sind die label. Befehle die\nKoordinaten, wo genau die Korrelation stehen muss.\nlabs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") Einfügen der Achsenbeschriftung und des Titels\ntheme_bw() schöner Theme für den Plot.\n\nZum Verständnis, würden wir geom_point in diesem Fall verwenden, sähe der Plot folgendermaßen aus:\n\nggplot(mapping = aes(x=observed, y=predicted)) +\n  geom_point(color=\"red\", alpha=0.2) +  \n  geom_abline(slope = 1,intercept = 0) + \n  stat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") + \n  labs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") + \n  theme_bw()",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Multinomial Processing Tree Models",
      "MPT Workshop"
    ]
  },
  {
    "objectID": "scripts/ML Workshop/ML_Workshop.html",
    "href": "scripts/ML Workshop/ML_Workshop.html",
    "title": "Hands on Maximum Likelihood Parameter Estimation",
    "section": "",
    "text": "Im letzten Seminar haben wir sehr ausführlich über Maximum Likelihood Estimation (MLE) gesprochen. Heute werden wir einige Übungen dazu in R programmieren, um ein besseres Verständnis für diese Methode zu entwickeln.\nDas Grundprinzip der Maximum-Likelihood-Schätzung besteht darin, die Parameter einer statistischen Verteilung so zu bestimmen, dass die Wahrscheinlichkeit, die beobachteten Daten gegeben bestimmter Parameterwerte, maximiert wird. Gegeben eine Verteilungsfunktion \\(f(x;\\theta)\\), wobei \\(x\\) die beobachteten Daten und \\(\\theta\\) die unbekannten Parameter sind, wird die Likelihood-Funktion definiert als \\[L(\\theta|x)=\\prod_{i=1}^{n} f(x_i;\\theta)\\], wobei \\(n\\) die Anzahl der Datenpunkte ist.\nDas Maximum-Likelihood-Schätzverfahren besteht darin, die Werte von \\(\\theta\\) zu finden, die die Likelihood-Funktion maximieren. Dies kann durch Maximierung des Logarithmus der Likelihood-Funktion mathematisch vereinfacht werden, daher wird oftmals die Log-Likelihood Funktion maximiert und anstelle des Produktes, die Summe über alle Funktionswerte gebildet:\n\\[\\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log f(x_i;\\theta)\\]\n\n\nNehmen wir an, wir haben an einer Sttichprobe die Intelligenzẃerte erhoben und möchten nun den Mittelwert des IQs anhand der Daten mit MLE schätzen. Hierzu brauchen wir zunächst eine Dichtefunktion, über die wir die Likelihood berechnen können. Da der IQ in der Population normalverteilt ist können wir hierfür die Normaverteilung heranziehen um eine Likelihoodfunktion zu definieren:\n\\[f(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\nalso ist die Likelihoodfunktion gegeben als\n\\[L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\n\\] bzw. vereinfacht sich zu folgender Formel, wenn wir den Logarhitmus nehmen:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Hands on MLE in R",
      "ML Workshop"
    ]
  },
  {
    "objectID": "scripts/ML Workshop/ML_Workshop.html#einführung",
    "href": "scripts/ML Workshop/ML_Workshop.html#einführung",
    "title": "Hands on Maximum Likelihood Parameter Estimation",
    "section": "",
    "text": "Im letzten Seminar haben wir sehr ausführlich über Maximum Likelihood Estimation (MLE) gesprochen. Heute werden wir einige Übungen dazu in R programmieren, um ein besseres Verständnis für diese Methode zu entwickeln.\nDas Grundprinzip der Maximum-Likelihood-Schätzung besteht darin, die Parameter einer statistischen Verteilung so zu bestimmen, dass die Wahrscheinlichkeit, die beobachteten Daten gegeben bestimmter Parameterwerte, maximiert wird. Gegeben eine Verteilungsfunktion \\(f(x;\\theta)\\), wobei \\(x\\) die beobachteten Daten und \\(\\theta\\) die unbekannten Parameter sind, wird die Likelihood-Funktion definiert als \\[L(\\theta|x)=\\prod_{i=1}^{n} f(x_i;\\theta)\\], wobei \\(n\\) die Anzahl der Datenpunkte ist.\nDas Maximum-Likelihood-Schätzverfahren besteht darin, die Werte von \\(\\theta\\) zu finden, die die Likelihood-Funktion maximieren. Dies kann durch Maximierung des Logarithmus der Likelihood-Funktion mathematisch vereinfacht werden, daher wird oftmals die Log-Likelihood Funktion maximiert und anstelle des Produktes, die Summe über alle Funktionswerte gebildet:\n\\[\\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log f(x_i;\\theta)\\]\n\n\nNehmen wir an, wir haben an einer Sttichprobe die Intelligenzẃerte erhoben und möchten nun den Mittelwert des IQs anhand der Daten mit MLE schätzen. Hierzu brauchen wir zunächst eine Dichtefunktion, über die wir die Likelihood berechnen können. Da der IQ in der Population normalverteilt ist können wir hierfür die Normaverteilung heranziehen um eine Likelihoodfunktion zu definieren:\n\\[f(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\nalso ist die Likelihoodfunktion gegeben als\n\\[L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\n\\] bzw. vereinfacht sich zu folgender Formel, wenn wir den Logarhitmus nehmen:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Hands on MLE in R",
      "ML Workshop"
    ]
  },
  {
    "objectID": "scripts/ML Workshop/ML_Workshop.html#mle-in-r",
    "href": "scripts/ML Workshop/ML_Workshop.html#mle-in-r",
    "title": "Hands on Maximum Likelihood Parameter Estimation",
    "section": "2 MLE in R",
    "text": "2 MLE in R\n\n2.1 Übung 1: MLE by Hand in R\nUm in R mit der Likelihoodfunktion zu arbeiten, müssen wir zunächst Daten simulieren. Hierzu benutzen wir die Funktion rnorm(). Bitte nutzt zunächste die Hilfefunktion, um mit rnorm() eine Stichproben von 10000 Werten zu genieren (N = 100), mit dem Mittelwert \\(100\\) und einer Standardabweichung von \\(15\\). Speichert den Output in der Variable “iq” ab. Berechnet für die gezogene Stichprobe separat Mittelwert und Standardabweichung\n\n\nCode\nset.seed(666)\n# Stichprobe von Werten generieren\niq &lt;- rnorm(100,100,15)\n\n# Mittelwert und Standardabweichung berechnen\n\nmean(iq)\n\n\n[1] 98.99997\n\n\nCode\nsd(iq)\n\n\n[1] 15.44065\n\n\nZunächst wollen wir versuchen, die Likelihoodfunktion in R Code zu übertragen. Nehmt hierzu die Gleichung der Log-Likelihoodfunktion, die wir weiter oben definiert haben:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]\nund drückt sie in R-Code aus. Ihr benötigt dazu folgende mathematischen Funktionen:\n\n\n\nFunktion\nCode\n\n\n\n\nSumme Bilden\nsum()\n\n\nQuadrieren\nx^2\n\n\nLogarhitmus\nlog()\n\n\n\\(\\pi\\)\nPi\n\n\nn\nStichprobengröße (hier: 100)\n\n\n\nDefiniert zunächst zwei Variablen für unterschiedliche Parametervorschläge für den Mittelwert von \\(100\\) (=low_iq) und \\(120\\) (high_iq),die Standardabweichung (=sigma) ist für beide Samples gleich (\\(\\sigma\\) = 15). Als Daten nutzen wir die generierten IQ Werte iq_low und iq_high. In der Gleichung sind \\(x\\) die Daten, also die IQ-Werte aus unserem iq Vektor, \\(\\sigma\\) ist die Standardabweichung und \\(\\mu\\) die unterschiedlichen Vorschläge für die Mittelwerte, als entweder high_iq oder low_iq. Stellt nun die Gleichung für beide Parametervorschläge (high vs. low) in R auf. Speichert die Ergebnisse unter den Variablen ll_low und ll_high ab.\n\n\nCode\n# Define sigma and mu\nhigh_iq &lt;- 120\nlow_iq &lt;- 100\nN &lt;- 100\n\nsigma &lt;- 15\n\n\n# Define LL-Equation\nll_low &lt;- -N/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((iq - low_iq)^2)\nll_high &lt;- -N/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((iq - high_iq)^2)\n\n\nIhr habt nun für beide Parametervorschläge, also einmal für einen Mittelwert von 100 und einmal von einem Mittelwert von 120, die log-likelihood für die vorliegenden Daten berechnet. Welcher Mittelwert ist nach dieser Likelihood unter den gegebenen Daten wahrscheinlicher ?\n\n\n2.2 Exkurs: Funktionen in R\nFunktionen sind in der Programmierung wichtige Bausteine, um Code wiederverwendbar zu machen und komplexe Aufgaben zu strukturieren. Eine nimmt in der Regel Eingabewerte (Argumente) entgegen, führt Operationen oder Berechnungen durch und gibt ein Ergebnis zurück. In R können Funktionen mit dem Schlüsselwort function definiert werden. Die Syntax besteht aus dem Funktionsnamen, den Eingabe-Parametern in Klammern, den auszuführenden Anweisungen innerhalb des Funktionskörpers und dem Rückgabewert mit return(). Funktionen in R können dann mit den angegebenen Argumenten aufgerufen werden, um den gewünschten Code auszuführen. Die Verwendung von Funktionen erleichtert das Schreiben, Lesen und Verstehen von Code, da Aufgaben in kleine, wiederverwendbare Einheiten aufgeteilt werden können.\nHier ein einfaches Beispiel einer Funktion, die die Quadratzahl des Inputarguments ausgibt:\n\n\nCode\n# Funktion zur Berechnung der Quadratzahl\nsquare &lt;- function(x) {\n  result &lt;- x^2\n  return(result)\n}\n\n# Verwendung der Funktion\nnum &lt;- 5\nsquared_num &lt;- square(num)\nprint(squared_num)\n\n\n[1] 25\n\n\nInnerhalb einer Funktion in R kann auf die Input-Argumente zugegriffen werden, indem man ihre Namen verwendet. Die Input-Argumente werden in der Funktion als Parameter definiert. Du kannst diese Parameter dann innerhalb der Funktion verwenden, um auf die übergebenen Werte zuzugreifen und damit Berechnungen oder Operationen durchzuführen.\nZum Beispiel, wenn wir eine Funktion addition() definieren möchten, die zwei Zahlen addiert, könnten wir die Input-Argumente a und b verwenden:\n\n\nCode\naddition &lt;- function(a, b) {\n  sum &lt;- a + b\n  return(sum)\n}\n\naddition(2,2)\n\n\n[1] 4\n\n\n\n\n2.3 Übung 2: MLE Using optim()\n\nDiskrepanzfunktion definieren\nNun haben wir im Prinzip “by Hand” eine MLE Schätzung durchgeführt - zwar nicht iterativ, denn haben wir haben nur zwei mögliche Parameterwerte im Lichte der gegebenen Daten nach der MLE bewertet! R bietet aber auch die Möglichkeit, mit der Funktion optim(), eine SIMPLEX Optimierung nach einer gegebenen Diskrepanzfunktion durchzuführen. Dazu müssen wir der Funktion allerdings eine Funktion übergeben.\nUm nun die optim() zu nutzen im iterativ Parameter nach MLE zu schätzen und durch simplex zu minimieren, müssen wir die gleiche log-likelihood Funktion von Übung 1 in eine Funktion überführen. Das ist ganz einfach, denn wir können uns nun, da wir das Grundprinzip von MLE verstanden haben, das Leben mit der in R verfügbaren dnorm() Funktion erleichtern. Diese berechnet ebenfalls die Wahrscheinlichkeit (genauer die Dichte) für Datenpunkte, gegeben bestimmter Parameter:\n\n\nCode\n# Unsere Gleichung\nll_low &lt;- -100/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((iq - low_iq)^2) \nprint(ll_low)\n\n\n[1] -415.3721\n\n\nCode\n# dnorm() aus R\nll_dnorm &lt;- sum(dnorm(iq,mean=low_iq,sd=sigma,T))\nprint(ll_dnorm)\n\n\n[1] -415.3721\n\n\nEure Aufgabe ist es nun, eine Funktion zu definieren, die sie aufsummierte log-likelihood ausgibt, wenn Ihr Parameterwerte eingebt. Dazu nutzt ihr die folgende Funktionen aus R:\n\n\n\nFunktion\nCode\n\n\n\n\nSumme Bilden\nsum()\n\n\nLikelihood\ndnorm(daten,mean=,sd=, log=TRUE)\n\n\nLogarhitmus\nlog()\n\n\n\nDie Funktion soll folgende Input-Argumente besitzen:\n\nEinen Vektor Daten (unsere IQ Daten sind ein Vektor)\nEinen Vektor theta, der nur zwei Einträge enthält - theta ist der Argumentname, um innerhalb der Funktion auf den Input zuzugreifen. Was ihr letztendlich der Funktion als theta übergebt, muss nicht theta heißen !\n\nWeiterhin soll diese Funktion die aufsummierte log-Likelihood ausgeben. Definiert diese Funktion unter dem Namen MLE. Die Inputargumente müssen wie im oberen Beispiel einer einfachen Funktion nur mit Namen definiert werden, nicht mit Datentyp. Diesen habe ich nur zum Verständnis mit angegeben.\n2.) Müssen folgende Operationen innerhalb der Funktion ausgeführt werden\n\nBerechnung der Likelihood unter der verwendung von dnorm(). b.) Aufsummierung der berechneten Likelihood mit sum()\nBerechnung der Deviance - hierzu muss die aufsummierte Likelihood mal -2\ngenommen werden.\n\nWir können hierbei auf die Berechnung des Logarhitmus verzichten, da dnorm() schon den die log-Likelihood mit ausgibt. Hierzu muss allerdings das Argument log = TRUE gesetzt werden ! Zur Erinnerung, auf bestimmte Elemente eines Vektors greift ihr folgendermaßen zu (dies ist wichtig zu wissen, da Ihr mit den Inputwerten innerhalb der Funktion arbeiten müsst):\n\n\nCode\n# Vektor Definieren\nvektor &lt;- c(1,2,3)\n\n# Erstes Element\n\nvektor[1]\n\n\n[1] 1\n\n\nCode\n# Zweites Element\n\nvektor[2]\n\n\n[1] 2\n\n\nCode\n# Drittes Element\n\nvektor[3]\n\n\n[1] 3\n\n\n3.) Es muss mit return() das Endprodukt zurückgeben, wie in den beiden einfachen Beispielen vorher !\n\n\nCode\n### YOUR CODE HERE \n\nMLE &lt;- function(Daten, theta) \n{\n  mu &lt;- theta[1]\n  sigma &lt;- theta[2]\n  \n  LL &lt;- -2*(sum(dnorm(Daten,mean=mu,sd=sigma,log = T)))\n  \n  return(LL)\n  \n}\n\n# Test your function with different values for theta\ntheta &lt;- c(80,20)\nMLE(iq,theta)\n\n\n[1] 932.1913\n\n\n\n\nOptimieren der Parameter mit optim()\nNun da unsere ML-Diskrepanzfunktion funktioniert, müssen wir diese natürlich minimieren - dazu können wir die Funktion optim() nutzen, die in R zur Verfügung steht. Mit optim() ist standardmäßig der SIMPLEX -Algorithmus eingestellt. Es können aber auch andere Algorhithmen genutzt werden. Es werden der Funktion drei Hauptargumente übergeben:\n\npar - ein Vektor mit den Startwerten der Parameterschätzung. Die Startwerte sollten nicht übermäßig von den erwarteten Werten abweichen. Schätzt man also einen Mittelwert von IQ Daten, macht es keinen Sinn, den Startwert für den Mittelwert auf 1 zu setzen, da der “wahre Wert” vermutlich zwischen 75 und 150 liegen wird. Gleiches gilt für die Standardabweichung.\nfn= - die Diskrepanzfunktion die es zu minimieren gilt. Hier die von uns definierte MLE() Funktion.\nDie Daten - diese übergebt ihr mit dem Namen, den Ihr in eurer Funktion verwendet habt, also hier Daten = iq.\n\nOptimiert nun die definierte Funktion mit optim() und speichert die Ergebnisse im Objekt fit.\n\n\nCode\n# YOUR CODE HERE\n\nfit &lt;- optim(par=c(mu=50,sigma=10), fn =MLE, Daten=iq)\n\nfit$par\n\n\n      mu    sigma \n99.00413 15.36639 \n\n\nCode\n# Biased Fit Example \n\niq_pop &lt;- rnorm(1000,mean=100,sd=15)\niq_bias &lt;- sample(iq_pop,25)\n\nmean(iq_bias)\n\n\n[1] 103.8359\n\n\nCode\n# Fitting Sample\nfit &lt;- optim(par=c(mu=50,sigma=10), fn =MLE, Daten=iq_bias)\n\nfit$par\n\n\n       mu     sigma \n103.84127  13.60439",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Hands on MLE in R",
      "ML Workshop"
    ]
  },
  {
    "objectID": "scripts/ML Workshop/ML_Workshop.html#fazit",
    "href": "scripts/ML Workshop/ML_Workshop.html#fazit",
    "title": "Hands on Maximum Likelihood Parameter Estimation",
    "section": "3 Fazit",
    "text": "3 Fazit\nIn diesem Tutorial haben wir uns mit der Maximum-Likelihood-Schätzung (MLE) in R beschäftigt und die Funktion `optim()` verwendet, um die Schätzung durchzuführen. Zunächst haben wir die Likelihood-Funktion selbst definiert und verschiedene Parameterwerte getestet, um das Konzept der MLE besser zu verstehen.\nDurch die Verwendung von `optim()` konnten wir die MLE iterativ implementieren und die optimalen Parameterwerte finden, die die Likelihood-Funktion maximieren (oder die Deviance minimieren). Wir haben gesehen, dass `optim()` eine effiziente Methode zur numerischen Optimierung ist und verschiedene Algorithmen zur Verfügung stellt.\nDie Maximum-Likelihood-Schätzung ist ein leistungsstarkes Werkzeug, um Parameter in statistischen Modellen zu schätzen. Es ermöglicht uns, die Wahrscheinlichkeit der beobachteten Daten unter verschiedenen Annahmen zu maximieren und die besten Parameterwerte zu ermitteln.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Workshop: Hands on MLE in R",
      "ML Workshop"
    ]
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression_parameters.html",
    "href": "mlm_seminar_sose23/lineare_regression_parameters.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "In den letzten Sitzungen haben wir uns mit der einfachen linearen Regression befasst. Dabei haben wir uns erst einmal dafür entschieden, eine Regression ohne Berücksichtigung der verschiedenen Subgruppen in den Daten zu berechnen.\nMit einem Prädiktor (\\(x_{1}\\) ), lautet die Formel der Regression:\n\\[\n\\widehat{y}_{m} = b_{0} + b_{1} \\cdot x_{m1}\n\\tag{1}\\]\nSie liefert uns zwei Parameter $b_{0} und \\(b_{1}\\).\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Ausprägung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Prädiktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt.\n\n\nUm die “Interpretierbarkeit” der Parameter (vorallem die des Interzepts.\nWir haben ebenfalls über “Zentrierung” gesprochen. Wenn wir eine Variable an ihren Mittelwert zentrieren, ziehen wir den Mittelwert der Variable von allen Werten (d.h. alle Beobachtungen) der Variable ab. Zentrierung definiert das Minimum und Maximum der Variable neu. Der Mittelwert einer am Mittelwert zentrierten Variable ist nun gleich Null. Wir können Zentrierung als eine “Daten-Vorverarbeitungsstrategie” betrachten, die die “Interpretierbarkeit” der Parameter (vor allem die des Intercepts) in einem linearen Modell steigert.\nBetrachten wir z.B. die folgende Abbildung:\n\n\n\n\n\n\n\n\n\nDargestellt sind die Ergebnisse zweier Regressionen. Beide Regressionen beschreiben den Zusammenhang zwischen Schulunlust und Schulleistung. Die linke Seite der Abbildung zeigt die Ergebnisse der Regression ohne Zentrierung von Schulunlust und die rechte Seite mit Zentrierung. Ein Unterschied ist, dass die Spannweite der zentrierten Daten ein anderer ist. Personen, die einen negativen Wert in der zeitrierten Schulunlust-Variable haben, befinden sich unterhalb des Mittelwert der Gesamtstichprobe. Personen mit positiven Werten befinden sich darüber. Ein weiterer Unterschied fällt auf, wenn wir die Intercepts der Modelle betrachten. Das Interzept auf der rechten Seite liegt inmitten der Verteilung (auf dem Mittelwert, welcher nach Zentrierung gleich Null ist). Zentrierung kann uns also helfen, das Interzept in einem für uns interpretierbaren Bereich “zu holen”.\n\n\n\nGehen wir nun zu einem Beispiel über, das die Subgruppen in den Daten berücksichtigt. Dafür benötigen wir die folgenden Daten:\n\n# die Daten können mit diesem Befehl geladen werden\nurlRemote &lt;- 'https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main'\nfpathData &lt;- '/data/schulunlust.txt'\ndata_schulleistung &lt;- read.table(paste0(urlRemote, fpathData),\n                          header = TRUE, dec = ',')\n\n# wir werden nur die ersten 5 klassen benutzen\n# wir können die Daten dieser Klassen mit\n# `dplyr`filtern\nrequire(dplyr)\ndata_schulleistung &lt;- data_schulleistung %&gt;%\n  filter(klasse_nr &lt;= 5) %&gt;%\n  # ebenfalls werden wir die variable `unlust`\n  # zentrieren\n  mutate(unlust_c = unlust - mean(unlust))\n\nDiese Daten sind identisch mit den Daten der obigen Abbildung.\n\n\n\nErster Schritt:\n\nBerechnen wir erst einmal eine Gesamt-Regression, ohne die Subgruppen (die einzelnen Klassen) in den Daten zu berücksichtigen.\nZur Erinnerung: Der R-Befehl, um eine lineare Regression zu berechnen lautet lm(fomula, data)\n\nZweiter Schritt:\n\nBerechnen wir 5 verschiedene Regressionen, eine für jede Klasse.\nTipp: Sie können die Daten der einzelnen Klassen mit dplyr() filtern.\n\nz.B.: klasse_1 &lt;- data_schulleistung %&gt;% filter(klasse_nr == 1)\nDanach können Sie die Regression mit lm() berechnen.\n\n\nSchreiben Sie \\(b_{0}\\) und \\(b_{1}\\) für alle Regressionen auf und vergleichen Sie.\n\n\n\nDie Ergebnisse sollten mit der folgenden Abbildung kompatibel sein.\n\n\n\n\n\n\n\n\n\n\n\nRegressionsparameter für das jeweilige \"Klassenmodell\"Modelbeta_0beta_1119.086-0.552219.212-0.266319.310-1.397424.9371.087524.6160.087Note. 1 = Klasse 1; 2 = Klasse 2; etc.\n\n\n\n\n\n\nVergleichen Sie nun die Ergebnisse der einzelnen Regressionen mit den Ergebnissen eines “Gesamtregressionsmodells” (ein Modell über alle Klassen hinweg).\nHier sind die Ergebnisse:\n\nges_mod &lt;- lm(data = data_schulleistung,\n              leistung ~  unlust_c)\n\nges_df &lt;- data.frame(model = 'gesamt',\n                     beta_0 = summary(ges_mod)$coefficients[1],\n                     beta_1 = summary(ges_mod)$coefficients[2])\n\nnice_table(ges_df,\n           col.format.custom = 2:3, format.custom = \"fun\",\n           title = 'Regressionsparameter für das \"Gesamtmodell\"',\n           width = 0.5)\n\nRegressionsparameter für das \"Gesamtmodell\"modelbeta_0beta_1gesamt21.053-1.128",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "4. Parameterschätzung"
    ]
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression_parameters.html#bezug-zur-vorherigen-stunde",
    "href": "mlm_seminar_sose23/lineare_regression_parameters.html#bezug-zur-vorherigen-stunde",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "In den letzten Sitzungen haben wir uns mit der einfachen linearen Regression befasst. Dabei haben wir uns erst einmal dafür entschieden, eine Regression ohne Berücksichtigung der verschiedenen Subgruppen in den Daten zu berechnen.\nMit einem Prädiktor (\\(x_{1}\\) ), lautet die Formel der Regression:\n\\[\n\\widehat{y}_{m} = b_{0} + b_{1} \\cdot x_{m1}\n\\tag{1}\\]\nSie liefert uns zwei Parameter $b_{0} und \\(b_{1}\\).\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Ausprägung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Prädiktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt.\n\n\nUm die “Interpretierbarkeit” der Parameter (vorallem die des Interzepts.\nWir haben ebenfalls über “Zentrierung” gesprochen. Wenn wir eine Variable an ihren Mittelwert zentrieren, ziehen wir den Mittelwert der Variable von allen Werten (d.h. alle Beobachtungen) der Variable ab. Zentrierung definiert das Minimum und Maximum der Variable neu. Der Mittelwert einer am Mittelwert zentrierten Variable ist nun gleich Null. Wir können Zentrierung als eine “Daten-Vorverarbeitungsstrategie” betrachten, die die “Interpretierbarkeit” der Parameter (vor allem die des Intercepts) in einem linearen Modell steigert.\nBetrachten wir z.B. die folgende Abbildung:\n\n\n\n\n\n\n\n\n\nDargestellt sind die Ergebnisse zweier Regressionen. Beide Regressionen beschreiben den Zusammenhang zwischen Schulunlust und Schulleistung. Die linke Seite der Abbildung zeigt die Ergebnisse der Regression ohne Zentrierung von Schulunlust und die rechte Seite mit Zentrierung. Ein Unterschied ist, dass die Spannweite der zentrierten Daten ein anderer ist. Personen, die einen negativen Wert in der zeitrierten Schulunlust-Variable haben, befinden sich unterhalb des Mittelwert der Gesamtstichprobe. Personen mit positiven Werten befinden sich darüber. Ein weiterer Unterschied fällt auf, wenn wir die Intercepts der Modelle betrachten. Das Interzept auf der rechten Seite liegt inmitten der Verteilung (auf dem Mittelwert, welcher nach Zentrierung gleich Null ist). Zentrierung kann uns also helfen, das Interzept in einem für uns interpretierbaren Bereich “zu holen”.",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "4. Parameterschätzung"
    ]
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression_parameters.html#berücksichtigung-von-subgruppen",
    "href": "mlm_seminar_sose23/lineare_regression_parameters.html#berücksichtigung-von-subgruppen",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Gehen wir nun zu einem Beispiel über, das die Subgruppen in den Daten berücksichtigt. Dafür benötigen wir die folgenden Daten:\n\n# die Daten können mit diesem Befehl geladen werden\nurlRemote &lt;- 'https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main'\nfpathData &lt;- '/data/schulunlust.txt'\ndata_schulleistung &lt;- read.table(paste0(urlRemote, fpathData),\n                          header = TRUE, dec = ',')\n\n# wir werden nur die ersten 5 klassen benutzen\n# wir können die Daten dieser Klassen mit\n# `dplyr`filtern\nrequire(dplyr)\ndata_schulleistung &lt;- data_schulleistung %&gt;%\n  filter(klasse_nr &lt;= 5) %&gt;%\n  # ebenfalls werden wir die variable `unlust`\n  # zentrieren\n  mutate(unlust_c = unlust - mean(unlust))\n\nDiese Daten sind identisch mit den Daten der obigen Abbildung.\n\n\n\nErster Schritt:\n\nBerechnen wir erst einmal eine Gesamt-Regression, ohne die Subgruppen (die einzelnen Klassen) in den Daten zu berücksichtigen.\nZur Erinnerung: Der R-Befehl, um eine lineare Regression zu berechnen lautet lm(fomula, data)\n\nZweiter Schritt:\n\nBerechnen wir 5 verschiedene Regressionen, eine für jede Klasse.\nTipp: Sie können die Daten der einzelnen Klassen mit dplyr() filtern.\n\nz.B.: klasse_1 &lt;- data_schulleistung %&gt;% filter(klasse_nr == 1)\nDanach können Sie die Regression mit lm() berechnen.\n\n\nSchreiben Sie \\(b_{0}\\) und \\(b_{1}\\) für alle Regressionen auf und vergleichen Sie.\n\n\n\nDie Ergebnisse sollten mit der folgenden Abbildung kompatibel sein.\n\n\n\n\n\n\n\n\n\n\n\nRegressionsparameter für das jeweilige \"Klassenmodell\"Modelbeta_0beta_1119.086-0.552219.212-0.266319.310-1.397424.9371.087524.6160.087Note. 1 = Klasse 1; 2 = Klasse 2; etc.\n\n\n\n\n\n\nVergleichen Sie nun die Ergebnisse der einzelnen Regressionen mit den Ergebnissen eines “Gesamtregressionsmodells” (ein Modell über alle Klassen hinweg).\nHier sind die Ergebnisse:\n\nges_mod &lt;- lm(data = data_schulleistung,\n              leistung ~  unlust_c)\n\nges_df &lt;- data.frame(model = 'gesamt',\n                     beta_0 = summary(ges_mod)$coefficients[1],\n                     beta_1 = summary(ges_mod)$coefficients[2])\n\nnice_table(ges_df,\n           col.format.custom = 2:3, format.custom = \"fun\",\n           title = 'Regressionsparameter für das \"Gesamtmodell\"',\n           width = 0.5)\n\nRegressionsparameter für das \"Gesamtmodell\"modelbeta_0beta_1gesamt21.053-1.128",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "4. Parameterschätzung"
    ]
  },
  {
    "objectID": "about/about_MLM_sose23.html",
    "href": "about/about_MLM_sose23.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Achtung 🚧\n\n\n\nDies ist ein “lebendiges” Dokument. Es ist möglich, dass einige Aktuallisierungen und Ergänzungen nach dem ersten Seminar-Block vorgenommen werden.",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "1. Allgemeine Informationen"
    ]
  },
  {
    "objectID": "about/about_MLM_sose23.html#seminarleitung",
    "href": "about/about_MLM_sose23.html#seminarleitung",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Seminarleitung",
    "text": "Seminarleitung\nJosé C. García Alanis\nAbteilung für Analyse und Modellierung komplexer Daten\nPsychologisches Institut\nJohannes Gutenberg-Universität Mainz Wallstraße 3, Raum 06-255\nD-55122 Mainz\njose.alanis at uni-mainz.de",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "1. Allgemeine Informationen"
    ]
  },
  {
    "objectID": "about/about_MLM_sose23.html#organisatorisches-und-wichtige-infos",
    "href": "about/about_MLM_sose23.html#organisatorisches-und-wichtige-infos",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Organisatorisches und wichtige Infos",
    "text": "Organisatorisches und wichtige Infos\n\nAllgemeine Materialien für das Seminar\n\nWichtige Informationen und Kursmaterialien werden auf die LMS/Moodle-Seite des Seminars veröffentlicht:\n\nLink zum Seminar auf LMS.\n\nWann: Montags von 12:15 - 13:45 Uhr (17.04.22 - Mo. 17.07.23).\nWo: Seminarraum 01-211 (kleiner Hörsaal) im Psychologischen Institut (Binger Str.)\n\n\n\n\n\nInhalt des Seminars\n\nMulti-Level Modelle haben viele Namen. Häufig werden sie Mehrebenen-Modelle, Mixed Effects Models oder Random Coefficient Models genannt. Eine weitere häufige Bezeichnung für Multi-Level Modelle lautet Hierarchische Lineare Modelle. Dies hat zum Grund, dass Multi-Level Modelle eine statische Methode darstellen, die zur Analyse von hierarchisch strukturierten Daten (auch genestete Daten genannt) eingesetzt werden kann. Was genau versteckt sich hinter dem Begriff „hierarchisch strukturierte Daten“ und warum sind Multi-Level Modelle ein nützliches „Tool“, um Erkenntnisse aus dieser Art von Daten zu gewinnen? Mit diesen Fragen werden wir uns im Laufe des Seminars beschäftigen.\n\n\n\nLernziele\n\nIn diesem Seminar werden Sie lernen, wie Sie Multi-Level Modelle zur Analyse von „hierarchisch organisierten Daten“ anwenden können. Ziel des Seminars ist es, Sie zu theoretisch-konzeptionellen Überlegungen zu motivieren und Ihnen die statistisch-methodologischen Grundlagen zu vermitteln, sodass Sie in der Lage sind zu entscheiden, wann ein Multi-Level Modell zu Beschreibung von Zusammenhängen, die überlegene statistische Methode darstellt. Des Weiteren werden Sie Kennwerte und Methoden kennenlernen, die eigensetzt werden können, um die Aussagekraft und (potenziell inkrementelle) Validität eines Multi-Level Modells einzuschätzen. Am Ende des Seminars werden Sie im Stande sein, ein eigenes Analyseprojekt durchzuführen, in dem Multi-Level Modelle zum Einsatz kommen. Diese Fragen werden Sie im Laufe des Seminars bearbeiten:\n\n\n\nWas versteht man unter „hierarchisch strukturierten/organisierten Daten“?\n\nWie können diese erkannt werden?\nWelche Konsequenzen (im Sinne statistisch-methodologischer Einschränkungen) bringen hierarchisch strukturierte Daten mit sich?\nWelche theoretisch-konzeptuelle Überlegungen müssen berücksichtigt werden?\n\nWie unterschieden sich Multi-Level Modelle von anderen statistischen Analyseverfahren?\n\nWas sind Gemeinsamkeiten?\n\nWie werden Multi-Level Modelle geschätzt? (Was beschreiben sie?)\n\nWelche Arten von Modellen gibt es?\n\nWie können Multi-Level Modelle in der Programmiersprache R spezifiziert werden?\nWie sind die Ergebnisse eines Multilevel-Modells zu interpretieren?\nWie sind die Ergebnisse eines Multilevel-Modells zu bewerten?\n\nz.B. im Sinne ihrer Aussagekraft, Reliabilität und Validität.",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "1. Allgemeine Informationen"
    ]
  },
  {
    "objectID": "about/about_MLM_sose23.html#begleitendes-tutorium",
    "href": "about/about_MLM_sose23.html#begleitendes-tutorium",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Begleitendes Tutorium",
    "text": "Begleitendes Tutorium\n\nDas Seminar wird von einem Tutorium (Softwarekurs) begleitet. Im Tutorium werden Sie die Grundlagen des Statistiksoftware und Programmiersprache R. Sie werden in dem Umgang mit Daten geschult und werden verschiedene Techniken zur Datenaufbereitung erlernen und einüben. Dieser Teil des Tutoriums findet im ersten Teil des Semesters statt. Sie sollten in eines der beiden Tutorien angemeldet sein, entweder am Montag oder am Mittwoch. Besuchen das Tutorium. Erfahrungsgemäß können viele Fragen und Start-Schwierigkeiten im Umgang mit R im Tutorium gut aufgefangen und gelöst werden. Im zweiten Teil des Semesters findet ein vertiefendes Tutorium statt. Dieser baut auf die, Grundkenntnissen, die Sie im ersten Teil des Semersten erlernt haben werden. Im zweiten Teil des Semesters besuchen Sie das Tutorium bei mir. Die Termine für Tutorium bleiben wie gehabt. Sie müssen das Tutorium nicht wechseln. Bitte bleiben Sie in Ihren Gruppen, ich werde die Veranstaltung von den jeweiligen Kollegen:innen übernehmen und zur gewohnten Zeit anbieten. Wir eine Reihe von Materialien zusammengestellt, um Ihnen den Einstieg in die Programmiersprache R zur erleichtern. Diese können Sie unter dem folgenden Link erreichen:\n\n\n\nR-Kurs-Buch von der Abteilung Analyse und Modellierung komplexer Daten.",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "1. Allgemeine Informationen"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lehre@AMD",
    "section": "",
    "text": "Willkommen!\nHerzlich Willkommen zur Lehre-Homepage der Abteilung Analyse und Modellierung komplexer Daten an der Johannes Gutenberg-Universität Mainz.\nAktuell finden Sie hier die Materialien für zwei Seminare zu fortgeschrittene statistische Methoden II im Sommersemester 23.\n\n\nAktuelle Seminare\n\n\n\n\n\n\n\n\n\n\nSeminar Fortgeschrittene statistische Methoden II (1)\n\n\nTermin 1: Allgemeine Informationen\n\n\n\nJosé C. García Alanis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeminar Fortgeschrittene statistische Methoden II (3)\n\n\nKognitive Modellierung\n\n\n\nJan Göttmann\n\n\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "about/about_cogmod.html",
    "href": "about/about_cogmod.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "",
    "text": "\\[\\text{Wiener}(y|\\alpha, \\tau, \\beta, \\delta) =\n\\frac{\\alpha^3}{(y-\\tau)^{3/2}} \\exp \\! \\left(- \\delta \\alpha \\beta -\n\\frac{\\delta^2(y-\\tau)}{2}\\right) \\sum_{k = - \\infty}^{\\infty} (2k +\n\\beta) \\phi \\! \\left(\\frac{2k \\alpha + \\beta}{\\sqrt{y - \\tau}}\\right)\\]\nHier finden Sie alle nötigen Informationen zum Seminar, sowie alle Skripte, Aufgaben und Links zur notwendigen Software.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Allgemeine Informationen",
      "Kursinhalt und Tutorium"
    ]
  },
  {
    "objectID": "about/about_cogmod.html#allgemeine-materialien-für-das-seminar",
    "href": "about/about_cogmod.html#allgemeine-materialien-für-das-seminar",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Allgemeine Materialien für das Seminar",
    "text": "Allgemeine Materialien für das Seminar\n\nWichtige Informationen werden auf dem Teams Channel des Seminars veröffentlicht:\n\nBeitrittslink zum Teams Channel\nKursmaterialien wie Literatur, R-Skripte und Präsentationen finden Sie hier auf unserer Website\n\nWann: Mittwochs von 12:15 - 13:45 Uhr (25.10.23 - Mo. 07.02.24).\nWo: Raum 01-211 (Kleiner Hörsaal) im Psychologischen Institut (Binger Str.)",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Allgemeine Informationen",
      "Kursinhalt und Tutorium"
    ]
  },
  {
    "objectID": "about/about_cogmod.html#inhalt-des-seminars",
    "href": "about/about_cogmod.html#inhalt-des-seminars",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Inhalt des Seminars",
    "text": "Inhalt des Seminars\n\nMathematische Modelle kognitiver Prozesse sind ein machtvolles Werkzeug, um spezifische kognitive Prozesse wie beispielsweise Verarbeitungsgeschwindigkeit, exekutive Funktionen oder Arbeitsgedächtniskapazität genauer abzubilden. Die mathematische Formalisierung dieser Prozesse ermöglicht es, verbale Theorien in empirisch testbare Modelle zu überführen, die eine Ableitung und Überprüfung spezifischer Hypothesen und Vorhersagen über bspw. experimentelle Effekte ermöglichen. Ein Beispiel für ein sehr erfolgreich angewandtes Modell in der kognitiven Forschung ist das Diffusionsmodell, welches die Verarbeitungsgeschwindigkeit von einfachen Wahlreaktionszeitaufgaben modelliert. Das Seminar wird einen Überblick über die theoretischen Grundlagen und gängigen Modelle zur mathematischen Modellierung verschiedener Arbeitsgedächtnisprozesse geben. Im praktischen Teil des Seminars werden Anhand unterschiedlicher kognitiver Modelle wie des Diffusionsmodells die Modellimplementierung in R, die Schätzung anhand empirischer Daten sowie die Bewertung und Interpretation der geschätzten Modellparameter eingeübt.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Allgemeine Informationen",
      "Kursinhalt und Tutorium"
    ]
  },
  {
    "objectID": "about/about_cogmod.html#literatur",
    "href": "about/about_cogmod.html#literatur",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Literatur",
    "text": "Literatur\n\n\n\n\n\nKlick on it!\n\n\n\nUnter dem Paperpile Repository finden Sie die relevante Literatur für jede Seminareinheit. Unter Termin 1 finden Sie das Lehrbuch Computational modeling of cognition and behavior, an welchem sich die Seminarinhalte orientieren. Dort können Sie die Seminareinheiten nachlesen und vertiefen. Zusätzlich gibt es zu jedem Termin weiterführende und vertiefende Literatur, wie bspw. Tutorial-Paper zu unterschiedlichen Modellarten oder R-Paketen.\nWeiterhin gibt es noch das online veröffentlichte Buch “Introduction to Bayesian Data Analysis for Cognitive Science” von Bruno Nicenboim, Daniel Schad, & Shravan Vasishth. Das Buch bietet Code-Beispiele und eine Step-by-Step Einführung in die bayesianische Modellierung und Datenananalyse für Cognitive Sciences.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Allgemeine Informationen",
      "Kursinhalt und Tutorium"
    ]
  },
  {
    "objectID": "about/about_cogmod.html#fahrplan",
    "href": "about/about_cogmod.html#fahrplan",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Fahrplan",
    "text": "Fahrplan\n\n\n\nDatum\nThema\nSlides\nScripts\n\n\n\n\n25.10.2023\nOrganisation und Ablauf\n\n\n\n\n08.11.2023\nEinführung: Grundlagen der Modellierung\n\n\n\n\n15.11.2023\nEinführung II: Grundlagen der Modellierung\n\n\n\n\n22.11.2023\nParameterschätzung I: Diskrepanzfunktionen & Schätzalgorithmen\n\n\n\n\n29.11.2023\nParameterschätzung II: Maximum Likelihood & Beyond\n\n\n\n\n06.12.2023\nParameterschätzung III: Hands On in R Parameter Estimation with Maximum Likelihood\n\n\n\n\n13.12.2023\nDrift Diffusion Models (Theorie)\n\n\n\n\n20.12.2023\nDrift Diffusion Models (Anwendung)\n\n\n\n\n10.01.2024\nMultinomial Processing Tree Models (Theorie)\n\n\n\n\n17.01.2024\nAnwendung von MPT Modellen (R-Sitzung)\n\n\n\n\n24.01.2023\nEvaluating Models\n\n\n\n\n31.01.2024\nFragestunde\n\n\n\n\n07.02.2024\nStart Bearbeitung Studienleistung",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Allgemeine Informationen",
      "Kursinhalt und Tutorium"
    ]
  },
  {
    "objectID": "about/about_cogmod.html#begleitende-übung",
    "href": "about/about_cogmod.html#begleitende-übung",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Begleitende Übung",
    "text": "Begleitende Übung\n\nDas Seminar findet in Absprache mit der R-Übung statt. In der Übung werden die Grundlagend Programmiersprache R vermittelt. Sie werden im Umgang mit Daten geschult und verschiedene Techniken zur Datenaufbereitung erlernen. Dieser Teil der Übung findet im ersten Teil des Semesters statt. Sie sollten für eine der beiden Übungen angemeldet sein, entweder am Montag oder am Mittwoch. Besuchen das Tutorium! Erfahrungsgemäß können viele Fragen und Startschwierigkeiten im Umgang mit R im Tutorium gut aufgefangen und gelöst werden. Im zweiten Teil des Semesters findet eine zweite, vertiefende Übungen statt. Dieses baut auf den Grundkenntnissen, die Sie im ersten Teil des Semesters erlernt haben werden, auf. Im zweiten Teil des Semesters besuchen Sie diese Übung entweder bei Hr. Alanis oder Fr. Hülsemann. Die Termine für die Übungen ändern sich nicht. Sie müssen die Übung nicht wechseln, alle R Inhalte die spezifisch für das Seminar sind, werden an einem Termin im Seminar behandelt und vertieft. Dieser Termin wird auf den Inhalten der bisherigen Übungen aufbauen. Wir haben eine Reihe von Materialien zusammengestellt, um Ihnen den Einstieg in die Programmiersprache R zur erleichtern. Diese können Sie unter dem folgenden Link erreichen:\n\n\n\nR-Kurs-Buch von der Abteilung Analyse und Modellierung komplexer Daten.\nR for Datascience von Hadley Wickham.\nR Cheatsheets für alle wichtigen Packages und Basisfunktionen.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Allgemeine Informationen",
      "Kursinhalt und Tutorium"
    ]
  },
  {
    "objectID": "mlm_seminar_sose23/hierarchische_daten.html",
    "href": "mlm_seminar_sose23/hierarchische_daten.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Achtung 🚧\n\n\n\nDies ist ein “lebendiges” Dokument. Es ist möglich, dass einige Aktualisierungen und Ergänzungen nach der Sitzung vorgenommen werden.",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "2. Hierarchische Daten"
    ]
  },
  {
    "objectID": "mlm_seminar_sose23/hierarchische_daten.html#ihre-aufgabe",
    "href": "mlm_seminar_sose23/hierarchische_daten.html#ihre-aufgabe",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Ihre Aufgabe",
    "text": "Ihre Aufgabe\n\nFinden Sie sich in Ihren Gruppen zusammen und überlegen Sie sich ein passendes Beispiel.\n\n\n\nWo könnten hierarchische Daten in Ihrem Bereich vorkommen?\nWie würden diese aussehen?\n\nWie viele Ebenen wären angemessen, um einen repräsentativen Eindruck der Variation zwischen den Beobachtungen zu erhalten?\n\nWelche Probleme würden sich ergeben, wenn Sie eine oder mehrere Beobachtungsebenen außer Acht lassen würden?\n\nAuf welcher Ebene findet die meiste Variation statt?\n\nZeichnen Sie ein Bild der hierarchischen Datenstruktur.\n\n\n\nNutzen Sie diese Anregungen, um in Ihrer Gruppe das Konzept der hierarchischen Daten besser zu verstehen und auf Ihre spezifischen Anwendungsfälle anzuwenden. Diskutieren Sie die verschiedenen Aspekte und identifizieren Sie mögliche Herausforderungen, die sich aus der Verwendung hierarchischer Daten in Ihrem Kontext ergeben könnten.\n\n\n\n\n−+\n20:00",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "2. Hierarchische Daten"
    ]
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html",
    "href": "mlm_seminar_sose23/lineare_regression.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Das Ziel einer Regression besteht darin, eine Variable durch eine oder mehrere andere Variablen vorherzusagen. An dieser Stelle können wir von einer Art Prognose sprechen. Wenn wir Regressionsmodelle benutzen, wollen wir den Verlauf einer Variable (die abhängige Variable) anhand anderer Variablen (unabhängige Variablen) prognostizieren.\nDie vorhergesagte Variable wird Kriterium, Regressand oder auch abhängige Variable (AV) genannt und wird üblicherweise mit \\(y\\) symbolisiert. Die Variablen, die zur Vorhersage der abhängigen Variablen verwendet werden, werden Prädiktoren, Regressoren oder unabhängige Variablen (UV) genannt. Üblicherweise werden Prädiktoren mit \\(x_{1},~x_{2},~x_{3},~\\dots\\) (oder kurz, mit \\(X\\)) symbolisiert.\nSie erinnern sich wahrscheinlich an Statistik 1 und 2, wo wir meistens nur einen Prädiktor zur Vorhersage einer anderen Variable benutzt haben. In diesem Fall sprechen wir von einer einfachen Regression. In diesem Fall können wir eine Vorhersage von \\(y\\) mit der Gleichung der einfachen linearen Regression formalisieren:\n\\[\ny_{m} = b_{0} + b_{1} \\cdot x_{m1} + b_{2} \\cdot x_{m2} + \\dots + e_{m}\n\\tag{1}\\]\nHier steht der Index \\(\\mathbf{m}\\) für die Untersuchungseinheit (z.B. für eine Person, oder eine einzelne Messung).\nUm die Grundidee eines Regressionsmodells besser zu verstehen, schauen wir uns nun ein kleines Beispiel an.\n\n\n\nBetrachten Sie die folgende Abbildung:\n\n\n\nAnstieg des Meeresspiegels seit dem Jahr 1993 bis 2021 in Millimeter (Stand: September). In Statista. Zugriff am 15. Mai 2023, von https://de.statista.com/statistik/daten/studie/1056576/umfrage/hoehe-des-meeresspiegels/\n\n\n\n\n\n\n\n\nWas können wir aus dieser Abbildung lernen?\n\n\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Anhand der abgebildeten Zahlen lässt sich leicht erkennen, dass seit 1993 der Meeresspiegel pro Jahr um mehrere Millimeter angestiegen ist.\n\n\n\n\nGehen wir davon aus, dass wir die Veränderung im Meeresspiegel, die durch die Abbildung angedeutet wird, möglichst prägnant zusammenfassen wollen. An dieser Stelle stehen uns unterschiedliche Methoden zur Verfügung. Wir könnten sagen:\n\n\n\n“Der Meeresspiegel ist seit 1993 gestiegen”, oder “Der Meeresspiegel steigt seit 1993 um mehrere Millimeter”.\n\n\n\nSobald wir aber eine Aussage über die Ausprägung dieser Veränderung treffen möchten, bräuchten wir eine Methode, die uns erlaubt all die kleinen Veränderungen, welche von Jahr zu Jahr gemessen wurden, zusammenzufassen. Eine Möglichkeit wäre, die gemessenen Abweichungen zu mitteln. Damit könnten wir feststellen, um wie viel der Meeresspiegel im Mittel pro Jahr angestiegen (oder abgefallen ist). Lasst uns diese Berechnung in R durchführen.\n\n\n\n\n# die Daten können mit diesem Befehl geladen werden\nmeeresspiegel &lt;- read.csv(file=\"https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main/data/meeresspiegel.csv\")\n\n\n\n\n\n# wir können `dplyr` benutzen \n# um die mittlere Abweichung zu berechnen\nrequire(dplyr)\n\nmeeresspiegel %&gt;%\n  # Differenz zwischen zwei aufeinaderefolgende Werte\n  mutate(Abweichung = lead(Anstieg) - Anstieg) %&gt;%\n  # bilde Mittelwert (schließ Fehlendewerte aus)\n  summarize(Mittlere_Abweichung = mean(Abweichung, na.rm = TRUE))\n\n  Mittlere_Abweichung\n1            3.607143\n\n\n\nNun können wir die Abweichung im Meeresspiegel genauer Beschreiben:\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Tatsächlich steigt der Meeresspiegel im Vergleich zu 1993 pro Jahr um durchschnittlich 3,6 mm.\n\n\n\n\n\n\nAnhand dieser Zahlen liese sich gleich prognostizieren, wie der Meerespiegel in den komenden Jahren verändern wird. Wenn alles so bleibt wie bisher, können wir davon ausgehen, dass der Meeresspiegel weiteransteigen wird. Das sind keine gute Nachrichten.\nEine gute Nachricht ist allerdings, dass lineare Regressionsmodelle, im Prinzip nichts anderes als die mittlere Abweichung zu berechnen, um eine Vorhersage von \\(y\\) (in unser Beispiel die Abweichung im gemessenen Meeresspiegel) anhand von \\(x\\) (in unser Beispiel, das Jahr der Messung) vorzunehmen. Die genauen Berechnungsschritte sind ein kleines bisschen komplexer, aber die Logik ist im Prinzip die gleiche.\nWir können dies leicht mit einem linearen Regression in R überprüfen.\n\n\nWir können eine lineare Regression mit der Funktion lm() in R berechnen.\n\n# lm() nimmt mehrere Argumente. Heute brauchen wir `data` und `formula`\n# `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\nlin_reg &lt;- lm(\n  # `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\n  data = meeresspiegel, \n  # `formula = 1 +  Anstieg ~ Jahr` sagt der Funktion wie die Formel unserer\n  # Regression aussehen soll\n              formula = 1 +  Anstieg ~ Jahr)\n\nMit summary() können wir uns die Ergebnisse der Regression anzeigen lassen:\n\nsummary(lin_reg)\n\n\nCall:\nlm(formula = 1 + Anstieg ~ Jahr, data = meeresspiegel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4157  -2.7138   0.3881   2.3164   7.7297 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.728e+03  1.882e+02  -35.75   &lt;2e-16 ***\nJahr         3.375e+00  9.375e-02   36.00   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.224 on 27 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9788 \nF-statistic:  1296 on 1 and 27 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nWie kommen diese Zahlen zustande?\nWie können wir aus vergangenen Beobachtungen zukünftige Beobachtungen vorhersagen?\nWas ist eine gute Vorhersage, und warum sind Vorhersagen mehr oder weniger akkurat?\n\n\nSchauen wir uns ein weiteres Beispiel an.\n\n\n\n\n\nStellen Sie sich vor, dass Sie als ambulante Psychotherapeut:in in einer Praxis arbeiten. Am Anfang eines jeden Quartals erstellen Sie eine Kostenplanung. Die erwartete Anzahl an Behandlungsstunden spielt dabei eine große Rolle, da Sie damit den erwarteten Wert Ihres Einkommens berechnen können. Gehen wir davon aus, dass Sie pro Patient:in ein Betrag von 66,6 Euro (nach Abzug aller laufenden Kosten) in der Quartalsabrechnung erwarten können.\n\nBetrachten Sie die folgende Abbildung:\n\n\nDie Abbildung zeigt einen linearen Zusammenhang zwischen der Anzahl an Patienten und dem Betrag auf der Abrechnung. Wir können dieser Zusammenhang bildlich darstellen (wie in der Abbildung) aber auch mithilfe mit eines Modells.\n\n\\[Betrag = (wie~von~x~auf~y~kommen?) + Fehler\\]\n\nIn unseren Beispiel, welche Funktion muss auf \\(x\\) angewendet werden, um eine möglichst genaue Schätzung von \\(y\\) zu bekommen?\nWir können diese Fragen mithilfe einer mathematischen Formel ausdrucken.\n\n\\[y = f(x) + Fehler\\]\n\nDies ist die Grundlage eines Regressionsmodells. Regressionsmodelle sind nichts anderes als eine Speziefietzierung dieser Frage. Mit einem Regressionsmodell, können wir die Funktion, die auf \\(x\\) angewendet werden muss, genauer beschreiben.\nDies ist die allgemeine Formel der Regression:\n\n\\[y_{i} = b_{0} + b_{1}~x_{1} + e_{i}\\]\n\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Ausprägung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Prädiktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt\n\n\\(e_{i} =\\) Regressionsresiduum (kurz: Residuum), Residualwert oder Fehlerwert:\n\nDie Differenz zwischen einem vorhergesagten (\\(\\hat{y}\\)) und beobachteten (\\(y\\)) \\(y\\)-Wert.\nJe größer die Fehlerwerte, umso größer ist die Abweichung eines beobachteten vom vorhergesagten Wert.",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "3. Linerare Regression"
    ]
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html#beispiel-1-der-meeresspiegel-steigt-an",
    "href": "mlm_seminar_sose23/lineare_regression.html#beispiel-1-der-meeresspiegel-steigt-an",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Betrachten Sie die folgende Abbildung:\n\n\n\nAnstieg des Meeresspiegels seit dem Jahr 1993 bis 2021 in Millimeter (Stand: September). In Statista. Zugriff am 15. Mai 2023, von https://de.statista.com/statistik/daten/studie/1056576/umfrage/hoehe-des-meeresspiegels/\n\n\n\n\n\n\n\n\nWas können wir aus dieser Abbildung lernen?\n\n\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Anhand der abgebildeten Zahlen lässt sich leicht erkennen, dass seit 1993 der Meeresspiegel pro Jahr um mehrere Millimeter angestiegen ist.\n\n\n\n\nGehen wir davon aus, dass wir die Veränderung im Meeresspiegel, die durch die Abbildung angedeutet wird, möglichst prägnant zusammenfassen wollen. An dieser Stelle stehen uns unterschiedliche Methoden zur Verfügung. Wir könnten sagen:\n\n\n\n“Der Meeresspiegel ist seit 1993 gestiegen”, oder “Der Meeresspiegel steigt seit 1993 um mehrere Millimeter”.\n\n\n\nSobald wir aber eine Aussage über die Ausprägung dieser Veränderung treffen möchten, bräuchten wir eine Methode, die uns erlaubt all die kleinen Veränderungen, welche von Jahr zu Jahr gemessen wurden, zusammenzufassen. Eine Möglichkeit wäre, die gemessenen Abweichungen zu mitteln. Damit könnten wir feststellen, um wie viel der Meeresspiegel im Mittel pro Jahr angestiegen (oder abgefallen ist). Lasst uns diese Berechnung in R durchführen.\n\n\n\n\n# die Daten können mit diesem Befehl geladen werden\nmeeresspiegel &lt;- read.csv(file=\"https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main/data/meeresspiegel.csv\")\n\n\n\n\n\n# wir können `dplyr` benutzen \n# um die mittlere Abweichung zu berechnen\nrequire(dplyr)\n\nmeeresspiegel %&gt;%\n  # Differenz zwischen zwei aufeinaderefolgende Werte\n  mutate(Abweichung = lead(Anstieg) - Anstieg) %&gt;%\n  # bilde Mittelwert (schließ Fehlendewerte aus)\n  summarize(Mittlere_Abweichung = mean(Abweichung, na.rm = TRUE))\n\n  Mittlere_Abweichung\n1            3.607143\n\n\n\nNun können wir die Abweichung im Meeresspiegel genauer Beschreiben:\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Tatsächlich steigt der Meeresspiegel im Vergleich zu 1993 pro Jahr um durchschnittlich 3,6 mm.",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "3. Linerare Regression"
    ]
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html#ist-die-mittlere-abweichung-ein-guter-schätzer",
    "href": "mlm_seminar_sose23/lineare_regression.html#ist-die-mittlere-abweichung-ein-guter-schätzer",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Anhand dieser Zahlen liese sich gleich prognostizieren, wie der Meerespiegel in den komenden Jahren verändern wird. Wenn alles so bleibt wie bisher, können wir davon ausgehen, dass der Meeresspiegel weiteransteigen wird. Das sind keine gute Nachrichten.\nEine gute Nachricht ist allerdings, dass lineare Regressionsmodelle, im Prinzip nichts anderes als die mittlere Abweichung zu berechnen, um eine Vorhersage von \\(y\\) (in unser Beispiel die Abweichung im gemessenen Meeresspiegel) anhand von \\(x\\) (in unser Beispiel, das Jahr der Messung) vorzunehmen. Die genauen Berechnungsschritte sind ein kleines bisschen komplexer, aber die Logik ist im Prinzip die gleiche.\nWir können dies leicht mit einem linearen Regression in R überprüfen.\n\n\nWir können eine lineare Regression mit der Funktion lm() in R berechnen.\n\n# lm() nimmt mehrere Argumente. Heute brauchen wir `data` und `formula`\n# `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\nlin_reg &lt;- lm(\n  # `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\n  data = meeresspiegel, \n  # `formula = 1 +  Anstieg ~ Jahr` sagt der Funktion wie die Formel unserer\n  # Regression aussehen soll\n              formula = 1 +  Anstieg ~ Jahr)\n\nMit summary() können wir uns die Ergebnisse der Regression anzeigen lassen:\n\nsummary(lin_reg)\n\n\nCall:\nlm(formula = 1 + Anstieg ~ Jahr, data = meeresspiegel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4157  -2.7138   0.3881   2.3164   7.7297 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.728e+03  1.882e+02  -35.75   &lt;2e-16 ***\nJahr         3.375e+00  9.375e-02   36.00   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.224 on 27 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9788 \nF-statistic:  1296 on 1 and 27 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nWie kommen diese Zahlen zustande?\nWie können wir aus vergangenen Beobachtungen zukünftige Beobachtungen vorhersagen?\nWas ist eine gute Vorhersage, und warum sind Vorhersagen mehr oder weniger akkurat?\n\n\nSchauen wir uns ein weiteres Beispiel an.",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "3. Linerare Regression"
    ]
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html#beispiel-2-more-patients-more-cash",
    "href": "mlm_seminar_sose23/lineare_regression.html#beispiel-2-more-patients-more-cash",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Stellen Sie sich vor, dass Sie als ambulante Psychotherapeut:in in einer Praxis arbeiten. Am Anfang eines jeden Quartals erstellen Sie eine Kostenplanung. Die erwartete Anzahl an Behandlungsstunden spielt dabei eine große Rolle, da Sie damit den erwarteten Wert Ihres Einkommens berechnen können. Gehen wir davon aus, dass Sie pro Patient:in ein Betrag von 66,6 Euro (nach Abzug aller laufenden Kosten) in der Quartalsabrechnung erwarten können.\n\nBetrachten Sie die folgende Abbildung:\n\n\nDie Abbildung zeigt einen linearen Zusammenhang zwischen der Anzahl an Patienten und dem Betrag auf der Abrechnung. Wir können dieser Zusammenhang bildlich darstellen (wie in der Abbildung) aber auch mithilfe mit eines Modells.\n\n\\[Betrag = (wie~von~x~auf~y~kommen?) + Fehler\\]\n\nIn unseren Beispiel, welche Funktion muss auf \\(x\\) angewendet werden, um eine möglichst genaue Schätzung von \\(y\\) zu bekommen?\nWir können diese Fragen mithilfe einer mathematischen Formel ausdrucken.\n\n\\[y = f(x) + Fehler\\]\n\nDies ist die Grundlage eines Regressionsmodells. Regressionsmodelle sind nichts anderes als eine Speziefietzierung dieser Frage. Mit einem Regressionsmodell, können wir die Funktion, die auf \\(x\\) angewendet werden muss, genauer beschreiben.\nDies ist die allgemeine Formel der Regression:\n\n\\[y_{i} = b_{0} + b_{1}~x_{1} + e_{i}\\]\n\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Ausprägung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Prädiktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt\n\n\\(e_{i} =\\) Regressionsresiduum (kurz: Residuum), Residualwert oder Fehlerwert:\n\nDie Differenz zwischen einem vorhergesagten (\\(\\hat{y}\\)) und beobachteten (\\(y\\)) \\(y\\)-Wert.\nJe größer die Fehlerwerte, umso größer ist die Abweichung eines beobachteten vom vorhergesagten Wert.",
    "crumbs": [
      "Seminar Multi-Level Modelle",
      "3. Linerare Regression"
    ]
  },
  {
    "objectID": "scripts/ML Workshop/ML_Workshop_stud.html",
    "href": "scripts/ML Workshop/ML_Workshop_stud.html",
    "title": "Hands on Maximum Likelihood Parameter Estimation",
    "section": "",
    "text": "Im letzten Seminar haben wir sehr ausführlich über Maximum Likelihood Estimation (MLE) gesprochen. Heute werden wir einige Übungen dazu in R programmieren, um ein besseres Verständnis für diese Methode zu entwickeln.\nDas Grundprinzip der Maximum-Likelihood-Schätzung besteht darin, die Parameter einer statistischen Verteilung so zu bestimmen, dass die Wahrscheinlichkeit, die beobachteten Daten gegeben bestimmter Parameterwerte, maximiert wird. Gegeben eine Verteilungsfunktion \\(f(x;\\theta)\\), wobei \\(x\\) die beobachteten Daten und \\(\\theta\\) die unbekannten Parameter sind, wird die Likelihood-Funktion definiert als \\[L(\\theta|x)=\\prod_{i=1}^{n} f(x_i;\\theta)\\], wobei \\(n\\) die Anzahl der Datenpunkte ist.\nDas Maximum-Likelihood-Schätzverfahren besteht darin, die Werte von \\(\\theta\\) zu finden, die die Likelihood-Funktion maximieren. Dies kann durch Maximierung des Logarithmus der Likelihood-Funktion mathematisch vereinfacht werden, daher wird oftmals die Log-Likelihood Funktion maximiert und anstelle des Produktes, die Summe über alle Funktionswerte gebildet:\n\\[\\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log f(x_i;\\theta)\\]\n\n\nNehmen wir an, wir haben an einer Sttichprobe die Intelligenzẃerte erhoben und möchten nun den Mittelwert des IQs anhand der Daten mit MLE schätzen. Hierzu brauchen wir zunächst eine Dichtefunktion, über die wir die Likelihood berechnen können. Da der IQ in der Population normalverteilt ist können wir hierfür die Normaverteilung heranziehen um eine Likelihoodfunktion zu definieren:\n\\[f(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\nalso ist die Likelihoodfunktion gegeben als\n\\[L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\n\\] bzw. vereinfacht sich zu folgender Formel, wenn wir den Logarhitmus nehmen:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]"
  },
  {
    "objectID": "scripts/ML Workshop/ML_Workshop_stud.html#einführung",
    "href": "scripts/ML Workshop/ML_Workshop_stud.html#einführung",
    "title": "Hands on Maximum Likelihood Parameter Estimation",
    "section": "",
    "text": "Im letzten Seminar haben wir sehr ausführlich über Maximum Likelihood Estimation (MLE) gesprochen. Heute werden wir einige Übungen dazu in R programmieren, um ein besseres Verständnis für diese Methode zu entwickeln.\nDas Grundprinzip der Maximum-Likelihood-Schätzung besteht darin, die Parameter einer statistischen Verteilung so zu bestimmen, dass die Wahrscheinlichkeit, die beobachteten Daten gegeben bestimmter Parameterwerte, maximiert wird. Gegeben eine Verteilungsfunktion \\(f(x;\\theta)\\), wobei \\(x\\) die beobachteten Daten und \\(\\theta\\) die unbekannten Parameter sind, wird die Likelihood-Funktion definiert als \\[L(\\theta|x)=\\prod_{i=1}^{n} f(x_i;\\theta)\\], wobei \\(n\\) die Anzahl der Datenpunkte ist.\nDas Maximum-Likelihood-Schätzverfahren besteht darin, die Werte von \\(\\theta\\) zu finden, die die Likelihood-Funktion maximieren. Dies kann durch Maximierung des Logarithmus der Likelihood-Funktion mathematisch vereinfacht werden, daher wird oftmals die Log-Likelihood Funktion maximiert und anstelle des Produktes, die Summe über alle Funktionswerte gebildet:\n\\[\\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log f(x_i;\\theta)\\]\n\n\nNehmen wir an, wir haben an einer Sttichprobe die Intelligenzẃerte erhoben und möchten nun den Mittelwert des IQs anhand der Daten mit MLE schätzen. Hierzu brauchen wir zunächst eine Dichtefunktion, über die wir die Likelihood berechnen können. Da der IQ in der Population normalverteilt ist können wir hierfür die Normaverteilung heranziehen um eine Likelihoodfunktion zu definieren:\n\\[f(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\nalso ist die Likelihoodfunktion gegeben als\n\\[L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\n\\] bzw. vereinfacht sich zu folgender Formel, wenn wir den Logarhitmus nehmen:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]"
  },
  {
    "objectID": "scripts/ML Workshop/ML_Workshop_stud.html#mle-in-r",
    "href": "scripts/ML Workshop/ML_Workshop_stud.html#mle-in-r",
    "title": "Hands on Maximum Likelihood Parameter Estimation",
    "section": "2 MLE in R",
    "text": "2 MLE in R\n\n2.1 Übung 1: MLE by Hand in R\nUm in R mit der Likelihoodfunktion zu arbeiten, müssen wir zunächst Daten simulieren. Hierzu benutzen wir die Funktion rnorm(). Bitte nutzt zunächste die Hilfefunktion, um mit rnorm() eine Stichproben von 10000 Werten zu genieren (N = 100), mit dem Mittelwert \\(100\\) und einer Standardabweichung von \\(15\\). Speichert den Output in der Variable “iq” ab. Berechnet für die gezogene Stichprobe separat Mittelwert und Standardabweichung\n\n\nCode\nset.seed(666)\n# Stichprobe von Werten generieren\n# YOUR CODE HERE....\n\n# Mittelwert und Standardabweichung berechnen\n\n# YOUR CODE HERE....\n# YOUR CODE HERE....\n\n\nZunächst wollen wir versuchen, die Likelihoodfunktion in R Code zu übertragen. Nehmt hierzu die Gleichung der Log-Likelihoodfunktion, die wir weiter oben definiert haben:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]\nund drückt sie in R-Code aus. Ihr benötigt dazu folgende mathematischen Funktionen:\n\n\n\nFunktion\nCode\n\n\n\n\nSumme Bilden\nsum()\n\n\nQuadrieren\nx^2\n\n\nLogarhitmus\nlog()\n\n\n\\(\\pi\\)\nPi\n\n\nn\nStichprobengröße (hier: 100)\n\n\n\nDefiniert zunächst zwei Variablen für unterschiedliche Parametervorschläge für den Mittelwert von \\(100\\) (=low_iq) und \\(120\\) (high_iq),die Standardabweichung (=sigma) ist für beide Samples gleich (\\(\\sigma\\) = 15). Als Daten nutzen wir die generierten IQ Werte iq_low und iq_high. In der Gleichung sind \\(x\\) die Daten, also die IQ-Werte aus unserem iq Vektor, \\(\\sigma\\) ist die Standardabweichung und \\(\\mu\\) die unterschiedlichen Vorschläge für die Mittelwerte, als entweder high_iq oder low_iq. Stellt nun die Gleichung für beide Parametervorschläge (high vs. low) in R auf. Speichert die Ergebnisse unter den Variablen ll_low und ll_high ab.\n\n\nCode\n# Define sigma and mu\n# high_iq &lt;- # YOUR CODE HERE....\n\n# low_iq &lt;- # YOUR CODE HERE....\n\n# N &lt;- # YOUR CODE HERE....\n# sigma &lt;- # YOUR CODE HERE....\n\n# Define LL-Equation\n\n# ll_low &lt;- # YOUR CODE HERE....\n\n# ll_high &lt;- # YOUR CODE HERE....\n\n\nIhr habt nun für beide Parametervorschläge, also einmal für einen Mittelwert von 100 und einmal von einem Mittelwert von 120, die log-likelihood für die vorliegenden Daten berechnet. Welcher Mittelwert ist nach dieser Likelihood unter den gegebenen Daten wahrscheinlicher ?\n\n\n2.2 Exkurs: Funktionen in R\nFunktionen sind in der Programmierung wichtige Bausteine, um Code wiederverwendbar zu machen und komplexe Aufgaben zu strukturieren. Eine nimmt in der Regel Eingabewerte (Argumente) entgegen, führt Operationen oder Berechnungen durch und gibt ein Ergebnis zurück. In R können Funktionen mit dem Schlüsselwort function definiert werden. Die Syntax besteht aus dem Funktionsnamen, den Eingabe-Parametern in Klammern, den auszuführenden Anweisungen innerhalb des Funktionskörpers und dem Rückgabewert mit return(). Funktionen in R können dann mit den angegebenen Argumenten aufgerufen werden, um den gewünschten Code auszuführen. Die Verwendung von Funktionen erleichtert das Schreiben, Lesen und Verstehen von Code, da Aufgaben in kleine, wiederverwendbare Einheiten aufgeteilt werden können.\nHier ein einfaches Beispiel einer Funktion, die die Quadratzahl des Inputarguments ausgibt:\n\n\nCode\n# Funktion zur Berechnung der Quadratzahl\nsquare &lt;- function(x) {\n  result &lt;- x^2\n  return(result)\n}\n\n# Verwendung der Funktion\nnum &lt;- 5\nsquared_num &lt;- square(num)\nprint(squared_num)\n\n\n[1] 25\n\n\nInnerhalb einer Funktion in R kann auf die Input-Argumente zugegriffen werden, indem man ihre Namen verwendet. Die Input-Argumente werden in der Funktion als Parameter definiert. Du kannst diese Parameter dann innerhalb der Funktion verwenden, um auf die übergebenen Werte zuzugreifen und damit Berechnungen oder Operationen durchzuführen.\nZum Beispiel, wenn wir eine Funktion addition() definieren möchten, die zwei Zahlen addiert, könnten wir die Input-Argumente a und b verwenden:\n\n\nCode\naddition &lt;- function(a, b) {\n  sum &lt;- a + b\n  return(sum)\n}\n\naddition(2,2)\n\n\n[1] 4\n\n\n\n\n2.3 Übung 2: MLE Using optim()\n\nDiskrepanzfunktion definieren\nNun haben wir im Prinzip “by Hand” eine MLE Schätzung durchgeführt - zwar nicht iterativ, denn haben wir haben nur zwei mögliche Parameterwerte im Lichte der gegebenen Daten nach der MLE bewertet! R bietet aber auch die Möglichkeit, mit der Funktion optim(), eine SIMPLEX Optimierung nach einer gegebenen Diskrepanzfunktion durchzuführen. Dazu müssen wir der Funktion allerdings eine Funktion übergeben.\nUm nun die optim() zu nutzen im iterativ Parameter nach MLE zu schätzen und durch simplex zu minimieren, müssen wir die gleiche log-likelihood Funktion von Übung 1 in eine Funktion überführen. Das ist ganz einfach, denn wir können uns nun, da wir das Grundprinzip von MLE verstanden haben, das Leben mit der in R verfügbaren dnorm() Funktion erleichtern. Diese berechnet ebenfalls die Wahrscheinlichkeit (genauer die Dichte) für Datenpunkte, gegeben bestimmter Parameter:\n\n\nCode\n# Unsere Gleichung\n# ll_low &lt;- -100/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((iq - low_iq)^2) \n# print(ll_low)\n# # dnorm() aus R\n# ll_dnorm &lt;- sum(dnorm(x = iq,mean=low_iq,sd=sigma,log = T))\n# print(ll_dnorm)\n\n\nEure Aufgabe ist es nun, eine Funktion zu definieren, die sie aufsummierte log-likelihood ausgibt, wenn Ihr Parameterwerte eingebt. Dazu nutzt ihr die folgende Funktionen aus R:\n\n\n\nFunktion\nCode\n\n\n\n\nSumme Bilden\nsum()\n\n\nLikelihood\ndnorm(daten,mean=,sd=, log=TRUE)\n\n\nLogarhitmus\nlog()\n\n\n\nDie Funktion soll folgende Input-Argumente besitzen:\n\nEinen Vektor Daten (unsere IQ Daten sind ein Vektor)\nEinen Vektor theta, der nur zwei Einträge enthält - theta ist der Argumentname, um innerhalb der Funktion auf den Input zuzugreifen. Was ihr letztendlich der Funktion als theta übergebt, muss nicht theta heißen !\n\nWeiterhin soll diese Funktion die aufsummierte log-Likelihood ausgeben. Definiert diese Funktion unter dem Namen MLE. Die Inputargumente müssen wie im oberen Beispiel einer einfachen Funktion nur mit Namen definiert werden, nicht mit Datentyp. Diesen habe ich nur zum Verständnis mit angegeben.\n2.) Müssen folgende Operationen innerhalb der Funktion ausgeführt werden\n\nBerechnung der Likelihood unter der verwendung von dnorm(). b.) Aufsummierung der berechneten Likelihood mit sum()\nBerechnung der Deviance - hierzu muss die aufsummierte Likelihood mal -2\ngenommen werden.\n\nWir können hierbei auf die Berechnung des Logarhitmus verzichten, da dnorm() schon den die log-Likelihood mit ausgibt. Hierzu muss allerdings das Argument log = TRUE gesetzt werden ! Zur Erinnerung, auf bestimmte Elemente eines Vektors greift ihr folgendermaßen zu (dies ist wichtig zu wissen, da Ihr mit den Inputwerten innerhalb der Funktion arbeiten müsst):\n\n\nCode\n# Vektor Definieren\nvektor &lt;- c(1,2,3)\n\n# Erstes Element\n\nvektor[1]\n\n\n[1] 1\n\n\nCode\n# Zweites Element\n\nvektor[2]\n\n\n[1] 2\n\n\nCode\n# Drittes Element\n\nvektor[3]\n\n\n[1] 3\n\n\n3.) Es muss mit return() das Endprodukt zurückgeben, wie in den beiden einfachen Beispielen vorher !\n\n\nCode\n### YOUR CODE HERE \n\n# MLE &lt;- function(Daten, theta) \n# {\n#   mu &lt;- theta[1]\n#   sigma &lt;- theta[2]\n#   \n#   LL &lt;- -2*(sum(dnorm(Daten,mean=mu,sd=sigma,log = T)))\n#   \n#   return(LL)\n#   \n# }\n# \n# # Test your function with different values for theta\n# theta &lt;- c(80,20)\n# MLE(iq,theta)\n\n\n\n\nOptimieren der Parameter mit optim()\nNun da unsere ML-Diskrepanzfunktion funktioniert, müssen wir diese natürlich minimieren - dazu können wir die Funktion optim() nutzen, die in R zur Verfügung steht. Mit optim() ist standardmäßig der SIMPLEX -Algorithmus eingestellt. Es können aber auch andere Algorhithmen genutzt werden. Es werden der Funktion drei Hauptargumente übergeben:\n\npar - ein Vektor mit den Startwerten der Parameterschätzung. Die Startwerte sollten nicht übermäßig von den erwarteten Werten abweichen. Schätzt man also einen Mittelwert von IQ Daten, macht es keinen Sinn, den Startwert für den Mittelwert auf 1 zu setzen, da der “wahre Wert” vermutlich zwischen 75 und 150 liegen wird. Gleiches gilt für die Standardabweichung.\nfn= - die Diskrepanzfunktion die es zu minimieren gilt. Hier die von uns definierte MLE() Funktion.\nDie Daten - diese übergebt ihr mit dem Namen, den Ihr in eurer Funktion verwendet habt, also hier Daten = iq.\n\nOptimiert nun die definierte Funktion mit optim() und speichert die Ergebnisse im Objekt fit.\n\n\nCode\n# YOUR CODE HERE....\n# fit &lt;- optim() \n\n# fit$par\n# \n# # Biased Fit Example \n# \n# iq_pop &lt;- rnorm(1000,mean=100,sd=15)\n# iq_bias &lt;- sample(iq_pop,25)\n# \n# mean(iq_bias)\n\n# Fitting Sample\n# fit &lt;- optim() # YOUR CODE HERE...."
  },
  {
    "objectID": "scripts/ML Workshop/ML_Workshop_stud.html#fazit",
    "href": "scripts/ML Workshop/ML_Workshop_stud.html#fazit",
    "title": "Hands on Maximum Likelihood Parameter Estimation",
    "section": "3 Fazit",
    "text": "3 Fazit\nIn diesem Tutorial haben wir uns mit der Maximum-Likelihood-Schätzung (MLE) in R beschäftigt und die Funktion `optim()` verwendet, um die Schätzung durchzuführen. Zunächst haben wir die Likelihood-Funktion selbst definiert und verschiedene Parameterwerte getestet, um das Konzept der MLE besser zu verstehen.\nDurch die Verwendung von `optim()` konnten wir die MLE iterativ implementieren und die optimalen Parameterwerte finden, die die Likelihood-Funktion maximieren (oder die Deviance minimieren). Wir haben gesehen, dass `optim()` eine effiziente Methode zur numerischen Optimierung ist und verschiedene Algorithmen zur Verfügung stellt.\nDie Maximum-Likelihood-Schätzung ist ein leistungsstarkes Werkzeug, um Parameter in statistischen Modellen zu schätzen. Es ermöglicht uns, die Wahrscheinlichkeit der beobachteten Daten unter verschiedenen Annahmen zu maximieren und die besten Parameterwerte zu ermitteln."
  },
  {
    "objectID": "scripts/advanced-r/Advanced_R_stud.html",
    "href": "scripts/advanced-r/Advanced_R_stud.html",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "scripts/advanced-r/Advanced_R_stud.html#fortgeschrittene-dplyr-funktionen",
    "href": "scripts/advanced-r/Advanced_R_stud.html#fortgeschrittene-dplyr-funktionen",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "scripts/advanced-r/Advanced_R_stud.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "href": "scripts/advanced-r/Advanced_R_stud.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "title": "Advanced Dplyr",
    "section": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen",
    "text": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen\nDie case_when Funktion in dplyr ermöglicht es, basierend auf bestimmten Bedingungen verschiedene Werte für eine Spalte in einem Datensatz auszuwählen und abhängig vom Variablenwert einen neue Variable zu erstellen. Es ist eine sehr einfache Möglichkeit, if-else Statements in eine dplyr-Pipeline zu integrieren.\nDie Syntax eines case_when() Befehles entspricht einer einfachen if oder if-else Verknüpfung. Eine if-else Verknüpfung prüft eine Bedingung und führt wenn diese Erfüllt ist einen definierten Befehl aus. Ist die Bedingung nicht erfüllt, wird andernfalls (else) ein anderer Befehl ausgeführt:\n\n\n# Vektor von Zahlen darauf testen, ob sie gerade oder ungerade sind:\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei geteilt keinen Rest hat, dann schreibe Gerade, ansonsten Ungerade.\n\nifelse(zahlen %% 2 == 0,\"Gerade\",\"Ungerade\")\n\ncase_when() funktioniert nach der Gleichen Logik:\n\n\n\nDabei wird immer zuerst die if-Bedinungen und deren Output definiert und anschließend mit TRUE die else Bedingung, welche angibt was passiert wenn die if - Bedingung nicht zutrifft:\n\n\n\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei\n# geteilt keinen Rest hat, dann schreibe Gerade, \n# ansonsten Ungerade.\n\ncase_when(zahlen %% 2 == 0 ~ \"Gerade\",\n          TRUE ~\"Ungerade\")\n\nDer Unterschied von case_when() und ifelse() ist, dass mit case_when() auch mehrere Bedingungen definiert werden können (z.B. wenn eine Variable in mehrere Kategorien eingeteilt werden soll):\n\nHier gibt es auch ein finales else Statement, dieses muss aber nicht sein, solange die Kategorien alle Fälle abdecken!\n\n2.0.1 case_when: Beispiele\nDas folgende ist ein einfaches Beispiel, bei dem case_when() verwendet wird, um in einem Datensatz eine neue Spalte Alter_Kategorie anhand des Werts in der Spalte Alter zu kategorisieren. Wenn eine neue Spalte erstellt werden soll, muss case_when() immer mit mutate() kombiniert werden:\n\n# Beispiel Datensatz erstellen\ndf &lt;- data.frame(Name = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45))\nhead(df)\n\n# mutate und case_when verwenden, um neue Variable in Abhängigkeit vom Alter zu erstellen:\ndf %&gt;% \n  mutate(Alter_Kategorie = case_when(\n    Alter &lt;= 25 ~ \"jung\",\n    Alter &gt; 25 & Alter &lt;= 35 ~ \"mittel\",\n    Alter &gt; 35 ~ \"alt\"\n  ))\n\nEin weiteres Beispiel haben wir letzte Woche bei der MPT-Modellierung der Daten von Frenken et al. gesehen. Im Datensatz ist shoot als 0 und not shoot als 1 kodiert. Für die MPT Modellierung benötigen wir aber für jede Kategorie (Black / Gun, Black / Phone, White / Gun, White / Phone) die Hits und Misses, also die Fehlerraten. Der Datensatz hat praktischerweise eine Spalte, die genau kodiert, was die VP gesehen hat:\n\n\nhead(Study_2_dm)\n\n# A tibble: 6 × 6\n  subj_idx stimulus stim     rt response condition\n     &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt; &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;    \n1        0 gun      bg    0.506        0 black    \n2        0 phone    wp    0.437        0 white    \n3        0 phone    wp    0.483        0 white    \n4        0 phone    bp    0.594        1 black    \n5        0 phone    wp    0.552        1 white    \n6        0 gun      wg    0.417        1 white    \n\n\nIn der zweiten Spalte stimulus ist angebenen, welches Objekt der jeweils zusammen mit der Hautfarbe gezeigt wurde. Da wir wissen, dass nur bei “gun” geschossen werden darf, können wir nun in Kombination mit response (0 - shoot, 1 - not shoot) die Hits mit case_when() in einer neuen Spalte ACC kodieren.\nDazu Verknüpfen wir hier zwei Bedingungen mit dem & Operator. Die erste Bedingung bezieht sich auf die Spalte stimulus, hier muss geprüft werden, welcher Stimulus gezeigt wurde (Gun vs. Phone). Die zweite Bedingung bezieht sich auf die Spalte response. Hier muss geprüft werden, ob geschossen wurde oder nicht. Ingesamt müssen also vier Bedinungen definiert werden, für jede Kombination von Objekt und Response:\n\nAccuracy in Abhängigkeit von Gegebener Response und Stimulus\n\n\nStimulus\nResponse\nAccuracy\n\n\n\n\nGun\n0\n1\n\n\nGun\n1\n0\n\n\nPhone\n0\n0\n\n\nPhone\n1\n1\n\n\n\nDies müssen wir nun in ein case_when() Befehl übernehmen\n\nMit mutate() die neue Outputspalte benennen\nInnerhalb von mutate() mit case_when() die Accuracy in Abhängigkeit der Spalten Stimulus und Response umkodieren:\n\n\n\nfreq_dat &lt;- Study_2_dm %&gt;% mutate(ACC = case_when(stimulus == \"gun\" & response == 0 ~ 1,\n                                                  stimulus == \"gun\" & response == 1 ~ 0,\n                                                  stimulus == \"phone\" & response == 1 ~ 1,\n                                                  stimulus ==\"phone\" & response == 0 ~ 0))\n\nAnschließend müssen wir nun die Hits und Misses auszählen. Generell eignen sich zum Auszählen von bestimmten Bedingungskombinationen oder Trials die Funktionen group_by(), summarise() und n(). n() ist eine einfache Zählfunktion, welche innerhalb von group_by() %&gt;% summarise() dazu führt, dass die alle Beobachtungen der gruppierten Variablen (z.B. Subject & Bedingung) innerhalb von summarise() gezählt werden.\nAlso zum Beispiel, wieviele Beobachtung von Subject 1 gibt es in Bedingung A, B und C. Dies macht aber nur Sinn, wenn ihr Daten auf Trial-Ebene (also für jede Versuchsperson alle Antworten über das ganez Experiment) vorliegen habt. Dies ist hier der Fall, da für jede Person und jede Bedingung, die diese Person durchlaufen hat, die gegebenen Antworten im Datensatz in der Spalte responses vorliegen.\n\n\nfreq_dat &lt;- freq_dat %&gt;% group_by(subj_idx,stim) %&gt;% \n  summarise(hits = sum(ACC), \n            ntrials=n(), \n            miss=ntrials-hits)\n\n`summarise()` has grouped output by 'subj_idx'. You can override using the\n`.groups` argument.\n\nhead(freq_dat)\n\n# A tibble: 6 × 5\n# Groups:   subj_idx [2]\n  subj_idx stim   hits ntrials  miss\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n1        0 bg       30      31     1\n2        0 bp       28      29     1\n3        0 wg       23      27     4\n4        0 wp       28      33     5\n5        1 bg       28      30     2\n6        1 bp       16      18     2\n\n\nWas passiert hier genau ? Schritt für Schritt:\n\ngroup_by(subj_idx, stim) : Wir gruppieren zunächst nach Subject und Bedingung\n\nsummarise(...\n\nhits = sum(ACC) - Wir haben alle Hits mit case_when als 1 kodiert, also können wir die Hits einfach berechnen, indem wir diese aufsummieren.\nnTrials = n() - Auszählen wieviele Responses ein Subject in jeder stimulus Bedinungen gegeben hat (Gesamtanzahl von gegebenen Antworten in einer Bedingungen pro Person\nmiss = ntrials - hits Alles was kein Hit ist muss ein Miss sein ! Daher können wir einfach die Hits von der Gesamtzahl der Antworten abziehen und erhalten die Anzahl der Misses in jeder Bedinung pro Subject !"
  },
  {
    "objectID": "scripts/advanced-r/Advanced_R_stud.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "href": "scripts/advanced-r/Advanced_R_stud.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "title": "Advanced Dplyr",
    "section": "3 Long- und Wide-Datenformat - Datentransformation in dplyr",
    "text": "3 Long- und Wide-Datenformat - Datentransformation in dplyr\nNun haben wir die Daten von Frenken et al. neu kodiert und die nötigen Informationen im Datensatz, um mit MPTinR zu arbeiten. Allerdings liegen die Daten noch im sogenannten long - Format vor. Oftmals ist es jedoch notwendig, Daten entweder von einem long in ein wide Format oder umgekehrt zu transfomieren. Im Kontext von Datenanalyse und statistischer Modellierung gibt es in der Regel zwei Haupttypen von Datenformaten: wide-Format (breit-Format) und long-Format (lang-Format).\n\n3.1 Wide-Format\n\nIn einem wide Format sind die verschiedenen Merkmale einer einzigen Beobachtung in separaten Spalten dargestellt.\nEine einzige Zeile in einem breiten Datensatz repräsentiert eine Beobachtung.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, wäre in einem breiten Format dargestellt, wobei jede Spalte einem bestimmten Fach entspricht.\nVorteil: Es ist einfach, schnelle Übersichten über große Datenmengen zu erhalten.\n\nBeispiel:\n\nwide_df &lt;- data.frame(Schüler_ID = c(1, 2, 3), \n                      Mathe = c(89, 76, 92), \n                      Englisch = c(92, 88, 95), \n                      Wissenschaft = c(88, 72, 98))\nwide_df\n##   Schüler_ID Mathe Englisch Wissenschaft\n## 1          1    89       92           88\n## 2          2    76       88           72\n## 3          3    92       95           98\n\n\n\n3.2 Long-Format\n\nIm long Format sind alle Merkmale einer einzigen Beobachtung in einer Zeile dargestellt.\nEine einzige Spalte in einem long Datensatz repräsentiert ein bestimmtes Merkmal.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, könnte in einem langen Format dargestellt werden, wobei jede Zeile einer bestimmten Schüler-Fach-Kombination entspricht.\nVorteil: Es ist einfach, bestimmte Merkmale für verschiedene Beobachtungen zu vergleichen oder zu analysieren. Außerdem ist es für manche statistische Methoden, wie lineare Regression oder einer ANOVA, das bevorzugte Format.\n\nBeispiel:\n\nlong_df &lt;- data.frame(Schüler_ID = c(rep(1, 3), rep(2, 3), rep(3, 3)), \n                      Fach = c(rep(\"Mathe\", 3), rep(\"Englisch\", 3), rep(\"Wissenschaft\", 3)), \n                      Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\nlong_df\n##   Schüler_ID         Fach Note\n## 1          1        Mathe   89\n## 2          1        Mathe   92\n## 3          1        Mathe   88\n## 4          2     Englisch   76\n## 5          2     Englisch   88\n## 6          2     Englisch   72\n## 7          3 Wissenschaft   92\n## 8          3 Wissenschaft   95\n## 9          3 Wissenschaft   98\n\nEs ist wichtig, das Konzept des wide- und long-Formats zu verstehen, da es bei der Datenaufbereitung und Analyse eine wichtige Rolle spielt. Zum Beispiel kann ein wide -Datensatz schwer zu analysieren sein, wenn man Vergleiche zwischen bestimmten Merkmalen über mehrere Beobachtungen hinweg machen möchte (Beispiel Ergebnisse der Diffusionsmodellierung!). Hier ist es oft besser, den Datensatz in ein long-format zu bringen.\n\n\n3.3 pivot - Funktionen in dplyr\nDie Funktionen pivot_wider und pivot_longer gehören zu den Funktionen von dplyr und dienen dazu, Datensätze zu transformieren.\npivot_wider verwandelt einen long-Format Datensatz in einen wide-Format Datensatz, indem es die Werte einer bestimmten Spalte zu neuen Spalten umbenennt. Dies kann manuell durchgeführt werden, aber pivot_wider macht dies automatisch und erleichtert so die Datentransformation:\n\n\npivot_longer verwandelt einen wide-Format Datensatz in einen long-Format Datensatz, indem es die Spalten mit bestimmten Werten in einer neuen Spalte zusammenfasst.\nEin Beispiel für den Einsatz von pivot_wider:\n\n# long data example\nlong_data &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\", \"John\", \"Jane\", \"Jim\"),\n  subject = c(\"physics\", \"physics\", \"physics\", \"math\", \"math\", \"math\"),\n  score = c(85, 90, 80, 75, 80, 70)\n)\n\nlong_data\n## # A tibble: 6 × 3\n##   name  subject score\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 John  physics    85\n## 2 Jane  physics    90\n## 3 Jim   physics    80\n## 4 John  math       75\n## 5 Jane  math       80\n## 6 Jim   math       70\n\n\n# pivot to wide format\npivot_wider(long_data, names_from = subject, values_from = score)\n## # A tibble: 3 × 3\n##   name  physics  math\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n## 1 John       85    75\n## 2 Jane       90    80\n## 3 Jim        80    70\n\nEin Beispiel für den Einsatz von pivot_longer:\n\n# wide data example\nwide_df &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\"),\n  Mathe_Note = c(85, 90, 80),\n  Englisch_Note = c(75, 80, 70)\n)\n\n\nwide_df\n## # A tibble: 3 × 3\n##   name  Mathe_Note Englisch_Note\n##   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n## 1 John          85            75\n## 2 Jane          90            80\n## 3 Jim           80            70\n\n\n# pivot to long format\npivot_longer(wide_df, cols = c(Mathe_Note, Englisch_Note), \n             names_to = \"Schüler\", values_to = \"Note\")\n## # A tibble: 6 × 3\n##   name  Schüler        Note\n##   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n## 1 John  Mathe_Note       85\n## 2 John  Englisch_Note    75\n## 3 Jane  Mathe_Note       90\n## 4 Jane  Englisch_Note    80\n## 5 Jim   Mathe_Note       80\n## 6 Jim   Englisch_Note    70\n\n\n\n3.4 Argumente für pivot_longer und pivot_wider\npivot_longer benötigt mindestens zwei Argumente:\n\ncols: Dies ist ein Zeichenvektor, der angibt, welche Spalten im Datensatz zusammengefasst werden sollen. -Schüler_ID bedeutet zum Beispiel, dass alle Spalten außer Schüler_ID zusammengefasst werden sollen.\nnames_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die zusammengefassten Werte gespeichert werden.\nvalues_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die Werte gespeichert werden, die aus den zusammengefassten Spalten stammen.\n\n\npivot_wider benötigt mindestens zwei Argumente:\n\nnames_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Namen für die neuen Spalten verwendet werden soll.\nvalues_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Werte für die neuen Spalten verwendet werden soll.\n\n\nBeide Funktionen haben auch weitere optionale Argumente wie values_fill und names_prefix oder names_sep()um die Daten bei Bedarf weiter anzupassen.\nHier nun die Umformung der Daten von Frenken et al., welche wir vom long Format in das wide Format bringen müssen:\n\n\n#| echo: true\n#| output: true\n#| warning: false\n#| code-overflow: wrap\n#| collapse: true\n\nfreq_dat %&gt;% pivot_wider(names_from = c(\"stim\"),\n                         values_from = c(\"hits\",\"miss\"),\n                         id_cols=\"subj_idx\")\n\n# A tibble: 137 × 9\n# Groups:   subj_idx [137]\n   subj_idx hits_bg hits_bp hits_wg hits_wp miss_bg miss_bp miss_wg miss_wp\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1        0      30      28      23      28       1       1       4       5\n 2        1      28      16      27      11       2       2       2       1\n 3        3      25      10      33      23       0       0       1       1\n 4        4      35      24      27      33       1       0       0       0\n 5        5      31      28      25      23       1       2       4       4\n 6        6      34      26      18      31       0       0       5       0\n 7        7      29      24      21      36       2       0       1       1\n 8        8      22      29      25      28       0       2      10       2\n 9        9      28      23      22      22       2       0       2       1\n10       10      26      35      25      24       4       0       0       3\n# ℹ 127 more rows\n\n\nWas passiert hier genau ? Schritt für Schritt:\n\nnames_from = c(stim) - Die neuen Spalten sollen aus der Spalte stim benannt werden - (bg,bp,wp,wg)\nvalues_from = c(\"hits\",\"miss\") - in den neuen Spalten sollen die Werte der Hits und Miss Spalten stehen - durch den ersten und zweiten Schritt entstehen also Spalten, die jeweils hits_bg etc. enthalten. Also die Hits aus der Bedingung “bg” usw.\nid_cols - dies soll für jedes Subject einzeln geschehen."
  },
  {
    "objectID": "scripts/advanced-r/Advanced_R_stud.html#übungen",
    "href": "scripts/advanced-r/Advanced_R_stud.html#übungen",
    "title": "Advanced Dplyr",
    "section": "4 Übungen",
    "text": "4 Übungen\n\n4.1 case_when\n1.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Note_Kategorie” zu erstellen, die “Sehr gut” für Noten über 90, “Gut” für Noten zwischen 80 und 90 und “Schlecht” für Noten unter 80 angibt.\n\ndf &lt;- data.frame(Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\n# Your Code Here\n\n2.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Bestanden” zu erstellen, um zu Prüfen ob ein Schüler einer bestimmten Schulform eine Prüfung bestanden hat. Die Bestehensgrenzen sind wie folgt.\n-   Für die Hauptschule liegt die Bestehensgrenze bei 50 %\n-   Für die Realschule liegt die Bestehensgrenze bei 60 %\n-   Für das Gymnasium liegt die Bestehensgrenze bei 70 %\nKodieren Sie das bestehen entweder mit “Pass” oder “Fail”.\n\ndf &lt;- data.frame(Note = runif(100,min=0, max=100),\n                 Schulform = sample(c(\"Gymnasium\",\"Realschule\", \"Hauptschule\"),\n                                    size = 100, replace = T))\n# Your Code here\n\n3.) Nutzen Sie die Funktion case_when und die dplyr-Library in R, um eine neue Spalte in dem Datensatz “df” zu erstellen, die die Einkommenskategorie jeder Person basierend auf ihrem Berufsstatus und ihrem Einkommen kategorisiert. Die Einkommenskategorien sollten wie folgt sein:\n\nFür Angestellte mit einem Einkommen von bis zu 50.000: “niedrig”\nFür Angestellte mit einem Einkommen zwischen 50.000 und 75.000: “mittel”\nFür Angestellte mit einem Einkommen über 75.000: “hoch”\nFür Freiberufler mit einem Einkommen von bis zu 60.000: “niedrig”\nFür Freiberufler mit einem Einkommen zwischen 60.000 und 100.000: “mittel”\nFür Freiberufler mit einem Einkommen über 100.000: “hoch”\nFür Ruheständler mit einem Einkommen von bis zu 30.000: “niedrig”\nFür Ruheständler mit einem Einkommen über 30.000: “mittel_hoch”\n\n\ndf &lt;- data.frame(ID = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45),\n                 Berufsstatus = c(\"Angestellter\", \"Freiberufler\", \"Ruheständler\"),\n                 Einkommen = c(45000, 75000, 32000))\n\n\n\n4.2 Pivotting\n1.) Konvertieren Sie die folgenden Datensatz von wide to long. Erstellen Sie aus den Spalten zwei neue Spalten mit den Namen (month und index). Nutzen Sie dazu das Argument names_sep = \"_\". Die Werte sollen in die Spalte “N” geschrieben werden.\nTip: Sie müssen bei names_to einen Vektor mit den Namen der neuen Spalten angeben.\n\ndf_wide &lt;- data.frame(\n  Jan_sales = c(10, 40, 70),\n  Feb_sales = c(20, 50, 80),\n  Mar_sales = c(30, 60, 90)\n)\n\n# Your Code here\n\n2.) Konvertieren Sie die folgendn Datensatz von long in wide.\n\n    df_long &lt;- data.frame(\n      ID = c(\"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\"),\n      month = c(\"Jan\", \"Jan\", \"Jan\", \"Feb\", \"Feb\", \"Feb\", \"Mar\", \"Mar\", \"Mar\"),\n      sales = c(10, 40, 70, 20, 50, 80, 30, 60, 90))\n      \n\n    # Your Code Here"
  },
  {
    "objectID": "scripts/advanced-r/advanced-r.html",
    "href": "scripts/advanced-r/advanced-r.html",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Advanced dpylr",
      "Workshop: Advanced Data wrangling with dplyr"
    ]
  },
  {
    "objectID": "scripts/advanced-r/advanced-r.html#fortgeschrittene-dplyr-funktionen",
    "href": "scripts/advanced-r/advanced-r.html#fortgeschrittene-dplyr-funktionen",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Advanced dpylr",
      "Workshop: Advanced Data wrangling with dplyr"
    ]
  },
  {
    "objectID": "scripts/advanced-r/advanced-r.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "href": "scripts/advanced-r/advanced-r.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "title": "Advanced Dplyr",
    "section": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen",
    "text": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen\nDie case_when Funktion in dplyr ermöglicht es, basierend auf bestimmten Bedingungen verschiedene Werte für eine Spalte in einem Datensatz auszuwählen und abhängig vom Variablenwert einen neue Variable zu erstellen. Es ist eine sehr einfache Möglichkeit, if-else Statements in eine dplyr-Pipeline zu integrieren.\nDie Syntax eines case_when() Befehles entspricht einer einfachen if oder if-else Verknüpfung. Eine if-else Verknüpfung prüft eine Bedingung und führt wenn diese Erfüllt ist einen definierten Befehl aus. Ist die Bedingung nicht erfüllt, wird andernfalls (else) ein anderer Befehl ausgeführt:\n\n\n# Vektor von Zahlen darauf testen, ob sie gerade oder ungerade sind:\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei geteilt keinen Rest hat, dann schreibe Gerade, ansonsten Ungerade.\n\nifelse(zahlen %% 2 == 0,\"Gerade\",\"Ungerade\")\n\ncase_when() funktioniert nach der Gleichen Logik:\n\n\n\nDabei wird immer zuerst die if-Bedinungen und deren Output definiert und anschließend mit TRUE die else Bedingung, welche angibt was passiert wenn die if - Bedingung nicht zutrifft:\n\n\n\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei\n# geteilt keinen Rest hat, dann schreibe Gerade, \n# ansonsten Ungerade.\n\ncase_when(zahlen %% 2 == 0 ~ \"Gerade\",\n          TRUE ~\"Ungerade\")\n\nDer Unterschied von case_when() und ifelse() ist, dass mit case_when() auch mehrere Bedingungen definiert werden können (z.B. wenn eine Variable in mehrere Kategorien eingeteilt werden soll):\n\nHier gibt es auch ein finales else Statement, dieses muss aber nicht sein, solange die Kategorien alle Fälle abdecken!\n\n2.0.1 case_when: Beispiele\nDas folgende ist ein einfaches Beispiel, bei dem case_when() verwendet wird, um in einem Datensatz eine neue Spalte Alter_Kategorie anhand des Werts in der Spalte Alter zu kategorisieren. Wenn eine neue Spalte erstellt werden soll, muss case_when() immer mit mutate() kombiniert werden:\n\n# Beispiel Datensatz erstellen\ndf &lt;- data.frame(Name = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45))\nhead(df)\n\n# mutate und case_when verwenden, um neue Variable in Abhängigkeit vom Alter zu erstellen:\ndf %&gt;% \n  mutate(Alter_Kategorie = case_when(\n    Alter &lt;= 25 ~ \"jung\",\n    Alter &gt; 25 & Alter &lt;= 35 ~ \"mittel\",\n    Alter &gt; 35 ~ \"alt\"\n  ))\n\nEin weiteres Beispiel haben wir letzte Woche bei der MPT-Modellierung der Daten von Frenken et al. gesehen. Im Datensatz ist shoot als 0 und not shoot als 1 kodiert. Für die MPT Modellierung benötigen wir aber für jede Kategorie (Black / Gun, Black / Phone, White / Gun, White / Phone) die Hits und Misses, also die Fehlerraten. Der Datensatz hat praktischerweise eine Spalte, die genau kodiert, was die VP gesehen hat:\n\n\nhead(Study_2_dm)\n\n# A tibble: 6 × 6\n  subj_idx stimulus stim     rt response condition\n     &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt; &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;    \n1        0 gun      bg    0.506        0 black    \n2        0 phone    wp    0.437        0 white    \n3        0 phone    wp    0.483        0 white    \n4        0 phone    bp    0.594        1 black    \n5        0 phone    wp    0.552        1 white    \n6        0 gun      wg    0.417        1 white    \n\n\nIn der zweiten Spalte stimulus ist angebenen, welches Objekt der jeweils zusammen mit der Hautfarbe gezeigt wurde. Da wir wissen, dass nur bei “gun” geschossen werden darf, können wir nun in Kombination mit response (0 - shoot, 1 - not shoot) die Hits mit case_when() in einer neuen Spalte ACC kodieren.\nDazu Verknüpfen wir hier zwei Bedingungen mit dem & Operator. Die erste Bedingung bezieht sich auf die Spalte stimulus, hier muss geprüft werden, welcher Stimulus gezeigt wurde (Gun vs. Phone). Die zweite Bedingung bezieht sich auf die Spalte response. Hier muss geprüft werden, ob geschossen wurde oder nicht. Ingesamt müssen also vier Bedinungen definiert werden, für jede Kombination von Objekt und Response:\n\nAccuracy in Abhängigkeit von Gegebener Response und Stimulus\n\n\nStimulus\nResponse\nAccuracy\n\n\n\n\nGun\n0\n1\n\n\nGun\n1\n0\n\n\nPhone\n0\n0\n\n\nPhone\n1\n1\n\n\n\nDies müssen wir nun in ein case_when() Befehl übernehmen\n\nMit mutate() die neue Outputspalte benennen\nInnerhalb von mutate() mit case_when() die Accuracy in Abhängigkeit der Spalten Stimulus und Response umkodieren:\n\n\n\nfreq_dat &lt;- Study_2_dm %&gt;% mutate(ACC = case_when(stimulus == \"gun\" & response == 0 ~ 1,\n                                                  stimulus == \"gun\" & response == 1 ~ 0,\n                                                  stimulus == \"phone\" & response == 1 ~ 1,\n                                                  stimulus ==\"phone\" & response == 0 ~ 0))\n\nAnschließend müssen wir nun die Hits und Misses auszählen. Generell eignen sich zum Auszählen von bestimmten Bedingungskombinationen oder Trials die Funktionen group_by(), summarise() und n(). n() ist eine einfache Zählfunktion, welche innerhalb von group_by() %&gt;% summarise() dazu führt, dass die alle Beobachtungen der gruppierten Variablen (z.B. Subject & Bedingung) innerhalb von summarise() gezählt werden.\nAlso zum Beispiel, wieviele Beobachtung von Subject 1 gibt es in Bedingung A, B und C. Dies macht aber nur Sinn, wenn ihr Daten auf Trial-Ebene (also für jede Versuchsperson alle Antworten über das ganez Experiment) vorliegen habt. Dies ist hier der Fall, da für jede Person und jede Bedingung, die diese Person durchlaufen hat, die gegebenen Antworten im Datensatz in der Spalte responses vorliegen.\n\n\nfreq_dat &lt;- freq_dat %&gt;% group_by(subj_idx,stim) %&gt;% \n  summarise(hits = sum(ACC), \n            ntrials=n(), \n            miss=ntrials-hits)\n\n`summarise()` has grouped output by 'subj_idx'. You can override using the\n`.groups` argument.\n\nhead(freq_dat)\n\n# A tibble: 6 × 5\n# Groups:   subj_idx [2]\n  subj_idx stim   hits ntrials  miss\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n1        0 bg       30      31     1\n2        0 bp       28      29     1\n3        0 wg       23      27     4\n4        0 wp       28      33     5\n5        1 bg       28      30     2\n6        1 bp       16      18     2\n\n\nWas passiert hier genau ? Schritt für Schritt:\n\ngroup_by(subj_idx, stim) : Wir gruppieren zunächst nach Subject und Bedingung\n\nsummarise(...\n\nhits = sum(ACC) - Wir haben alle Hits mit case_when als 1 kodiert, also können wir die Hits einfach berechnen, indem wir diese aufsummieren.\nnTrials = n() - Auszählen wieviele Responses ein Subject in jeder stimulus Bedinungen gegeben hat (Gesamtanzahl von gegebenen Antworten in einer Bedingungen pro Person\nmiss = ntrials - hits Alles was kein Hit ist muss ein Miss sein ! Daher können wir einfach die Hits von der Gesamtzahl der Antworten abziehen und erhalten die Anzahl der Misses in jeder Bedinung pro Subject !",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Advanced dpylr",
      "Workshop: Advanced Data wrangling with dplyr"
    ]
  },
  {
    "objectID": "scripts/advanced-r/advanced-r.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "href": "scripts/advanced-r/advanced-r.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "title": "Advanced Dplyr",
    "section": "3 Long- und Wide-Datenformat - Datentransformation in dplyr",
    "text": "3 Long- und Wide-Datenformat - Datentransformation in dplyr\nNun haben wir die Daten von Frenken et al. neu kodiert und die nötigen Informationen im Datensatz, um mit MPTinR zu arbeiten. Allerdings liegen die Daten noch im sogenannten long - Format vor. Oftmals ist es jedoch notwendig, Daten entweder von einem long in ein wide Format oder umgekehrt zu transfomieren. Im Kontext von Datenanalyse und statistischer Modellierung gibt es in der Regel zwei Haupttypen von Datenformaten: wide-Format (breit-Format) und long-Format (lang-Format).\n\n3.1 Wide-Format\n\nIn einem wide Format sind die verschiedenen Merkmale einer einzigen Beobachtung in separaten Spalten dargestellt.\nEine einzige Zeile in einem breiten Datensatz repräsentiert eine Beobachtung.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, wäre in einem breiten Format dargestellt, wobei jede Spalte einem bestimmten Fach entspricht.\nVorteil: Es ist einfach, schnelle Übersichten über große Datenmengen zu erhalten.\n\nBeispiel:\n\nwide_df &lt;- data.frame(Schüler_ID = c(1, 2, 3), \n                      Mathe = c(89, 76, 92), \n                      Englisch = c(92, 88, 95), \n                      Wissenschaft = c(88, 72, 98))\nwide_df\n##   Schüler_ID Mathe Englisch Wissenschaft\n## 1          1    89       92           88\n## 2          2    76       88           72\n## 3          3    92       95           98\n\n\n\n3.2 Long-Format\n\nIm long Format sind alle Merkmale einer einzigen Beobachtung in einer Zeile dargestellt.\nEine einzige Spalte in einem long Datensatz repräsentiert ein bestimmtes Merkmal.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, könnte in einem langen Format dargestellt werden, wobei jede Zeile einer bestimmten Schüler-Fach-Kombination entspricht.\nVorteil: Es ist einfach, bestimmte Merkmale für verschiedene Beobachtungen zu vergleichen oder zu analysieren. Außerdem ist es für manche statistische Methoden, wie lineare Regression oder einer ANOVA, das bevorzugte Format.\n\nBeispiel:\n\nlong_df &lt;- data.frame(Schüler_ID = c(rep(1, 3), rep(2, 3), rep(3, 3)), \n                      Fach = c(rep(\"Mathe\", 3), rep(\"Englisch\", 3), rep(\"Wissenschaft\", 3)), \n                      Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\nlong_df\n##   Schüler_ID         Fach Note\n## 1          1        Mathe   89\n## 2          1        Mathe   92\n## 3          1        Mathe   88\n## 4          2     Englisch   76\n## 5          2     Englisch   88\n## 6          2     Englisch   72\n## 7          3 Wissenschaft   92\n## 8          3 Wissenschaft   95\n## 9          3 Wissenschaft   98\n\nEs ist wichtig, das Konzept des wide- und long-Formats zu verstehen, da es bei der Datenaufbereitung und Analyse eine wichtige Rolle spielt. Zum Beispiel kann ein wide -Datensatz schwer zu analysieren sein, wenn man Vergleiche zwischen bestimmten Merkmalen über mehrere Beobachtungen hinweg machen möchte (Beispiel Ergebnisse der Diffusionsmodellierung!). Hier ist es oft besser, den Datensatz in ein long-format zu bringen.\n\n\n3.3 pivot - Funktionen in dplyr\nDie Funktionen pivot_wider und pivot_longer gehören zu den Funktionen von dplyr und dienen dazu, Datensätze zu transformieren.\npivot_wider verwandelt einen long-Format Datensatz in einen wide-Format Datensatz, indem es die Werte einer bestimmten Spalte zu neuen Spalten umbenennt. Dies kann manuell durchgeführt werden, aber pivot_wider macht dies automatisch und erleichtert so die Datentransformation:\n\n\npivot_longer verwandelt einen wide-Format Datensatz in einen long-Format Datensatz, indem es die Spalten mit bestimmten Werten in einer neuen Spalte zusammenfasst.\nEin Beispiel für den Einsatz von pivot_wider:\n\n# long data example\nlong_data &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\", \"John\", \"Jane\", \"Jim\"),\n  subject = c(\"physics\", \"physics\", \"physics\", \"math\", \"math\", \"math\"),\n  score = c(85, 90, 80, 75, 80, 70)\n)\n\nlong_data\n## # A tibble: 6 × 3\n##   name  subject score\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 John  physics    85\n## 2 Jane  physics    90\n## 3 Jim   physics    80\n## 4 John  math       75\n## 5 Jane  math       80\n## 6 Jim   math       70\n\n\n# pivot to wide format\npivot_wider(long_data, names_from = subject, values_from = score)\n## # A tibble: 3 × 3\n##   name  physics  math\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n## 1 John       85    75\n## 2 Jane       90    80\n## 3 Jim        80    70\n\nEin Beispiel für den Einsatz von pivot_longer:\n\n# wide data example\nwide_df &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\"),\n  Mathe_Note = c(85, 90, 80),\n  Englisch_Note = c(75, 80, 70)\n)\n\n\nwide_df\n## # A tibble: 3 × 3\n##   name  Mathe_Note Englisch_Note\n##   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n## 1 John          85            75\n## 2 Jane          90            80\n## 3 Jim           80            70\n\n\n# pivot to long format\npivot_longer(wide_df, cols = c(Mathe_Note, Englisch_Note), \n             names_to = \"Schüler\", values_to = \"Note\")\n## # A tibble: 6 × 3\n##   name  Schüler        Note\n##   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n## 1 John  Mathe_Note       85\n## 2 John  Englisch_Note    75\n## 3 Jane  Mathe_Note       90\n## 4 Jane  Englisch_Note    80\n## 5 Jim   Mathe_Note       80\n## 6 Jim   Englisch_Note    70\n\n\n\n3.4 Argumente für pivot_longer und pivot_wider\npivot_longer benötigt mindestens zwei Argumente:\n\ncols: Dies ist ein Zeichenvektor, der angibt, welche Spalten im Datensatz zusammengefasst werden sollen. -Schüler_ID bedeutet zum Beispiel, dass alle Spalten außer Schüler_ID zusammengefasst werden sollen.\nnames_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die zusammengefassten Werte gespeichert werden.\nvalues_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die Werte gespeichert werden, die aus den zusammengefassten Spalten stammen.\n\n\npivot_wider benötigt mindestens zwei Argumente:\n\nnames_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Namen für die neuen Spalten verwendet werden soll.\nvalues_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Werte für die neuen Spalten verwendet werden soll.\n\n\nBeide Funktionen haben auch weitere optionale Argumente wie values_fill und names_prefix oder names_sep()um die Daten bei Bedarf weiter anzupassen.\nHier nun die Umformung der Daten von Frenken et al., welche wir vom long Format in das wide Format bringen müssen:\n\n\n#| echo: true\n#| output: true\n#| warning: false\n#| code-overflow: wrap\n#| collapse: true\n\nfreq_dat %&gt;% pivot_wider(names_from = c(\"stim\"),\n                         values_from = c(\"hits\",\"miss\"),\n                         id_cols=\"subj_idx\")\n\n# A tibble: 137 × 9\n# Groups:   subj_idx [137]\n   subj_idx hits_bg hits_bp hits_wg hits_wp miss_bg miss_bp miss_wg miss_wp\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1        0      30      28      23      28       1       1       4       5\n 2        1      28      16      27      11       2       2       2       1\n 3        3      25      10      33      23       0       0       1       1\n 4        4      35      24      27      33       1       0       0       0\n 5        5      31      28      25      23       1       2       4       4\n 6        6      34      26      18      31       0       0       5       0\n 7        7      29      24      21      36       2       0       1       1\n 8        8      22      29      25      28       0       2      10       2\n 9        9      28      23      22      22       2       0       2       1\n10       10      26      35      25      24       4       0       0       3\n# ℹ 127 more rows\n\n\nWas passiert hier genau ? Schritt für Schritt:\n\nnames_from = c(stim) - Die neuen Spalten sollen aus der Spalte stim benannt werden - (bg,bp,wp,wg)\nvalues_from = c(\"hits\",\"miss\") - in den neuen Spalten sollen die Werte der Hits und Miss Spalten stehen - durch den ersten und zweiten Schritt entstehen also Spalten, die jeweils hits_bg etc. enthalten. Also die Hits aus der Bedingung “bg” usw.\nid_cols - dies soll für jedes Subject einzeln geschehen.",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Advanced dpylr",
      "Workshop: Advanced Data wrangling with dplyr"
    ]
  },
  {
    "objectID": "scripts/advanced-r/advanced-r.html#übungen",
    "href": "scripts/advanced-r/advanced-r.html#übungen",
    "title": "Advanced Dplyr",
    "section": "4 Übungen",
    "text": "4 Übungen\n\n4.1 case_when\n1.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Note_Kategorie” zu erstellen, die “Sehr gut” für Noten über 90, “Gut” für Noten zwischen 80 und 90 und “Schlecht” für Noten unter 80 angibt.\n\ndf &lt;- data.frame(Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\n# Your Code Here\ndf_categorized &lt;- df %&gt;% \n  mutate(Note_Kategorie = case_when(\n    Note &gt;= 90 ~ \"Sehr gut\",\n    Note &gt;= 80 & Note &lt; 90 ~ \"Gut\",\n    Note &lt; 80 ~ \"Schlecht\"\n  ))\n\n2.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Bestanden” zu erstellen, um zu Prüfen ob ein Schüler einer bestimmten Schulform eine Prüfung bestanden hat. Die Bestehensgrenzen sind wie folgt.\n-   Für die Hauptschule liegt die Bestehensgrenze bei 50 %\n-   Für die Realschule liegt die Bestehensgrenze bei 60 %\n-   Für das Gymnasium liegt die Bestehensgrenze bei 70 %\nKodieren Sie das bestehen entweder mit “Pass” oder “Fail”.\n\ndf &lt;- data.frame(Note = runif(100,min=0, max=100),\n                 Schulform = sample(c(\"Gymnasium\",\"Realschule\", \"Hauptschule\"),\n                                    size = 100, replace = T))\n# Your Code here\ndf %&gt;% \n  mutate(Pruefungsergebnis = case_when(\n    Schulform == \"Hauptschule\" & Note &gt;= 50 ~ \"Pass\",\n    Schulform == \"Realschule\" & Note &gt;= 60 ~ \"Pass\",\n    Schulform == \"Gymnasium\" & Note &gt;= 70 ~ \"Pass\",\n    TRUE ~ \"Fail\"\n  ))\n\n3.) Nutzen Sie die Funktion case_when und die dplyr-Library in R, um eine neue Spalte in dem Datensatz “df” zu erstellen, die die Einkommenskategorie jeder Person basierend auf ihrem Berufsstatus und ihrem Einkommen kategorisiert. Die Einkommenskategorien sollten wie folgt sein:\n\nFür Angestellte mit einem Einkommen von bis zu 50.000: “niedrig”\nFür Angestellte mit einem Einkommen zwischen 50.000 und 75.000: “mittel”\nFür Angestellte mit einem Einkommen über 75.000: “hoch”\nFür Freiberufler mit einem Einkommen von bis zu 60.000: “niedrig”\nFür Freiberufler mit einem Einkommen zwischen 60.000 und 100.000: “mittel”\nFür Freiberufler mit einem Einkommen über 100.000: “hoch”\nFür Ruheständler mit einem Einkommen von bis zu 30.000: “niedrig”\nFür Ruheständler mit einem Einkommen über 30.000: “mittel_hoch”\n\n\ndf &lt;- data.frame(ID = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45),\n                 Berufsstatus = c(\"Angestellter\", \"Freiberufler\", \"Ruheständler\"),\n                 Einkommen = c(45000, 75000, 32000))\n\ndf %&gt;% \n  mutate(Einkommenskategorie = case_when(\n    Berufsstatus == \"Angestellter\" & Einkommen &lt;= 50000 ~ \"niedrig\",\n    Berufsstatus == \"Angestellter\" & Einkommen &gt; 50000 & Einkommen &lt;= 75000 ~ \"mittel\",\n    Berufsstatus == \"Angestellter\" & Einkommen &gt; 75000 ~ \"hoch\",\n    Berufsstatus == \"Freiberufler\" & Einkommen &lt;= 60000 ~ \"niedrig\",\n    Berufsstatus == \"Freiberufler\" & Einkommen &gt; 60000 & Einkommen &lt;= 100000 ~ \"mittel\",\n    Berufsstatus == \"Freiberufler\" & Einkommen &gt; 100000 ~ \"hoch\",\n    Berufsstatus == \"Ruheständler\" & Einkommen &lt;= 30000 ~ \"niedrig\",\n    Berufsstatus == \"Ruheständler\" & Einkommen &gt; 30000 ~ \"mittel_hoch\"\n  ))\n##      ID Alter Berufsstatus Einkommen Einkommenskategorie\n## 1 Peter    25 Angestellter     45000             niedrig\n## 2  Anna    35 Freiberufler     75000              mittel\n## 3   Max    45 Ruheständler     32000         mittel_hoch\n\n\n\n4.2 Pivotting\n1.) Konvertieren Sie die folgenden Datensatz von wide to long. Erstellen Sie aus den Spalten zwei neue Spalten mit den Namen (month und index). Nutzen Sie dazu das Argument names_sep = \"_\". Die Werte sollen in die Spalte “N” geschrieben werden.\nTip: Sie müssen bei names_to einen Vektor mit den Namen der neuen Spalten angeben.\n\ndf_wide &lt;- data.frame(\n  Jan_sales = c(10, 40, 70),\n  Feb_sales = c(20, 50, 80),\n  Mar_sales = c(30, 60, 90)\n)\n\n# Your Code here\ndf_long &lt;- df_wide %&gt;% pivot_longer(cols=everything(),names_to = c(\"month\",\"index\"), values_to = \"N\",names_sep = \"_\")\n\n2.) Konvertieren Sie die folgendn Datensatz von long in wide.\n\n    df_long &lt;- data.frame(\n      ID = c(\"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\"),\n      month = c(\"Jan\", \"Jan\", \"Jan\", \"Feb\", \"Feb\", \"Feb\", \"Mar\", \"Mar\", \"Mar\"),\n      sales = c(10, 40, 70, 20, 50, 80, 30, 60, 90))\n      \n\n    # Your Code Here\n    df_long %&gt;% pivot_wider(names_from = month, values_from = sales)",
    "crumbs": [
      "Seminar kognitive Modellierung",
      "Advanced dpylr",
      "Workshop: Advanced Data wrangling with dplyr"
    ]
  }
]