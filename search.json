[
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Auf der nachfolgenden Seite findet ihr den kompletten Workshop zur MPT Modellierung. Zudem das Tutorial als Video in dem alles Schritt für Schritt durchgegangen wird! Da das Video aus dem letzten Semester ist, es kann kleinere Abweichungen geben !\n\n\n\nHeute Beschäftigen wir uns mit der Anwendung von MPT Modellen in R. Hierzu nutzen wir einen Datensatz von Frenken et al. (2022), der sich mit der Verarbeitung von Stereotypen befasst. Zunächst müssen wir aber die entsprechenden Packages installieren. Wir nutzen das Package (MPTinR), das viele Funktionalitäten für das MPT Modelling mit sich bringt ! Zusätzlich installieren wir noch ggpubr und effectsize um später einige nette Features beim Plotten und Analysieren der Ergebnisse zu haben:\n\n\nCode\n# Install MPT Package\n# install.packages(\"MPTinR\")\n# install.packages(\"ggpubr\")\n# install.packages(\"effectsize\")\n\n\nNun laden wir die Pakete die wir für den weiteren Verlauf brauchen werden sowie unseren Datensatz ein:\n\n\nCode\nlibrary(MPTinR)\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(effectsize)\n\n\nStudy_2_dm &lt;- read_csv(\"Data/Study_2_dm.csv\", \n                       col_types = cols(...1 = col_skip(), \n                                        stimulus = col_factor(levels = c(\"gun\",\"phone\")), \n                                        response = col_integer(), \n                                        condition = col_factor(levels = c(\"black\",\"white\"))))\n\n\n\n\n\nWie schon in der letzten Sitzung vorgestellt handelt es sich beim beim First Person Shooter Task (FPST) um ein Paradigma, mit dem Stereotype und deren Auswirkungen auf Entscheidungen untersucht werden können. Hierbei werden üblicherweise verschiedene Ethnien (z.B. schwarze oder weiße Personen), entweder mit einer Waffe (threat) oder einem ungefährlichem Objekt (z.B. Telefon, harmless) gezeigt. Die Versuchspersonen werden instruiert unabhängig von der Hautdfarbe so schnell und korrekt wie möglich auf bewaffnete Ziele zu schiessen. Im Gegensatz dazu soll nicht auf unbewaffnete Ziele geschossen werden.\n\n\n\nVerschiedene Paradigmen des FPST\n\n\n\n\n\nWir werden heute versuchen, die Daten von Frenken et al. mit dem Process Dissociation Model zu fitten. Dieses müssen wir aber zunächst in R definieren. Die Modellgleichungen leiten sich aus folgendem zugrundeliegendem Wahrscheinlichkeitsbaum ab:\n\n\n\nProcess Dissociation Model (Payne,2001)\n\n\nDas Modell hat nur zwei freie Parameter, zum einen c , der die Wahrscheinlichkeit angibt das kontrollierte Verarbeitungsprozesse aktiviert werden und zum anderen a der die Wahrscheinlichkeit angibt, dass diese Kontrolle fehlschlägt und eine stereotype Verarbeitung aktiviert wird.\n\nZur Erinnerung: das experimentelle Paradigma in Payne et al. ist ein Sequential Priming Task, bei dem die Versuchspersonen ein schwarzes oder weißes Gesicht gesehen haben und dann entscheiden sollten, ob es sich bei dem anschließend gezeigten Objekt um einen Waffe oder ein Werkzeug handelt:\n\n\nIn unserem Fall ist das Paradigma also ähnlich, die Versuchspersonen sehen entweder einen Schwarzen oder einen Weißen (unbewaffnet vs. bewaffnet) und sollen dann entscheiden, ob geschossen werden soll oder nicht. Das heisst, wir können das Modell im Prinzip ekaxt so wie es ist, auf die Daten von Frenken et al. übertragen.\n\nNun müssen wir das Modell allerdings definieren. Hierzu müssen die Gleichungen für jeden Wahrscheinlichkeitsbaum aufgestellt werden. Jeder Baum repräsentiert eine Zielkategorie:\n\n\n\n\nHautfarbe\nObjekt\n\n\n\n\nSchwarz\nWaffe\n\n\nWeiß\nWaffe\n\n\nSchwarz\nTelefon\n\n\nWeiß\nTelefon\n\n\n\nEs müssen nun für jede dieser Outcome Kategorien Gleichungen für die unterschiedlichen Pfade definiert werden, welche zu den jeweiligen “Hits” und “Misses” führen (“+” bzw. “-”auf der Abbildung). Zusätzlichmüssen insgesamt 4 Parameter definiert werden. A und C jeweils für schwarze und weiße Hautfarbe, um Unterschiede in den Parametern, die auf die Hautfarbe zurückgehen, identifizieren zu können:\n\n\nFür MPTinR können die Gleichungen in einem einfachen “String” definiert werden. Hier ist die erste Gleichung für die Kategorie “White/ Phone” bereits eingetragen. In der Kategorie “White/Phone” führt der Pfad \\[c\\] und der Pfad \\[(1-c) \\cdot a\\] zu einem “Hit”.\nIm Gegensatz dazu führt \\[(1-c) \\cdot (1-a)\\] zu einem Miss. Tragt immer zunüchst die Gleichung für einen Hit und dann die Gleichungen für einen Miss ein. Da zwischen Pfaden die Wahrscheinlichkeiten addiert werden können, ist also die Gesamtgleichung eines “Hits” für die Kategorie “White/Phone”:\n\\[c + (1-c) \\cdot a\\] und für einen Miss\n\\[ (1-c) \\cdot (1-a)\\]\nda es nur einen Pfad zu einem “Miss” gibt ! Vervollständigt nun die Gleichungen für die restlichen Kategorien. Denkt daran das Ihr die Parameter für die schwarze und weiße Hautfarbe unterschiedlich benennt!\n\n\n\nCode\n# Parameters for Black and White Skin Color! \n# c_w = c white, a_w = a white c_b = black, a_b = a black\n\n\npd &lt;- \"\n# WT\nc_w+(1-c_w)*a_w\n(1-c_w)*(1-a_w)\n\n# WG\nc_w + (1-c_w)*(1-a_w)\n(1-c_w) * a_w\n\n# BT\nc_b + (1-c_b)*(1-a_b)\n(1-c_b) * a_b\n\n# BG\nc_b+(1-c_b)*a_b\n(1-c_b)*(1-a_b)\n\"\n\n\n\nWenn ihr die Gleichungen alle definiert habt, könnt ihr das Modell mit MPTinR auf Korrektheit überprüfen lassen:\n\n\n\nCode\ncheck.mpt(textConnection(pd))\n\n\n$probabilities.eq.1\n[1] TRUE\n\n$n.trees\n[1] 4\n\n$n.model.categories\n[1] 8\n\n$n.independent.categories\n[1] 4\n\n$n.params\n[1] 4\n\n$parameters\n[1] \"a_b\" \"a_w\" \"c_b\" \"c_w\"\n\n\nWir sehen hier einen Test ob alle Wahrscheinlichkeiten sich zu 1 addieren, die Anzahl der Bäume, der gesamten und unabhängigen Kategorien, sowie der Parameter. In den Daten müssen immer exakt so viele Kategorien vorhanden sein, wie im Modell definiert ! Zusätzlich darf die Anzahl Parameter nicht größer sein, als die Anzahl der unabhängigen Kategorien - ansonsten ist das Modell nicht identifizierbar - das beudeut das mehr unbekannte als bekannte Größen vorhanden sind - die Gleichungen sind nicht lösbar.\n\n\n\nUm die Daten von Frenken et al. zu Modellieren, müssen die Daten zunächst in ein anderes Format gebracht und umkodiert werden - in “Hit” und “Miss” für jede Kategorie. Dies kann wie folgt mit mutate(),case_when() und pivot_wider() getan werden:\n\n\nCode\n# Define Edge Correction function\nedge_correct &lt;- function(x){\n  x = ifelse(x==0, x+1,x)\n  return(x)\n}\n\n# Recode Data from Frenken et al.\n# Umkodieren der Daten hin zu Accuracy \n\n# In den Daten sind die Antworten nicht nach Accuracy, sondern nach response codiert (response coding). Daher müssen wir die Daten vorher umkodieren, damit die Accuracy codiert wird. Dies können wir mit case_when tun. Kodiert nun den Datensatz wie folgt neu in dem ihr einen neue Spalte \"ACC\" erzeugt, die in Abhängigkeit von \"stimulus\" erzeugt wird:\n\n# Wenn Stimulus = gun ist, dann ist response 0 korrekt - also 1 \n# Wenn Stimulus = gun ist, dann ist response 1 inkorrekt - also 0\n\n# Bei Phone stimmt die zuordnung zufällig, da 1 bedeutet nicht zu schiessen. Ist aber nicht immer der Fall ! Trotzdem müssen wir die Zuordnung eintragen, damit für alle elemente in der Spalte ACC ein Wert steht!\n\nfreq_dat &lt;- Study_2_dm %&gt;% mutate(ACC= case_when(stimulus == \"gun\" & response == 0 ~ 1,\n                                                 stimulus == \"gun\" & response ==1 ~0,\n                                                 stimulus == \"phone\" & response ==1 ~ 1,\n                                                 stimulus == \"phone\" & response == 0 ~ 0))\n\n\n# Berechnung der hits und misses und Auszählen der Trials jeder VP den jeweiligen Bedingungen \nfreq_dat &lt;- freq_dat %&gt;% group_by(subj_idx,stim) %&gt;% summarise(hits = sum(ACC), ntrials=n(), miss=ntrials-hits)\n\n\n`summarise()` has grouped output by 'subj_idx'. You can override using the\n`.groups` argument.\n\n\nCode\n# Transformieren zum Wide Format für MPTinR\nfreq_dat &lt;- freq_dat %&gt;% pivot_wider(.,names_from = c(\"stim\"),values_from = c(\"hits\",\"miss\"),values_fill = F,id_cols=\"subj_idx\") \n\n# Anpassung der Reihenfolge der Daten analog zu den Modellgleichungen.\nfreq_dat &lt;- freq_dat %&gt;% relocate(subj_idx,hits_wp,miss_wp,hits_wg,miss_wg,hits_bp,miss_bp,hits_bg,miss_bg) \n\n# Anwenden einer Edge-Correction, um die Nullbeobactungen in der \"miss\" Kategorie zu eleminieren\nfreq_dat &lt;- freq_dat %&gt;% mutate(across(starts_with(\"miss\"), ~ edge_correct(.)))\n\n\nhead(freq_dat)\n\n\n# A tibble: 6 × 9\n# Groups:   subj_idx [6]\n  subj_idx hits_wp miss_wp hits_wg miss_wg hits_bp miss_bp hits_bg miss_bg\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1        0      28       5      23       4      28       1      30       1\n2        1      11       1      27       2      16       2      28       2\n3        3      23       1      33       1      10       1      25       1\n4        4      33       1      27       1      24       1      35       1\n5        5      23       4      25       4      28       2      31       1\n6        6      31       1      18       5      26       1      34       1\n\n\n\n\n\n\n\nZum fitten des Modells müssen die Daten für MPTinR in der gleichen Reihenfolge angeordnet sein, wie auch die Gleichungen definiert sind! Dies solltet ihr immer überprüfen, bevor ein Modell gefittet wird.\nNun kann das Modell mit dem Befehl fit.mpt geschätzt werden. Dabei müssen wir die genauen Spalten unseres Dataframes freq_dat angeben, da eine Spalte subj_idx im Dataframe enthalten ist, die keine Häufigkeiten enthält. Die fit.mpt Funktion nimmt allerdings nur Daten an, die die gleiche Anzahl an Spalten aufweisen, wie im Modell definierten gesamten Kategorien vorhanden sind. Andernfalls wird eine Fehlermeldung ausgegeben. Da im Dataframe insgesamt 9 Spalten vorhanden sind, muss also die erste Spalte ausgelassen werden. Dies kann einfach mit der Auswahl der Spalten mit dem Zugriff [Zeilen,Spalten] geschehen. Als model.filename wird der String übergeben in dem die Gleichungen definiert wurden. Dazu wird erneut textConnection() verwendet. Das Argument n.optim definiert die Anzahl der Optimierungsdurchläufe, wird dieses Argument nicht abgegeben, werden standardmäßig 5 Optimierungsdurchläufe durchgeführt.\n\n\nCode\n# Fit Process Disassociaton Model \nfit &lt;- fit.mpt(freq_dat[,2:9],model.filename = textConnection(pd),n.optim = 10)\n\n\nPresenting the best result out of 10 minimization runs.\n\n\n[1] \"Model fitting begins at 2023-06-30 14:10:53.815274\"\n[1] \"Model fitting stopped at 2023-06-30 14:11:00.164152\"\nTime difference of 6.348878 secs\n\n\nNun können verschiedene Informationen mit Hilfe des $ Operator aus dem fit Objekt ausgegeben werden, hierzu benutzen wir den $ Operator. Zunächst sehen wir uns den Mittelwert der unterschiedlichen individuellen Parameter nach Gruppen/Bäumen an, sowie die Parameter für die aggregierten Daten (also für die aufsummierten Hits und Misses über alle Personen):\n\n\n\nCode\n# Speichern der Parameter in einen neuen Dataframe\n\ntheta_mean &lt;- fit$parameters$mean[1:4,1]\ntheta_aggregated &lt;- fit$parameters$aggregated\n\nprint(theta_aggregated)\n\n\n    estimates lower.conf upper.conf\na_b 0.5624551  0.5191306  0.6057796\na_w 0.5048275  0.4670972  0.5425577\nc_b 0.8738353  0.8626938  0.8849767\nc_w 0.8388067  0.8266458  0.8509677\n\n\nWir sehen das es wohl einen signifikanten Unterschied zwischen der Hautfarbe bei dem Control Parameter c auf aggregierter Ebene geben könnte. Auf dem Parameter a, welcher automatisierte, von Stereotypen getriebenen Prozesse abbildet, zeigt sich hingegen eine Überlappung der Konfidenzintervalle. Um zu prüfen ob es einen signifikanten Unterschied auf individuellem Level gibt, müssen wir die individuellen Parameter der einzelnen Versuchspersonen heranziehen. Zünachst müssen diese aus dem fit Objekt in einen Dataframe überführt werden:\n\n\nCode\n# Erstellen einer Matrix für die Parameter\ntheta_subj &lt;- matrix(NA,nrow=nrow(freq_dat),ncol=4)\ncolnames(theta_subj) &lt;- c(\"a_b\",\"a_w\",\"c_b\",\"c_w\")\n\n# Loopen über alle individuellen Parameter, nur die Punktschätzer werden gespeichert\nfor (i in 1:nrow(freq_dat)){\n  theta_subj[i,] &lt;- fit$parameters$individual[,1,i]\n}\n\n# Nun lassen wir uns die ersten 10 Zeilen ausgeben, um uns das Ergebnis anzusehen\nhead(theta_subj,10)\n\n\n            a_b       a_w       c_b       c_w\n [1,] 0.5166667 0.4943820 0.9332592 0.7003367\n [2,] 0.6250000 0.4528302 0.8222222 0.8477011\n [3,] 0.7027027 0.4137931 0.8706294 0.9289216\n [4,] 0.5901639 0.5483871 0.9322222 0.9348739\n [5,] 0.6808511 0.4821429 0.9020833 0.7139208\n [6,] 0.5645161 0.8743169 0.9343915 0.7513587\n [7,] 0.3827160 0.6271186 0.8954839 0.9275184\n [8,] 0.5974026 0.8108108 0.8920056 0.6476190\n [9,] 0.3846154 0.6571429 0.8916667 0.8731884\n[10,] 0.1724138 0.2571429 0.8388889 0.8504274\n\n\n\n\n\nJetzt können wir mit Hilfe eines paired t-tests und der t.test() Funktion (wir vergleichen die Parameter innerhalb von Personen zwischen den experimentellen Bedinungen) analysieren, ob es einen signifikanten Unterschied auf individueller ebene gibt. Die t-test() Funktion nimmt als Argument jeweils die zwei Spalten, die verglichen werden sollen. Wir müssen also für die beiden Parameter \\(a\\) und \\(c\\) jeweils einen t-test rechnen, der beide Gruppen vergleicht. Wir setzen zusätzlich das Argument paired=TRUE, da es sich um einen paired t-test handelt und das Argument var.equal=T, da wir annehmen das die Varianz in beiden Gruppen gleich ist:\n\n\nCode\n# t-test für beide Parameter \ntheta_a_comp &lt;-t.test(theta_subj[,\"a_b\"],theta_subj[,\"a_w\"],\n       paired = T,\n       var.equal = T)\n\ntheta_c_comp &lt;-t.test(theta_subj[,\"c_b\"],theta_subj[,\"c_w\"],\n       paired = T,\n       var.equal = T)\n\n# Ergebnis ausgeben\nprint(theta_a_comp)\n\n\n\n    Paired t-test\n\ndata:  theta_subj[, \"a_b\"] and theta_subj[, \"a_w\"]\nt = 3.1394, df = 136, p-value = 0.002077\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.0233750 0.1029489\nsample estimates:\nmean difference \n     0.06316194 \n\n\nCode\nprint(theta_c_comp)\n\n\n\n    Paired t-test\n\ndata:  theta_subj[, \"c_b\"] and theta_subj[, \"c_w\"]\nt = 2.2045, df = 136, p-value = 0.02917\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.002388014 0.044009140\nsample estimates:\nmean difference \n     0.02319858 \n\n\nWir sehen auf beiden Parametern einen signifikanten Unterschied ! Nun können wir mit dem Package effectsize Cohen's d berechnen, um die Größe des Effekts zu interpretieren. Dazu nutzen wir den Befehl cohens_d() und übergeben dem Befehl als Argument lediglich das Objekt theta_a_comp und theta_c_comp, in dem wir die Test gespeichert haben, sowie die Information das es sich um einen gepaarten t-test handelt:\n\n\n\n\nCode\n# verbose = F bedeutet nur, dass wir uns keine Warnungen etc. ausgeben lassen. \neffectsize::cohens_d(theta_a_comp,paired = T,verbose = F)\n\n\nd    |       95% CI\n-------------------\n0.27 | [0.10, 0.44]\n\n\n\n\nCode\neffectsize::cohens_d(theta_c_comp,paired = T,verbose = F)\n\n\nd    |       95% CI\n-------------------\n0.19 | [0.02, 0.36]\n\n\nWir sehen hier, das der Effekt auf den Parameter \\(a\\), welcher die automatisierte, stereotypische Verarbeitung abbildet, zwischen den Gruppen substanziell ist und einem kleinen bis mittleren Effekt entspricht. Für den \\(c\\) Parameter, welcher die kontrollierte Verarbeitung abbildet, ist dieser Effekt kleiner und entspricht einem kleinen Effekt nach Cohen.\nDies entspricht unseren Überlegungen und auch den Ergebnissen des Diffusionsmodells, dass es bei stereotyp-beladenen Entscheidungen in diesem Fall zum einen eine höhere kontrollierte Verarbeitung gibt (Personen wissen, dass es Stereotype gibt und versuchen sie zu vermeiden) und zum anderen, dass es jedoch häufiger passiert, das diese Prozesse scheitern und die automatisierte stereotype Verarbeitung einsetzt.\n\n\n\n\nMPTinR gibt uns auch eine Reihe von Indikatoren zur Bestimmung des Modelfits aus. Auf diese kann wie folgt zugegriffen werden\n\n\nCode\nmodel_fit &lt;- fit$goodness.of.fit$aggregated\n\nprint(model_fit)\n\n\n  Log.Likelihood G.Squared df p.value\n1      -3895.856         0  0       1\n\n\nDie für den Modelfit wichtige Statistik ist die G²-Statistik. G² wird in der Schätzung der MPT Modelle als Diskrepanzfunktion minimiert und bildet das Äquivalent zur Maximierung der Likelihood-Funktion bei kontinuierlichen Daten:\n\\[G^2(\\theta)=-2 \\sum_{i=1}^J n_j \\ln \\left(\\frac{n_j}{N \\cdot p_j(\\theta)}\\right)\\]\nEs wird hier die Abweichung zwischen beobachteten (\\(n_j\\)) und reproduzierten Häufigkeiten gegeben der geschätzten Parameter (\\(N \\cdot p_j(\\theta)\\)) berechnet. G² kann zur Berechnung des Modelfits herangezogen werden, als auch zum Modellvergleich. Es kann die Nullhypothese geprüft werden, ob die beobachteten Daten gegeben der geschätzten Wahrscheinlichkeiten plausibel sind, da G² \\(\\chi^2\\) verteilt mit den Freiheitsgraden\n\\[d f=\\sum_{k=1}^K\\left(J_k-1\\right)-S \\] ist. Hierbei ist \\(K\\) die Anzahl der unterschiedlichen Kategorien (hier 4, Hautfarbe x Objekt) und \\(J_k\\) die Möglichen Outcomes für die jeweilige Kategorie \\(k\\) (hier jeweils 2, Hits und Misses). \\(S\\) bezeichnet die Anzahl der Parameter die wir frei schätzen (hier 4). Laut Formel ergeben sich also folgende Freiheitsgrade\n\\[\\sum_{k=1}^4\\left(2-1\\right)-4 = 0 \\] Dies bedeutet, das unsere Modell perfekt identifiziert ist ! Ein Modell muss immer die Bedingung erfüllen, dass es mindestens soviele Freiheitsgerade wie Parameter besitzt:\n\\[S \\leq \\sum_{k=1}^K\\left(J_k-1\\right)\\] Ansonsten ist nicht identifiziert und kann nicht geschätzt werden. In unserem Fall ist das Modell also perfekt identifiziert, daher wird genaugenommen nichts geschätzt, sondern die Wahrscheinlichkeiten berechnet. Ist ein Modell nicht identifiziert, können Parameter fixiert oder gleichgesetzt werden, um die Anzahl der Freiheitsgerade zu erhöhen.\nIn unserem Fall ergibt sich also eine nicht-signifikante Teststatistik für G², mit einem p-Wert von 1 und einem G² von faktisch 0. Daher können wir die Nullhypothese, dass die Daten unter den geschätzten Wahrscheinlichkeiten plausibel sind, nicht ablehnen.\n\n\nCode\nmodel_fit\n\n\n  Log.Likelihood G.Squared df p.value\n1      -3895.856         0  0       1\n\n\nDies bedeutet das Modell fittet perfekt, was an der eindeutigen Identifizierung des Modells liegt, da\n\\[df = S\\] gilt.\n\n\n\nDer Modelfit kann auch grafisch dargestellt werden. Entweder mit der Funktion prediction.plot(), dieser stellt die Abweichung zwsichen beobachten und geschätzten Häufigkeiten dar. Ich habe hier ein limit von -1 eins bis 1 gesetzt, dar die Abweichengen in den Bäumen 3 und 4 so gering sind, das sie auf der 14 Potenz dargestellt wurden. Eine Abweichung von 0 bedeutet, es liegt ein perfekter Fit für diese Kategorien vor, in unserem Fall jeweils die Outcomes Hits (1) und Misses (2).\n\n\nCode\nMPTinR::prediction.plot(fit,model.filename = textConnection(pd),ylim = c(1,-1))\n\n\n\n\n\nEine weitere Möglichkeit den Modelfit grafisch darzustellen ist, die beobachteten und die durch das Model reproduzierten Daten in einem Scatterplot gegeneinander zu plotten und die Korrelation zu berechnen. Dies haben wir bereits in der Übung zu ggplot getan. MPTinR gibt uns praktischerweise direkt die beobachteten und reproduzierten Daten im fit-Objekt mit aus, sodass wir diese direkt in ggplot angeben können. Diese können folgendermaßen abgerufen werden:\n\n\nCode\nobserved&lt;- fit$data$observed$individual \n\npredicted &lt;-fit$data$predicted$individual \n\n\n\n\nCode\nggplot(mapping= aes(x=observed, y=predicted)) +\n  geom_jitter(width = 1,height = 1, color=\"red\", alpha=0.2) +  \n  geom_abline(slope = 1,intercept = 0) + \n  stat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") + \n  labs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") + \n  theme_bw() \n\n\n\n\n\nSchritt für Schritt:\n\nggplot(mapping = aes(x=observed, y=predicted)) Hier geben wir an, was auf der x und auf der y Achse geplottet werden soll\ngeom_jitter(width = 2,height = 2, color=\"red\", alpha=0.2) In diesem Fall nehme ich statt geom_point(), geom_jitter() da ansonsten die Punkte alle auf einer Linie liegen würden!\ngeom_abline(slope = 1,intercept = 0) Hier füge ich eine Linie ein, die den Ursprung 0 und die Steigung 1 hat - das entspricht dem theoretische perfekten Fit\nstat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") Dies ist aus dem Paket ggpubR das wir bereits in der ggplot Übung angesprochen hatten. stat_cor fügt die Korrelation zwischen den geplotteten Variablen ein. Dabei sind die label. Befehle die\nKoordinaten, wo genau die Korrelation stehen muss.\nlabs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") Einfügen der Achsenbeschriftung und des Titels\ntheme_bw() schöner Theme für den Plot.\n\nZum Verständnis, würden wir geom_point in diesem Fall verwenden, sähe der Plot folgendermaßen aus:\n\n\nCode\nggplot(mapping = aes(x=observed, y=predicted)) +\n  geom_point(color=\"red\", alpha=0.2) +  \n  geom_abline(slope = 1,intercept = 0) + \n  stat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") + \n  labs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") + \n  theme_bw()"
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#einführung",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#einführung",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Heute Beschäftigen wir uns mit der Anwendung von MPT Modellen in R. Hierzu nutzen wir einen Datensatz von Frenken et al. (2022), der sich mit der Verarbeitung von Stereotypen befasst. Zunächst müssen wir aber die entsprechenden Packages installieren. Wir nutzen das Package (MPTinR), das viele Funktionalitäten für das MPT Modelling mit sich bringt ! Zusätzlich installieren wir noch ggpubr und effectsize um später einige nette Features beim Plotten und Analysieren der Ergebnisse zu haben:\n\n\nCode\n# Install MPT Package\n# install.packages(\"MPTinR\")\n# install.packages(\"ggpubr\")\n# install.packages(\"effectsize\")\n\n\nNun laden wir die Pakete die wir für den weiteren Verlauf brauchen werden sowie unseren Datensatz ein:\n\n\nCode\nlibrary(MPTinR)\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(effectsize)\n\n\nStudy_2_dm &lt;- read_csv(\"Data/Study_2_dm.csv\", \n                       col_types = cols(...1 = col_skip(), \n                                        stimulus = col_factor(levels = c(\"gun\",\"phone\")), \n                                        response = col_integer(), \n                                        condition = col_factor(levels = c(\"black\",\"white\"))))"
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#der-shooter---bias",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#der-shooter---bias",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Wie schon in der letzten Sitzung vorgestellt handelt es sich beim beim First Person Shooter Task (FPST) um ein Paradigma, mit dem Stereotype und deren Auswirkungen auf Entscheidungen untersucht werden können. Hierbei werden üblicherweise verschiedene Ethnien (z.B. schwarze oder weiße Personen), entweder mit einer Waffe (threat) oder einem ungefährlichem Objekt (z.B. Telefon, harmless) gezeigt. Die Versuchspersonen werden instruiert unabhängig von der Hautdfarbe so schnell und korrekt wie möglich auf bewaffnete Ziele zu schiessen. Im Gegensatz dazu soll nicht auf unbewaffnete Ziele geschossen werden.\n\n\n\nVerschiedene Paradigmen des FPST"
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#model-definition",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#model-definition",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Wir werden heute versuchen, die Daten von Frenken et al. mit dem Process Dissociation Model zu fitten. Dieses müssen wir aber zunächst in R definieren. Die Modellgleichungen leiten sich aus folgendem zugrundeliegendem Wahrscheinlichkeitsbaum ab:\n\n\n\nProcess Dissociation Model (Payne,2001)\n\n\nDas Modell hat nur zwei freie Parameter, zum einen c , der die Wahrscheinlichkeit angibt das kontrollierte Verarbeitungsprozesse aktiviert werden und zum anderen a der die Wahrscheinlichkeit angibt, dass diese Kontrolle fehlschlägt und eine stereotype Verarbeitung aktiviert wird.\n\nZur Erinnerung: das experimentelle Paradigma in Payne et al. ist ein Sequential Priming Task, bei dem die Versuchspersonen ein schwarzes oder weißes Gesicht gesehen haben und dann entscheiden sollten, ob es sich bei dem anschließend gezeigten Objekt um einen Waffe oder ein Werkzeug handelt:\n\n\nIn unserem Fall ist das Paradigma also ähnlich, die Versuchspersonen sehen entweder einen Schwarzen oder einen Weißen (unbewaffnet vs. bewaffnet) und sollen dann entscheiden, ob geschossen werden soll oder nicht. Das heisst, wir können das Modell im Prinzip ekaxt so wie es ist, auf die Daten von Frenken et al. übertragen.\n\nNun müssen wir das Modell allerdings definieren. Hierzu müssen die Gleichungen für jeden Wahrscheinlichkeitsbaum aufgestellt werden. Jeder Baum repräsentiert eine Zielkategorie:\n\n\n\n\nHautfarbe\nObjekt\n\n\n\n\nSchwarz\nWaffe\n\n\nWeiß\nWaffe\n\n\nSchwarz\nTelefon\n\n\nWeiß\nTelefon\n\n\n\nEs müssen nun für jede dieser Outcome Kategorien Gleichungen für die unterschiedlichen Pfade definiert werden, welche zu den jeweiligen “Hits” und “Misses” führen (“+” bzw. “-”auf der Abbildung). Zusätzlichmüssen insgesamt 4 Parameter definiert werden. A und C jeweils für schwarze und weiße Hautfarbe, um Unterschiede in den Parametern, die auf die Hautfarbe zurückgehen, identifizieren zu können:\n\n\nFür MPTinR können die Gleichungen in einem einfachen “String” definiert werden. Hier ist die erste Gleichung für die Kategorie “White/ Phone” bereits eingetragen. In der Kategorie “White/Phone” führt der Pfad \\[c\\] und der Pfad \\[(1-c) \\cdot a\\] zu einem “Hit”.\nIm Gegensatz dazu führt \\[(1-c) \\cdot (1-a)\\] zu einem Miss. Tragt immer zunüchst die Gleichung für einen Hit und dann die Gleichungen für einen Miss ein. Da zwischen Pfaden die Wahrscheinlichkeiten addiert werden können, ist also die Gesamtgleichung eines “Hits” für die Kategorie “White/Phone”:\n\\[c + (1-c) \\cdot a\\] und für einen Miss\n\\[ (1-c) \\cdot (1-a)\\]\nda es nur einen Pfad zu einem “Miss” gibt ! Vervollständigt nun die Gleichungen für die restlichen Kategorien. Denkt daran das Ihr die Parameter für die schwarze und weiße Hautfarbe unterschiedlich benennt!\n\n\n\nCode\n# Parameters for Black and White Skin Color! \n# c_w = c white, a_w = a white c_b = black, a_b = a black\n\n\npd &lt;- \"\n# WT\nc_w+(1-c_w)*a_w\n(1-c_w)*(1-a_w)\n\n# WG\nc_w + (1-c_w)*(1-a_w)\n(1-c_w) * a_w\n\n# BT\nc_b + (1-c_b)*(1-a_b)\n(1-c_b) * a_b\n\n# BG\nc_b+(1-c_b)*a_b\n(1-c_b)*(1-a_b)\n\"\n\n\n\nWenn ihr die Gleichungen alle definiert habt, könnt ihr das Modell mit MPTinR auf Korrektheit überprüfen lassen:\n\n\n\nCode\ncheck.mpt(textConnection(pd))\n\n\n$probabilities.eq.1\n[1] TRUE\n\n$n.trees\n[1] 4\n\n$n.model.categories\n[1] 8\n\n$n.independent.categories\n[1] 4\n\n$n.params\n[1] 4\n\n$parameters\n[1] \"a_b\" \"a_w\" \"c_b\" \"c_w\"\n\n\nWir sehen hier einen Test ob alle Wahrscheinlichkeiten sich zu 1 addieren, die Anzahl der Bäume, der gesamten und unabhängigen Kategorien, sowie der Parameter. In den Daten müssen immer exakt so viele Kategorien vorhanden sein, wie im Modell definiert ! Zusätzlich darf die Anzahl Parameter nicht größer sein, als die Anzahl der unabhängigen Kategorien - ansonsten ist das Modell nicht identifizierbar - das beudeut das mehr unbekannte als bekannte Größen vorhanden sind - die Gleichungen sind nicht lösbar."
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#data-dredging",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#data-dredging",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Um die Daten von Frenken et al. zu Modellieren, müssen die Daten zunächst in ein anderes Format gebracht und umkodiert werden - in “Hit” und “Miss” für jede Kategorie. Dies kann wie folgt mit mutate(),case_when() und pivot_wider() getan werden:\n\n\nCode\n# Define Edge Correction function\nedge_correct &lt;- function(x){\n  x = ifelse(x==0, x+1,x)\n  return(x)\n}\n\n# Recode Data from Frenken et al.\n# Umkodieren der Daten hin zu Accuracy \n\n# In den Daten sind die Antworten nicht nach Accuracy, sondern nach response codiert (response coding). Daher müssen wir die Daten vorher umkodieren, damit die Accuracy codiert wird. Dies können wir mit case_when tun. Kodiert nun den Datensatz wie folgt neu in dem ihr einen neue Spalte \"ACC\" erzeugt, die in Abhängigkeit von \"stimulus\" erzeugt wird:\n\n# Wenn Stimulus = gun ist, dann ist response 0 korrekt - also 1 \n# Wenn Stimulus = gun ist, dann ist response 1 inkorrekt - also 0\n\n# Bei Phone stimmt die zuordnung zufällig, da 1 bedeutet nicht zu schiessen. Ist aber nicht immer der Fall ! Trotzdem müssen wir die Zuordnung eintragen, damit für alle elemente in der Spalte ACC ein Wert steht!\n\nfreq_dat &lt;- Study_2_dm %&gt;% mutate(ACC= case_when(stimulus == \"gun\" & response == 0 ~ 1,\n                                                 stimulus == \"gun\" & response ==1 ~0,\n                                                 stimulus == \"phone\" & response ==1 ~ 1,\n                                                 stimulus == \"phone\" & response == 0 ~ 0))\n\n\n# Berechnung der hits und misses und Auszählen der Trials jeder VP den jeweiligen Bedingungen \nfreq_dat &lt;- freq_dat %&gt;% group_by(subj_idx,stim) %&gt;% summarise(hits = sum(ACC), ntrials=n(), miss=ntrials-hits)\n\n\n`summarise()` has grouped output by 'subj_idx'. You can override using the\n`.groups` argument.\n\n\nCode\n# Transformieren zum Wide Format für MPTinR\nfreq_dat &lt;- freq_dat %&gt;% pivot_wider(.,names_from = c(\"stim\"),values_from = c(\"hits\",\"miss\"),values_fill = F,id_cols=\"subj_idx\") \n\n# Anpassung der Reihenfolge der Daten analog zu den Modellgleichungen.\nfreq_dat &lt;- freq_dat %&gt;% relocate(subj_idx,hits_wp,miss_wp,hits_wg,miss_wg,hits_bp,miss_bp,hits_bg,miss_bg) \n\n# Anwenden einer Edge-Correction, um die Nullbeobactungen in der \"miss\" Kategorie zu eleminieren\nfreq_dat &lt;- freq_dat %&gt;% mutate(across(starts_with(\"miss\"), ~ edge_correct(.)))\n\n\nhead(freq_dat)\n\n\n# A tibble: 6 × 9\n# Groups:   subj_idx [6]\n  subj_idx hits_wp miss_wp hits_wg miss_wg hits_bp miss_bp hits_bg miss_bg\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1        0      28       5      23       4      28       1      30       1\n2        1      11       1      27       2      16       2      28       2\n3        3      23       1      33       1      10       1      25       1\n4        4      33       1      27       1      24       1      35       1\n5        5      23       4      25       4      28       2      31       1\n6        6      31       1      18       5      26       1      34       1"
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#fit-the-model",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#fit-the-model",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Zum fitten des Modells müssen die Daten für MPTinR in der gleichen Reihenfolge angeordnet sein, wie auch die Gleichungen definiert sind! Dies solltet ihr immer überprüfen, bevor ein Modell gefittet wird.\nNun kann das Modell mit dem Befehl fit.mpt geschätzt werden. Dabei müssen wir die genauen Spalten unseres Dataframes freq_dat angeben, da eine Spalte subj_idx im Dataframe enthalten ist, die keine Häufigkeiten enthält. Die fit.mpt Funktion nimmt allerdings nur Daten an, die die gleiche Anzahl an Spalten aufweisen, wie im Modell definierten gesamten Kategorien vorhanden sind. Andernfalls wird eine Fehlermeldung ausgegeben. Da im Dataframe insgesamt 9 Spalten vorhanden sind, muss also die erste Spalte ausgelassen werden. Dies kann einfach mit der Auswahl der Spalten mit dem Zugriff [Zeilen,Spalten] geschehen. Als model.filename wird der String übergeben in dem die Gleichungen definiert wurden. Dazu wird erneut textConnection() verwendet. Das Argument n.optim definiert die Anzahl der Optimierungsdurchläufe, wird dieses Argument nicht abgegeben, werden standardmäßig 5 Optimierungsdurchläufe durchgeführt.\n\n\nCode\n# Fit Process Disassociaton Model \nfit &lt;- fit.mpt(freq_dat[,2:9],model.filename = textConnection(pd),n.optim = 10)\n\n\nPresenting the best result out of 10 minimization runs.\n\n\n[1] \"Model fitting begins at 2023-06-30 14:10:53.815274\"\n[1] \"Model fitting stopped at 2023-06-30 14:11:00.164152\"\nTime difference of 6.348878 secs\n\n\nNun können verschiedene Informationen mit Hilfe des $ Operator aus dem fit Objekt ausgegeben werden, hierzu benutzen wir den $ Operator. Zunächst sehen wir uns den Mittelwert der unterschiedlichen individuellen Parameter nach Gruppen/Bäumen an, sowie die Parameter für die aggregierten Daten (also für die aufsummierten Hits und Misses über alle Personen):\n\n\n\nCode\n# Speichern der Parameter in einen neuen Dataframe\n\ntheta_mean &lt;- fit$parameters$mean[1:4,1]\ntheta_aggregated &lt;- fit$parameters$aggregated\n\nprint(theta_aggregated)\n\n\n    estimates lower.conf upper.conf\na_b 0.5624551  0.5191306  0.6057796\na_w 0.5048275  0.4670972  0.5425577\nc_b 0.8738353  0.8626938  0.8849767\nc_w 0.8388067  0.8266458  0.8509677\n\n\nWir sehen das es wohl einen signifikanten Unterschied zwischen der Hautfarbe bei dem Control Parameter c auf aggregierter Ebene geben könnte. Auf dem Parameter a, welcher automatisierte, von Stereotypen getriebenen Prozesse abbildet, zeigt sich hingegen eine Überlappung der Konfidenzintervalle. Um zu prüfen ob es einen signifikanten Unterschied auf individuellem Level gibt, müssen wir die individuellen Parameter der einzelnen Versuchspersonen heranziehen. Zünachst müssen diese aus dem fit Objekt in einen Dataframe überführt werden:\n\n\nCode\n# Erstellen einer Matrix für die Parameter\ntheta_subj &lt;- matrix(NA,nrow=nrow(freq_dat),ncol=4)\ncolnames(theta_subj) &lt;- c(\"a_b\",\"a_w\",\"c_b\",\"c_w\")\n\n# Loopen über alle individuellen Parameter, nur die Punktschätzer werden gespeichert\nfor (i in 1:nrow(freq_dat)){\n  theta_subj[i,] &lt;- fit$parameters$individual[,1,i]\n}\n\n# Nun lassen wir uns die ersten 10 Zeilen ausgeben, um uns das Ergebnis anzusehen\nhead(theta_subj,10)\n\n\n            a_b       a_w       c_b       c_w\n [1,] 0.5166667 0.4943820 0.9332592 0.7003367\n [2,] 0.6250000 0.4528302 0.8222222 0.8477011\n [3,] 0.7027027 0.4137931 0.8706294 0.9289216\n [4,] 0.5901639 0.5483871 0.9322222 0.9348739\n [5,] 0.6808511 0.4821429 0.9020833 0.7139208\n [6,] 0.5645161 0.8743169 0.9343915 0.7513587\n [7,] 0.3827160 0.6271186 0.8954839 0.9275184\n [8,] 0.5974026 0.8108108 0.8920056 0.6476190\n [9,] 0.3846154 0.6571429 0.8916667 0.8731884\n[10,] 0.1724138 0.2571429 0.8388889 0.8504274"
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#analyse",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#analyse",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Jetzt können wir mit Hilfe eines paired t-tests und der t.test() Funktion (wir vergleichen die Parameter innerhalb von Personen zwischen den experimentellen Bedinungen) analysieren, ob es einen signifikanten Unterschied auf individueller ebene gibt. Die t-test() Funktion nimmt als Argument jeweils die zwei Spalten, die verglichen werden sollen. Wir müssen also für die beiden Parameter \\(a\\) und \\(c\\) jeweils einen t-test rechnen, der beide Gruppen vergleicht. Wir setzen zusätzlich das Argument paired=TRUE, da es sich um einen paired t-test handelt und das Argument var.equal=T, da wir annehmen das die Varianz in beiden Gruppen gleich ist:\n\n\nCode\n# t-test für beide Parameter \ntheta_a_comp &lt;-t.test(theta_subj[,\"a_b\"],theta_subj[,\"a_w\"],\n       paired = T,\n       var.equal = T)\n\ntheta_c_comp &lt;-t.test(theta_subj[,\"c_b\"],theta_subj[,\"c_w\"],\n       paired = T,\n       var.equal = T)\n\n# Ergebnis ausgeben\nprint(theta_a_comp)\n\n\n\n    Paired t-test\n\ndata:  theta_subj[, \"a_b\"] and theta_subj[, \"a_w\"]\nt = 3.1394, df = 136, p-value = 0.002077\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.0233750 0.1029489\nsample estimates:\nmean difference \n     0.06316194 \n\n\nCode\nprint(theta_c_comp)\n\n\n\n    Paired t-test\n\ndata:  theta_subj[, \"c_b\"] and theta_subj[, \"c_w\"]\nt = 2.2045, df = 136, p-value = 0.02917\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.002388014 0.044009140\nsample estimates:\nmean difference \n     0.02319858 \n\n\nWir sehen auf beiden Parametern einen signifikanten Unterschied ! Nun können wir mit dem Package effectsize Cohen's d berechnen, um die Größe des Effekts zu interpretieren. Dazu nutzen wir den Befehl cohens_d() und übergeben dem Befehl als Argument lediglich das Objekt theta_a_comp und theta_c_comp, in dem wir die Test gespeichert haben, sowie die Information das es sich um einen gepaarten t-test handelt:\n\n\n\n\nCode\n# verbose = F bedeutet nur, dass wir uns keine Warnungen etc. ausgeben lassen. \neffectsize::cohens_d(theta_a_comp,paired = T,verbose = F)\n\n\nd    |       95% CI\n-------------------\n0.27 | [0.10, 0.44]\n\n\n\n\nCode\neffectsize::cohens_d(theta_c_comp,paired = T,verbose = F)\n\n\nd    |       95% CI\n-------------------\n0.19 | [0.02, 0.36]\n\n\nWir sehen hier, das der Effekt auf den Parameter \\(a\\), welcher die automatisierte, stereotypische Verarbeitung abbildet, zwischen den Gruppen substanziell ist und einem kleinen bis mittleren Effekt entspricht. Für den \\(c\\) Parameter, welcher die kontrollierte Verarbeitung abbildet, ist dieser Effekt kleiner und entspricht einem kleinen Effekt nach Cohen.\nDies entspricht unseren Überlegungen und auch den Ergebnissen des Diffusionsmodells, dass es bei stereotyp-beladenen Entscheidungen in diesem Fall zum einen eine höhere kontrollierte Verarbeitung gibt (Personen wissen, dass es Stereotype gibt und versuchen sie zu vermeiden) und zum anderen, dass es jedoch häufiger passiert, das diese Prozesse scheitern und die automatisierte stereotype Verarbeitung einsetzt."
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#modelfit-und-identifizierbarkeit",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#modelfit-und-identifizierbarkeit",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "MPTinR gibt uns auch eine Reihe von Indikatoren zur Bestimmung des Modelfits aus. Auf diese kann wie folgt zugegriffen werden\n\n\nCode\nmodel_fit &lt;- fit$goodness.of.fit$aggregated\n\nprint(model_fit)\n\n\n  Log.Likelihood G.Squared df p.value\n1      -3895.856         0  0       1\n\n\nDie für den Modelfit wichtige Statistik ist die G²-Statistik. G² wird in der Schätzung der MPT Modelle als Diskrepanzfunktion minimiert und bildet das Äquivalent zur Maximierung der Likelihood-Funktion bei kontinuierlichen Daten:\n\\[G^2(\\theta)=-2 \\sum_{i=1}^J n_j \\ln \\left(\\frac{n_j}{N \\cdot p_j(\\theta)}\\right)\\]\nEs wird hier die Abweichung zwischen beobachteten (\\(n_j\\)) und reproduzierten Häufigkeiten gegeben der geschätzten Parameter (\\(N \\cdot p_j(\\theta)\\)) berechnet. G² kann zur Berechnung des Modelfits herangezogen werden, als auch zum Modellvergleich. Es kann die Nullhypothese geprüft werden, ob die beobachteten Daten gegeben der geschätzten Wahrscheinlichkeiten plausibel sind, da G² \\(\\chi^2\\) verteilt mit den Freiheitsgraden\n\\[d f=\\sum_{k=1}^K\\left(J_k-1\\right)-S \\] ist. Hierbei ist \\(K\\) die Anzahl der unterschiedlichen Kategorien (hier 4, Hautfarbe x Objekt) und \\(J_k\\) die Möglichen Outcomes für die jeweilige Kategorie \\(k\\) (hier jeweils 2, Hits und Misses). \\(S\\) bezeichnet die Anzahl der Parameter die wir frei schätzen (hier 4). Laut Formel ergeben sich also folgende Freiheitsgrade\n\\[\\sum_{k=1}^4\\left(2-1\\right)-4 = 0 \\] Dies bedeutet, das unsere Modell perfekt identifiziert ist ! Ein Modell muss immer die Bedingung erfüllen, dass es mindestens soviele Freiheitsgerade wie Parameter besitzt:\n\\[S \\leq \\sum_{k=1}^K\\left(J_k-1\\right)\\] Ansonsten ist nicht identifiziert und kann nicht geschätzt werden. In unserem Fall ist das Modell also perfekt identifiziert, daher wird genaugenommen nichts geschätzt, sondern die Wahrscheinlichkeiten berechnet. Ist ein Modell nicht identifiziert, können Parameter fixiert oder gleichgesetzt werden, um die Anzahl der Freiheitsgerade zu erhöhen.\nIn unserem Fall ergibt sich also eine nicht-signifikante Teststatistik für G², mit einem p-Wert von 1 und einem G² von faktisch 0. Daher können wir die Nullhypothese, dass die Daten unter den geschätzten Wahrscheinlichkeiten plausibel sind, nicht ablehnen.\n\n\nCode\nmodel_fit\n\n\n  Log.Likelihood G.Squared df p.value\n1      -3895.856         0  0       1\n\n\nDies bedeutet das Modell fittet perfekt, was an der eindeutigen Identifizierung des Modells liegt, da\n\\[df = S\\] gilt."
  },
  {
    "objectID": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#grafische-darstellung-des-modelfits",
    "href": "cogmod_sem_sose23/MPT-Workshop/MPT-Workshop.html#grafische-darstellung-des-modelfits",
    "title": "Workshop Multinomial Processing Tree Models",
    "section": "",
    "text": "Der Modelfit kann auch grafisch dargestellt werden. Entweder mit der Funktion prediction.plot(), dieser stellt die Abweichung zwsichen beobachten und geschätzten Häufigkeiten dar. Ich habe hier ein limit von -1 eins bis 1 gesetzt, dar die Abweichengen in den Bäumen 3 und 4 so gering sind, das sie auf der 14 Potenz dargestellt wurden. Eine Abweichung von 0 bedeutet, es liegt ein perfekter Fit für diese Kategorien vor, in unserem Fall jeweils die Outcomes Hits (1) und Misses (2).\n\n\nCode\nMPTinR::prediction.plot(fit,model.filename = textConnection(pd),ylim = c(1,-1))\n\n\n\n\n\nEine weitere Möglichkeit den Modelfit grafisch darzustellen ist, die beobachteten und die durch das Model reproduzierten Daten in einem Scatterplot gegeneinander zu plotten und die Korrelation zu berechnen. Dies haben wir bereits in der Übung zu ggplot getan. MPTinR gibt uns praktischerweise direkt die beobachteten und reproduzierten Daten im fit-Objekt mit aus, sodass wir diese direkt in ggplot angeben können. Diese können folgendermaßen abgerufen werden:\n\n\nCode\nobserved&lt;- fit$data$observed$individual \n\npredicted &lt;-fit$data$predicted$individual \n\n\n\n\nCode\nggplot(mapping= aes(x=observed, y=predicted)) +\n  geom_jitter(width = 1,height = 1, color=\"red\", alpha=0.2) +  \n  geom_abline(slope = 1,intercept = 0) + \n  stat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") + \n  labs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") + \n  theme_bw() \n\n\n\n\n\nSchritt für Schritt:\n\nggplot(mapping = aes(x=observed, y=predicted)) Hier geben wir an, was auf der x und auf der y Achse geplottet werden soll\ngeom_jitter(width = 2,height = 2, color=\"red\", alpha=0.2) In diesem Fall nehme ich statt geom_point(), geom_jitter() da ansonsten die Punkte alle auf einer Linie liegen würden!\ngeom_abline(slope = 1,intercept = 0) Hier füge ich eine Linie ein, die den Ursprung 0 und die Steigung 1 hat - das entspricht dem theoretische perfekten Fit\nstat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") Dies ist aus dem Paket ggpubR das wir bereits in der ggplot Übung angesprochen hatten. stat_cor fügt die Korrelation zwischen den geplotteten Variablen ein. Dabei sind die label. Befehle die\nKoordinaten, wo genau die Korrelation stehen muss.\nlabs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") Einfügen der Achsenbeschriftung und des Titels\ntheme_bw() schöner Theme für den Plot.\n\nZum Verständnis, würden wir geom_point in diesem Fall verwenden, sähe der Plot folgendermaßen aus:\n\n\nCode\nggplot(mapping = aes(x=observed, y=predicted)) +\n  geom_point(color=\"red\", alpha=0.2) +  \n  geom_abline(slope = 1,intercept = 0) + \n  stat_cor(method = \"pearson\", label.x = 3, label.y = 30,cor.coef.name = \"r\") + \n  labs(y=\"Observed Data\", x=\"Predicted Data\",title = \"Fit of Process Disassociation Model\") + \n  theme_bw()"
  },
  {
    "objectID": "cogmod_sem_sose23/ML_Workshop.html",
    "href": "cogmod_sem_sose23/ML_Workshop.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "",
    "text": "Im letzten Seminar haben wir sehr ausführlich über Maximum Likelihood Estimation (MLE) gesprochen. Heute werden wir einige Übungen dazu in R programmieren, um ein besseres Verständnis für diese Methode zu entwickeln.\nDas Grundprinzip der Maximum-Likelihood-Schätzung besteht darin, die Parameter einer statistischen Verteilung so zu bestimmen, dass die Wahrscheinlichkeit, die beobachteten Daten gegeben bestimmter Parameterwerte, maximiert wird. Gegeben eine Verteilungsfunktion \\(f(x;\\theta)\\), wobei \\(x\\) die beobachteten Daten und \\(\\theta\\) die unbekannten Parameter sind, wird die Likelihood-Funktion definiert als \\[L(\\theta|x)=\\prod_{i=1}^{n} f(x_i;\\theta)\\], wobei \\(n\\) die Anzahl der Datenpunkte ist.\nDas Maximum-Likelihood-Schätzverfahren besteht darin, die Werte von \\(\\theta\\) zu finden, die die Likelihood-Funktion maximieren. Dies kann durch Maximierung des Logarithmus der Likelihood-Funktion mathematisch vereinfacht werden, daher wird oftmals die Log-Likelihood Funktion maximiert und anstelle des Produktes, die Summe über alle Funktionswerte gebildet:\n\\[\\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log f(x_i;\\theta)\\]\n\n\nNehmen wir an, wir haben an einer Sttichprobe die Intelligenzẃerte erhoben und möchten nun den Mittelwert des IQs anhand der Daten mit MLE schätzen. Hierzu brauchen wir zunächst eine Dichtefunktion, über die wir die Likelihood berechnen können. Da der IQ in der Population normalverteilt ist können wir hierfür die Normaverteilung heranziehen um eine Likelihoodfunktion zu definieren:\n\\[f(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\nalso ist die Likelihoodfunktion gegeben als\n\\[L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\n\\] bzw. vereinfacht sich zu folgender Formel, wenn wir den Logarhitmus nehmen:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]"
  },
  {
    "objectID": "cogmod_sem_sose23/ML_Workshop.html#einführung",
    "href": "cogmod_sem_sose23/ML_Workshop.html#einführung",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "",
    "text": "Im letzten Seminar haben wir sehr ausführlich über Maximum Likelihood Estimation (MLE) gesprochen. Heute werden wir einige Übungen dazu in R programmieren, um ein besseres Verständnis für diese Methode zu entwickeln.\nDas Grundprinzip der Maximum-Likelihood-Schätzung besteht darin, die Parameter einer statistischen Verteilung so zu bestimmen, dass die Wahrscheinlichkeit, die beobachteten Daten gegeben bestimmter Parameterwerte, maximiert wird. Gegeben eine Verteilungsfunktion \\(f(x;\\theta)\\), wobei \\(x\\) die beobachteten Daten und \\(\\theta\\) die unbekannten Parameter sind, wird die Likelihood-Funktion definiert als \\[L(\\theta|x)=\\prod_{i=1}^{n} f(x_i;\\theta)\\], wobei \\(n\\) die Anzahl der Datenpunkte ist.\nDas Maximum-Likelihood-Schätzverfahren besteht darin, die Werte von \\(\\theta\\) zu finden, die die Likelihood-Funktion maximieren. Dies kann durch Maximierung des Logarithmus der Likelihood-Funktion mathematisch vereinfacht werden, daher wird oftmals die Log-Likelihood Funktion maximiert und anstelle des Produktes, die Summe über alle Funktionswerte gebildet:\n\\[\\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log f(x_i;\\theta)\\]\n\n\nNehmen wir an, wir haben an einer Sttichprobe die Intelligenzẃerte erhoben und möchten nun den Mittelwert des IQs anhand der Daten mit MLE schätzen. Hierzu brauchen wir zunächst eine Dichtefunktion, über die wir die Likelihood berechnen können. Da der IQ in der Population normalverteilt ist können wir hierfür die Normaverteilung heranziehen um eine Likelihoodfunktion zu definieren:\n\\[f(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\nalso ist die Likelihoodfunktion gegeben als\n\\[L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\n\\] bzw. vereinfacht sich zu folgender Formel, wenn wir den Logarhitmus nehmen:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]"
  },
  {
    "objectID": "cogmod_sem_sose23/ML_Workshop.html#mle-in-r",
    "href": "cogmod_sem_sose23/ML_Workshop.html#mle-in-r",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "MLE in R",
    "text": "MLE in R\n\nÜbung 1: MLE by Hand in R\nUm in R mit der Likelihoodfunktion zu arbeiten, müssen wir zunächst Daten simulieren. Hierzu benutzen wir die Funktion rnorm(). Bitte nutzt zunächste die Hilfefunktion, um mit rnorm() eine Stichproben von 10000 Werten zu genieren (N = 100), mit dem Mittelwert \\(100\\) und einer Standardabweichung von \\(15\\). Speichert den Output in der Variable “iq” ab. Berechnet für die gezogene Stichprobe separat Mittelwert und Standardabweichung\n\nset.seed(666)\n# Stichprobe von Werten generieren\niq &lt;- rnorm(100,100,15)\n\n# Mittelwert und Standardabweichung berechnen\n\nmean(iq)\n\n[1] 98.99997\n\nsd(iq)\n\n[1] 15.44065\n\n\nZunächst wollen wir versuchen, die Likelihoodfunktion in R Code zu übertragen. Nehmt hierzu die Gleichung der Log-Likelihoodfunktion, die wir weiter oben definiert haben:\n\\[ \\ln[L(\\mu,\\sigma)] =  -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]\nund drückt sie in R-Code aus. Ihr benötigt dazu folgende mathematischen Funktionen:\n\n\n\nFunktion\nCode\n\n\n\n\nSumme Bilden\nsum()\n\n\nQuadrieren\nx^2\n\n\nLogarhitmus\nlog()\n\n\n\\(\\pi\\)\nPi\n\n\nn\nStichprobengröße (hier: 100)\n\n\n\nDefiniert zunächst zwei Variablen für unterschiedliche Parametervorschläge für den Mittelwert von \\(100\\) (=low_iq) und \\(120\\) (high_iq),die Standardabweichung (=sigma) ist für beide Samples gleich (\\(\\sigma\\) = 15). Als Daten nutzen wir die generierten IQ Werte iq_low und iq_high. In der Gleichung sind \\(x\\) die Daten, also die IQ-Werte aus unserem iq Vektor, \\(\\sigma\\) ist die Standardabweichung und \\(\\mu\\) die unterschiedlichen Vorschläge für die Mittelwerte, als entweder high_iq oder low_iq. Stellt nun die Gleichung für beide Parametervorschläge (high vs. low) in R auf. Speichert die Ergebnisse unter den Variablen ll_low und ll_high ab.\n\n# Define sigma and mu\nhigh_iq &lt;- 120\nlow_iq &lt;- 100\nN &lt;- 100\n\nsigma &lt;- 15\n\n\n# Define LL-Equation\nll_low &lt;- -N/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((iq - low_iq)^2)\nll_high &lt;- -N/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((iq - high_iq)^2)\n\nIhr habt nun für beide Parametervorschläge, also einmal für einen Mittelwert von 100 und einmal von einem Mittelwert von 120, die log-likelihood für die vorliegenden Daten berechnet. Welcher Mittelwert ist nach dieser Likelihood unter den gegebenen Daten wahrscheinlicher ?\n\n\nExkurs: Funktionen in R\nFunktionen sind in der Programmierung wichtige Bausteine, um Code wiederverwendbar zu machen und komplexe Aufgaben zu strukturieren. Eine nimmt in der Regel Eingabewerte (Argumente) entgegen, führt Operationen oder Berechnungen durch und gibt ein Ergebnis zurück. In R können Funktionen mit dem Schlüsselwort function definiert werden. Die Syntax besteht aus dem Funktionsnamen, den Eingabe-Parametern in Klammern, den auszuführenden Anweisungen innerhalb des Funktionskörpers und dem Rückgabewert mit return(). Funktionen in R können dann mit den angegebenen Argumenten aufgerufen werden, um den gewünschten Code auszuführen. Die Verwendung von Funktionen erleichtert das Schreiben, Lesen und Verstehen von Code, da Aufgaben in kleine, wiederverwendbare Einheiten aufgeteilt werden können.\nHier ein einfaches Beispiel einer Funktion, die die Quadratzahl des Inputarguments ausgibt:\n\n# Funktion zur Berechnung der Quadratzahl\nsquare &lt;- function(x) {\n  result &lt;- x^2\n  return(result)\n}\n\n# Verwendung der Funktion\nnum &lt;- 5\nsquared_num &lt;- square(num)\nprint(squared_num)\n\n[1] 25\n\n\nInnerhalb einer Funktion in R kann auf die Input-Argumente zugegriffen werden, indem man ihre Namen verwendet. Die Input-Argumente werden in der Funktion als Parameter definiert. Du kannst diese Parameter dann innerhalb der Funktion verwenden, um auf die übergebenen Werte zuzugreifen und damit Berechnungen oder Operationen durchzuführen.\nZum Beispiel, wenn wir eine Funktion addition() definieren möchten, die zwei Zahlen addiert, könnten wir die Input-Argumente a und b verwenden:\n\naddition &lt;- function(a, b) {\n  sum &lt;- a + b\n  return(sum)\n}\n\naddition(2,2)\n\n[1] 4\n\n\n\n\nÜbung 2: MLE Using optim()\n\nDiskrepanzfunktion definieren\nNun haben wir im Prinzip “by Hand” eine MLE Schätzung durchgeführt - zwar nicht iterativ, denn haben wir haben nur zwei mögliche Parameterwerte im Lichte der gegebenen Daten nach der MLE bewertet! R bietet aber auch die Möglichkeit, mit der Funktion optim(), eine SIMPLEX Optimierung nach einer gegebenen Diskrepanzfunktion durchzuführen. Dazu müssen wir der Funktion allerdings eine Funktion übergeben.\nUm nun die optim() zu nutzen im iterativ Parameter nach MLE zu schätzen und durch simplex zu minimieren, müssen wir die gleiche log-likelihood Funktion von Übung 1 in eine Funktion überführen. Das ist ganz einfach, denn wir können uns nun, da wir das Grundprinzip von MLE verstanden haben, das Leben mit der in R verfügbaren dnorm() Funktion erleichtern. Diese berechnet ebenfalls die Wahrscheinlichkeit (genauer die Dichte) für Datenpunkte, gegeben bestimmter Parameter:\n\n# Unsere Gleichung\nll_low &lt;- -100/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((iq - low_iq)^2) \nprint(ll_low)\n\n[1] -415.3721\n\n# dnorm() aus R\nll_dnorm &lt;- sum(dnorm(iq,mean=low_iq,sd=sigma,T))\nprint(ll_dnorm)\n\n[1] -415.3721\n\n\nEure Aufgabe ist es nun, eine Funktion zu definieren, die sie aufsummierte log-likelihood ausgibt, wenn Ihr Parameterwerte eingebt. Dazu nutzt ihr die folgende Funktionen aus R:\n\n\n\nFunktion\nCode\n\n\n\n\nSumme Bilden\nsum()\n\n\nLikelihood\ndnorm(daten,mean=,sd=, log=TRUE)\n\n\nLogarhitmus\nlog()\n\n\n\nDie Funktion soll folgende Input-Argumente besitzen:\n\nEinen Vektor Daten (unsere IQ Daten sind ein Vektor)\nEinen Vektor theta, der nur zwei Einträge enthält - theta ist der Argumentname, um innerhalb der Funktion auf den Input zuzugreifen. Was ihr letztendlich der Funktion als theta übergebt, muss nicht theta heißen !\n\nWeiterhin soll diese Funktion die aufsummierte log-Likelihood ausgeben. Definiert diese Funktion unter dem Namen MLE. Die Inputargumente müssen wie im oberen Beispiel einer einfachen Funktion nur mit Namen definiert werden, nicht mit Datentyp. Diesen habe ich nur zum Verständnis mit angegeben.\n2.) Müssen folgende Operationen innerhalb der Funktion ausgeführt werden\n\nBerechnung der Likelihood unter der verwendung von dnorm(). b.) Aufsummierung der berechneten Likelihood mit sum()\nBerechnung der Deviance - hierzu muss die aufsummierte Likelihood mal -2\ngenommen werden.\n\nWir können hierbei auf die Berechnung des Logarhitmus verzichten, da dnorm() schon den die log-Likelihood mit ausgibt. Hierzu muss allerdings das Argument log = TRUE gesetzt werden ! Zur Erinnerung, auf bestimmte Elemente eines Vektors greift ihr folgendermaßen zu (dies ist wichtig zu wissen, da Ihr mit den Inputwerten innerhalb der Funktion arbeiten müsst):\n\n# Vektor Definieren\nvektor &lt;- c(1,2,3)\n\n# Erstes Element\n\nvektor[1]\n\n[1] 1\n\n# Zweites Element\n\nvektor[2]\n\n[1] 2\n\n# Drittes Element\n\nvektor[3]\n\n[1] 3\n\n\n3.) Es muss mit return() das Endprodukt zurückgeben, wie in den beiden einfachen Beispielen vorher !\n\n### YOUR CODE HERE \n\nMLE &lt;- function(Daten, theta) \n{\n  mu &lt;- theta[1]\n  sigma &lt;- theta[2]\n  \n  LL &lt;- -2*(sum(dnorm(Daten,mean=mu,sd=sigma,log = T)))\n  \n  return(LL)\n  \n}\n\n# Test your function with different values for theta\ntheta &lt;- c(80,20)\nMLE(iq,theta)\n\n[1] 932.1913\n\n\n\n\nOptimieren der Parameter mit optim()\nNun da unsere ML-Diskrepanzfunktion funktioniert, müssen wir diese natürlich minimieren - dazu können wir die Funktion optim() nutzen, die in R zur Verfügung steht. Mit optim() ist standardmäßig der SIMPLEX -Algorithmus eingestellt. Es können aber auch andere Algorhithmen genutzt werden. Es werden der Funktion drei Hauptargumente übergeben:\n\npar - ein Vektor mit den Startwerten der Parameterschätzung. Die Startwerte sollten nicht übermäßig von den erwarteten Werten abweichen. Schätzt man also einen Mittelwert von IQ Daten, macht es keinen Sinn, den Startwert für den Mittelwert auf 1 zu setzen, da der “wahre Wert” vermutlich zwischen 75 und 150 liegen wird. Gleiches gilt für die Standardabweichung.\nfn= - die Diskrepanzfunktion die es zu minimieren gilt. Hier die von uns definierte MLE() Funktion.\nDie Daten - diese übergebt ihr mit dem Namen, den Ihr in eurer Funktion verwendet habt, also hier Daten = iq.\n\nOptimiert nun die definierte Funktion mit optim() und speichert die Ergebnisse im Objekt fit.\n\n# YOUR CODE HERE\n\nfit &lt;- optim(par=c(mu=50,sigma=10), fn =MLE, Daten=iq)\n\nfit$par\n\n      mu    sigma \n99.00413 15.36639 \n\n# Biased Fit Example \n\niq_pop &lt;- rnorm(1000,mean=100,sd=15)\niq_bias &lt;- sample(iq_pop,25)\n\nmean(iq_bias)\n\n[1] 103.8359\n\n# Fitting Sample\nfit &lt;- optim(par=c(mu=50,sigma=10), fn =MLE, Daten=iq_bias)\n\nfit$par\n\n       mu     sigma \n103.84127  13.60439"
  },
  {
    "objectID": "cogmod_sem_sose23/ML_Workshop.html#fazit",
    "href": "cogmod_sem_sose23/ML_Workshop.html#fazit",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Fazit",
    "text": "Fazit\nIn diesem Tutorial haben wir uns mit der Maximum-Likelihood-Schätzung (MLE) in R beschäftigt und die Funktion `optim()` verwendet, um die Schätzung durchzuführen. Zunächst haben wir die Likelihood-Funktion selbst definiert und verschiedene Parameterwerte getestet, um das Konzept der MLE besser zu verstehen.\nDurch die Verwendung von `optim()` konnten wir die MLE iterativ implementieren und die optimalen Parameterwerte finden, die die Likelihood-Funktion maximieren (oder die Deviance minimieren). Wir haben gesehen, dass `optim()` eine effiziente Methode zur numerischen Optimierung ist und verschiedene Algorithmen zur Verfügung stellt.\nDie Maximum-Likelihood-Schätzung ist ein leistungsstarkes Werkzeug, um Parameter in statistischen Modellen zu schätzen. Es ermöglicht uns, die Wahrscheinlichkeit der beobachteten Daten unter verschiedenen Annahmen zu maximieren und die besten Parameterwerte zu ermitteln."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#fortgeschrittene-dplyr-funktionen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#fortgeschrittene-dplyr-funktionen",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "title": "Advanced Dplyr",
    "section": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen",
    "text": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen\nDie case_when Funktion in dplyr ermöglicht es, basierend auf bestimmten Bedingungen verschiedene Werte für eine Spalte in einem Datensatz auszuwählen und abhängig vom Variablenwert einen neue Variable zu erstellen. Es ist eine sehr einfache Möglichkeit, if-else Statements in eine dplyr-Pipeline zu integrieren.\nDie Syntax eines case_when() Befehles entspricht einer einfachen if oder if-else Verknüpfung. Eine if-else Verknüpfung prüft eine Bedingung und führt wenn diese Erfüllt ist einen definierten Befehl aus. Ist die Bedingung nicht erfüllt, wird andernfalls (else) ein anderer Befehl ausgeführt:\n\n\n\nCode\n# Vektor von Zahlen darauf testen, ob sie gerade oder ungerade sind:\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei geteilt keinen Rest hat, dann schreibe Gerade, ansonsten Ungerade.\n\nifelse(zahlen %% 2 == 0,\"Gerade\",\"Ungerade\")\n\n\ncase_when() funktioniert nach der Gleichen Logik:\n\n\n\nDabei wird immer zuerst die if-Bedinungen und deren Output definiert und anschließend mit TRUE die else Bedingung, welche angibt was passiert wenn die if - Bedingung nicht zutrifft:\n\n\n\n\nCode\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei\n# geteilt keinen Rest hat, dann schreibe Gerade, \n# ansonsten Ungerade.\n\ncase_when(zahlen %% 2 == 0 ~ \"Gerade\",\n          TRUE ~\"Ungerade\")\n\n\nDer Unterschied von case_when() und ifelse() ist, dass mit case_when() auch mehrere Bedingungen definiert werden können (z.B. wenn eine Variable in mehrere Kategorien eingeteilt werden soll):\n\nHier gibt es auch ein finales else Statement, dieses muss aber nicht sein, solange die Kategorien alle Fälle abdecken!\n\n2.0.1 case_when(): Beispiele\nDas folgende ist ein einfaches Beispiel, bei dem case_when() verwendet wird, um in einem Datensatz eine neue Spalte Alter_Kategorie anhand des Werts in der Spalte Alter zu kategorisieren. Wenn eine neue Spalte erstellt werden soll, muss case_when() immer mit mutate() kombiniert werden:\n\n\nCode\n# Beispiel Datensatz erstellen\ndf &lt;- data.frame(Name = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45))\nhead(df)\n\n# mutate und case_when verwenden, um neue Variable in Abhängigkeit vom Alter zu erstellen:\ndf %&gt;% \n  mutate(Alter_Kategorie = case_when(\n    Alter &lt;= 25 ~ \"jung\",\n    Alter &gt; 25 & Alter &lt;= 35 ~ \"mittel\",\n    Alter &gt; 35 ~ \"alt\"\n  ))"
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "title": "Advanced Dplyr",
    "section": "3 Long- und Wide-Datenformat - Datentransformation in dplyr",
    "text": "3 Long- und Wide-Datenformat - Datentransformation in dplyr\nNun haben wir die Daten von Frenken et al. neu kodiert und die nötigen Informationen im Datensatz, um mit MPTinR zu arbeiten. Allerdings liegen die Daten noch im sogenannten long - Format vor. Oftmals ist es jedoch notwendig, Daten entweder von einem long in ein wide Format oder umgekehrt zu transfomieren. Im Kontext von Datenanalyse und statistischer Modellierung gibt es in der Regel zwei Haupttypen von Datenformaten: wide-Format (breit-Format) und long-Format (lang-Format).\n\n3.1 Wide-Format\n\nIn einem wide Format sind die verschiedenen Merkmale einer einzigen Beobachtung in separaten Spalten dargestellt.\nEine einzige Zeile in einem breiten Datensatz repräsentiert eine Beobachtung.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, wäre in einem breiten Format dargestellt, wobei jede Spalte einem bestimmten Fach entspricht.\nVorteil: Es ist einfach, schnelle Übersichten über große Datenmengen zu erhalten.\n\nBeispiel:\n\n\nCode\nwide_df &lt;- data.frame(Schüler_ID = c(1, 2, 3), \n                      Mathe = c(89, 76, 92), \n                      Englisch = c(92, 88, 95), \n                      Wissenschaft = c(88, 72, 98))\nwide_df\n##   Schüler_ID Mathe Englisch Wissenschaft\n## 1          1    89       92           88\n## 2          2    76       88           72\n## 3          3    92       95           98\n\n\n\n\n3.2 Long-Format\n\nIm long Format sind alle Merkmale einer einzigen Beobachtung in einer Zeile dargestellt.\nEine einzige Spalte in einem long Datensatz repräsentiert ein bestimmtes Merkmal.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, könnte in einem langen Format dargestellt werden, wobei jede Zeile einer bestimmten Schüler-Fach-Kombination entspricht.\nVorteil: Es ist einfach, bestimmte Merkmale für verschiedene Beobachtungen zu vergleichen oder zu analysieren. Außerdem ist es für manche statistische Methoden, wie lineare Regression oder einer ANOVA, das bevorzugte Format.\n\nBeispiel:\n\n\nCode\nlong_df &lt;- data.frame(Schüler_ID = c(rep(1, 3), rep(2, 3), rep(3, 3)), \n                      Fach = c(rep(\"Mathe\", 3), rep(\"Englisch\", 3), rep(\"Wissenschaft\", 3)), \n                      Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\nlong_df\n##   Schüler_ID         Fach Note\n## 1          1        Mathe   89\n## 2          1        Mathe   92\n## 3          1        Mathe   88\n## 4          2     Englisch   76\n## 5          2     Englisch   88\n## 6          2     Englisch   72\n## 7          3 Wissenschaft   92\n## 8          3 Wissenschaft   95\n## 9          3 Wissenschaft   98\n\n\nEs ist wichtig, das Konzept des wide- und long-Formats zu verstehen, da es bei der Datenaufbereitung und Analyse eine wichtige Rolle spielt. Zum Beispiel kann ein wide -Datensatz schwer zu analysieren sein, wenn man Vergleiche zwischen bestimmten Merkmalen über mehrere Beobachtungen hinweg machen möchte (z.b. bei Varianzanalysen). Hier ist es oft besser, den Datensatz in ein long-Format zu bringen.Umgekehrt kann es der Fall sein, das eine bestimmte Analyseform oder Modellierung die Daten im wide-Format\n\n\n3.3 pivot - Funktionen in dplyr\nDie Funktionen pivot_wider und pivot_longer gehören zu den Funktionen von dplyr und dienen dazu, Datensätze zu transformieren.\n\npivot_wider verwandelt einen long-Format Datensatz in einen wide-Format Datensatz, indem es die Werte einer bestimmten Spalte zu neuen Spalten umbenennt. Dies kann manuell durchgeführt werden, aber pivot_wider macht dies automatisch und erleichtert so die Datentransformation:\npivot_longer verwandelt einen wide-Format Datensatz in einen long-Format Datensatz, indem es die Spalten mit bestimmten Werten in einer neuen Spalte zusammenfasst.\n\nEin Beispiel für den Einsatz von pivot_wider:\n\n\nCode\n# long data example\nlong_data &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\", \"John\", \"Jane\", \"Jim\"),\n  subject = c(\"physics\", \"physics\", \"physics\", \"math\", \"math\", \"math\"),\n  score = c(85, 90, 80, 75, 80, 70)\n)\n\nlong_data\n## # A tibble: 6 × 3\n##   name  subject score\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 John  physics    85\n## 2 Jane  physics    90\n## 3 Jim   physics    80\n## 4 John  math       75\n## 5 Jane  math       80\n## 6 Jim   math       70\n\n\n# pivot to wide format\npivot_wider(long_data, names_from = subject, values_from = score)\n## # A tibble: 3 × 3\n##   name  physics  math\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n## 1 John       85    75\n## 2 Jane       90    80\n## 3 Jim        80    70\n\n\nEin Beispiel für den Einsatz von pivot_longer:\n\n\nCode\n# wide data example\nwide_df &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\"),\n  Mathe_Note = c(85, 90, 80),\n  Englisch_Note = c(75, 80, 70)\n)\n\n\nwide_df\n## # A tibble: 3 × 3\n##   name  Mathe_Note Englisch_Note\n##   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n## 1 John          85            75\n## 2 Jane          90            80\n## 3 Jim           80            70\n\n\n# pivot to long format\npivot_longer(wide_df, cols = c(Mathe_Note, Englisch_Note), \n             names_to = \"Schüler\", values_to = \"Note\")\n## # A tibble: 6 × 3\n##   name  Schüler        Note\n##   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n## 1 John  Mathe_Note       85\n## 2 John  Englisch_Note    75\n## 3 Jane  Mathe_Note       90\n## 4 Jane  Englisch_Note    80\n## 5 Jim   Mathe_Note       80\n## 6 Jim   Englisch_Note    70\n\n\n\n\n3.4 Argumente für pivot_longer und pivot_wider\npivot_longer benötigt mindestens zwei Argumente:\n\ncols: Dies ist ein Zeichenvektor, der angibt, welche Spalten im Datensatz zusammengefasst werden sollen. -Schüler_ID bedeutet zum Beispiel, dass alle Spalten außer Schüler_ID zusammengefasst werden sollen.\nnames_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die zusammengefassten Werte gespeichert werden.\nvalues_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die Werte gespeichert werden, die aus den zusammengefassten Spalten stammen.\n\n\npivot_wider benötigt mindestens zwei Argumente:\n\nnames_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Namen für die neuen Spalten verwendet werden soll.\nvalues_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Werte für die neuen Spalten verwendet werden soll.\n\n\nBeide Funktionen haben auch weitere optionale Argumente wie values_fill und names_prefix oder names_sep()um die Daten bei Bedarf weiter anzupassen."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#praktisches-beispiel",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#praktisches-beispiel",
    "title": "Advanced Dplyr",
    "section": "4 Praktisches Beispiel",
    "text": "4 Praktisches Beispiel\nIm nächstes Beipsiel werden wir einen Datensatz umformen, mit dem wir uns im nächsten Workshop zu MPT-Modellen beschäftigen werden. Der Datensatz enthält fünf Spalten, die die Daten aus einem Priming-Experiment codieren. Insgesamt hat jede Versuchsperson 4 Bedingungen durchlaufen, die in der Spalte stim codiert sind (bg,bp,wg,wp). In jeder dieser Bedingungen mussten die VP eine binäre Entscheidungsaufgabe bearbeiten. in den Spalten hits und miss ist codiert, ob die VP die richtige oder falsche Entscheidung getroffen haben. Die Spalte ntrials gibt die Gesamtzahl der Versuche einer Person pro Bedingung an.\n\n\nCode\nhead(freq_dat,5)\n\n\n# A tibble: 5 × 5\n# Groups:   subj_idx [2]\n  subj_idx stim   hits ntrials  miss\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n1        0 bg       30      31     1\n2        0 bp       28      29     1\n3        0 wg       23      27     4\n4        0 wp       28      33     5\n5        1 bg       28      30     2\n\n\nUm in der Praxis mit dem Datensatz arbeiten zu können, müssen wir diesen jedoch zunächst in das richtige Format bringen. Der Datensatz liegt momentan im long-Format vor, für bestimmte Analysen muss dieser jedoch in das wide-Format umgewandelt werden. Das führt dazu, dass es für jede Bedingung je eine Spalte für hits und misses geben wird, also z.B. hits_bg und misses_bg.\nNach der Umformung, sieht der Datensatz dann wie folgt aus:\n\n\nCode\n#| echo: true\n#| output: true\n#| warning: false\n#| code-overflow: wrap\n#| collapse: true\n\nhead(freq_dat %&gt;% pivot_wider(names_from = c(\"stim\"),\n                         values_from = c(\"hits\",\"miss\"),\n                         id_cols=\"subj_idx\"),5)\n\n\n# A tibble: 5 × 9\n# Groups:   subj_idx [5]\n  subj_idx hits_bg hits_bp hits_wg hits_wp miss_bg miss_bp miss_wg miss_wp\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1        0      30      28      23      28       1       1       4       5\n2        1      28      16      27      11       2       2       2       1\n3        3      25      10      33      23       0       0       1       1\n4        4      35      24      27      33       1       0       0       0\n5        5      31      28      25      23       1       2       4       4\n\n\nWas passiert hier genau ? Schritt für Schritt:\n\nnames_from = c(stim) - Die neuen Spalten sollen aus der Spalte stim benannt werden - (bg,bp,wp,wg)\nvalues_from = c(\"hits\",\"miss\") - in den neuen Spalten sollen die Werte der Hits und Miss Spalten stehen - durch den ersten und zweiten Schritt entstehen also Spalten, die jeweils hits_bg etc. enthalten. Also die Hits aus der Bedingung “bg” usw.\nid_cols - dies soll für jedes Subject einzeln geschehen."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#übungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_SS23.html#übungen",
    "title": "Advanced Dplyr",
    "section": "5 Übungen",
    "text": "5 Übungen\n\n5.1 case_when()\n1.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Note_Kategorie” zu erstellen, die “Sehr gut” für Noten über 90, “Gut” für Noten zwischen 80 und 90 und “Schlecht” für Noten unter 80 angibt.\n\n\nCode\ndf &lt;- data.frame(Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\n# Your Code Here\n\n\n2.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Bestanden” zu erstellen, um zu Prüfen ob ein Schüler einer bestimmten Schulform eine Prüfung bestanden hat. Die Bestehensgrenzen sind wie folgt.\n-   Für die Hauptschule liegt die Bestehensgrenze bei 50 %\n-   Für die Realschule liegt die Bestehensgrenze bei 60 %\n-   Für das Gymnasium liegt die Bestehensgrenze bei 70 %\nKodieren Sie das bestehen entweder mit “Pass” oder “Fail”.\n\n\nCode\ndf &lt;- data.frame(Note = runif(100,min=0, max=100),\n                 Schulform = sample(c(\"Gymnasium\",\"Realschule\", \"Hauptschule\"),\n                                    size = 100, replace = T))\n# Your Code here\n\n\n3.) Nutzen Sie die Funktion case_when und die dplyr-Library in R, um eine neue Spalte in dem Datensatz “df” zu erstellen, die die Einkommenskategorie jeder Person basierend auf ihrem Berufsstatus und ihrem Einkommen kategorisiert. Die Einkommenskategorien sollten wie folgt sein:\n\nFür Angestellte mit einem Einkommen von bis zu 50.000: “niedrig”\nFür Angestellte mit einem Einkommen zwischen 50.000 und 75.000: “mittel”\nFür Angestellte mit einem Einkommen über 75.000: “hoch”\nFür Freiberufler mit einem Einkommen von bis zu 60.000: “niedrig”\nFür Freiberufler mit einem Einkommen zwischen 60.000 und 100.000: “mittel”\nFür Freiberufler mit einem Einkommen über 100.000: “hoch”\nFür Ruheständler mit einem Einkommen von bis zu 30.000: “niedrig”\nFür Ruheständler mit einem Einkommen über 30.000: “mittel_hoch”\n\n\n\nCode\ndf &lt;- data.frame(ID = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45),\n                 Berufsstatus = c(\"Angestellter\", \"Freiberufler\", \"Ruheständler\"),\n                 Einkommen = c(45000, 75000, 32000))\n\n\n# Your Code here\n\n\n\n\n5.2 Pivotting\n1.) Konvertieren Sie die folgenden Datensatz von wide to long. Erstellen Sie aus den Spalten zwei neue Spalten mit den Namen (month und index). Nutzen Sie dazu das Argument names_sep = \"_\". Die Werte sollen in die Spalte “N” geschrieben werden.\nTip: Sie müssen bei names_to einen Vektor mit den Namen der neuen Spalten angeben.\n\n\nCode\ndf_wide &lt;- data.frame(\n  Jan_sales = c(10, 40, 70),\n  Feb_sales = c(20, 50, 80),\n  Mar_sales = c(30, 60, 90)\n)\n\n# Your Code here\n\n\n2.) Konvertieren Sie die folgendn Datensatz von long in wide.\n\n\nCode\n    df_long &lt;- data.frame(\n      ID = c(\"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\"),\n      month = c(\"Jan\", \"Jan\", \"Jan\", \"Feb\", \"Feb\", \"Feb\", \"Mar\", \"Mar\", \"Mar\"),\n      sales = c(10, 40, 70, 20, 50, 80, 30, 60, 90))\n      \n\n    # Your Code Here"
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#fortgeschrittene-dplyr-funktionen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#fortgeschrittene-dplyr-funktionen",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "title": "Advanced Dplyr",
    "section": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen",
    "text": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen\nDie case_when Funktion in dplyr ermöglicht es, basierend auf bestimmten Bedingungen verschiedene Werte für eine Spalte in einem Datensatz auszuwählen und abhängig vom Variablenwert einen neue Variable zu erstellen. Es ist eine sehr einfache Möglichkeit, if-else Statements in eine dplyr-Pipeline zu integrieren.\nDie Syntax eines case_when() Befehles entspricht einer einfachen if oder if-else Verknüpfung. Eine if-else Verknüpfung prüft eine Bedingung und führt wenn diese Erfüllt ist einen definierten Befehl aus. Ist die Bedingung nicht erfüllt, wird andernfalls (else) ein anderer Befehl ausgeführt:\n\n\n# Vektor von Zahlen darauf testen, ob sie gerade oder ungerade sind:\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei geteilt keinen Rest hat, dann schreibe Gerade, ansonsten Ungerade.\n\nifelse(zahlen %% 2 == 0,\"Gerade\",\"Ungerade\")\n\ncase_when() funktioniert nach der Gleichen Logik:\n\n\n\nDabei wird immer zuerst die if-Bedinungen und deren Output definiert und anschließend mit TRUE die else Bedingung, welche angibt was passiert wenn die if - Bedingung nicht zutrifft:\n\n\n\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei\n# geteilt keinen Rest hat, dann schreibe Gerade, \n# ansonsten Ungerade.\n\ncase_when(zahlen %% 2 == 0 ~ \"Gerade\",\n          TRUE ~\"Ungerade\")\n\nDer Unterschied von case_when() und ifelse() ist, dass mit case_when() auch mehrere Bedingungen definiert werden können (z.B. wenn eine Variable in mehrere Kategorien eingeteilt werden soll):\n\nHier gibt es auch ein finales else Statement, dieses muss aber nicht sein, solange die Kategorien alle Fälle abdecken!\n\n2.0.1 case_when: Beispiele\nDas folgende ist ein einfaches Beispiel, bei dem case_when() verwendet wird, um in einem Datensatz eine neue Spalte Alter_Kategorie anhand des Werts in der Spalte Alter zu kategorisieren. Wenn eine neue Spalte erstellt werden soll, muss case_when() immer mit mutate() kombiniert werden:\n\n# Beispiel Datensatz erstellen\ndf &lt;- data.frame(Name = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45))\nhead(df)\n\n# mutate und case_when verwenden, um neue Variable in Abhängigkeit vom Alter zu erstellen:\ndf %&gt;% \n  mutate(Alter_Kategorie = case_when(\n    Alter &lt;= 25 ~ \"jung\",\n    Alter &gt; 25 & Alter &lt;= 35 ~ \"mittel\",\n    Alter &gt; 35 ~ \"alt\"\n  ))\n\nEin weiteres Beispiel haben wir letzte Woche bei der MPT-Modellierung der Daten von Frenken et al. gesehen. Im Datensatz ist shoot als 0 und not shoot als 1 kodiert. Für die MPT Modellierung benötigen wir aber für jede Kategorie (Black / Gun, Black / Phone, White / Gun, White / Phone) die Hits und Misses, also die Fehlerraten. Der Datensatz hat praktischerweise eine Spalte, die genau kodiert, was die VP gesehen hat:\n\n\nhead(Study_2_dm)\n\n# A tibble: 6 × 6\n  subj_idx stimulus stim     rt response condition\n     &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt; &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;    \n1        0 gun      bg    0.506        0 black    \n2        0 phone    wp    0.437        0 white    \n3        0 phone    wp    0.483        0 white    \n4        0 phone    bp    0.594        1 black    \n5        0 phone    wp    0.552        1 white    \n6        0 gun      wg    0.417        1 white    \n\n\nIn der zweiten Spalte stimulus ist angebenen, welches Objekt der jeweils zusammen mit der Hautfarbe gezeigt wurde. Da wir wissen, dass nur bei “gun” geschossen werden darf, können wir nun in Kombination mit response (0 - shoot, 1 - not shoot) die Hits mit case_when() in einer neuen Spalte ACC kodieren.\nDazu Verknüpfen wir hier zwei Bedingungen mit dem & Operator. Die erste Bedingung bezieht sich auf die Spalte stimulus, hier muss geprüft werden, welcher Stimulus gezeigt wurde (Gun vs. Phone). Die zweite Bedingung bezieht sich auf die Spalte response. Hier muss geprüft werden, ob geschossen wurde oder nicht. Ingesamt müssen also vier Bedinungen definiert werden, für jede Kombination von Objekt und Response:\n\nAccuracy in Abhängigkeit von Gegebener Response und Stimulus\n\n\nStimulus\nResponse\nAccuracy\n\n\n\n\nGun\n0\n1\n\n\nGun\n1\n0\n\n\nPhone\n0\n0\n\n\nPhone\n1\n1\n\n\n\nDies müssen wir nun in ein case_when() Befehl übernehmen\n\nMit mutate() die neue Outputspalte benennen\nInnerhalb von mutate() mit case_when() die Accuracy in Abhängigkeit der Spalten Stimulus und Response umkodieren:\n\n\n\nfreq_dat &lt;- Study_2_dm %&gt;% mutate(ACC = case_when(stimulus == \"gun\" & response == 0 ~ 1,\n                                                  stimulus == \"gun\" & response == 1 ~ 0,\n                                                  stimulus == \"phone\" & response == 1 ~ 1,\n                                                  stimulus ==\"phone\" & response == 0 ~ 0))\n\nAnschließend müssen wir nun die Hits und Misses auszählen. Generell eignen sich zum Auszählen von bestimmten Bedingungskombinationen oder Trials die Funktionen group_by(), summarise() und n(). n() ist eine einfache Zählfunktion, welche innerhalb von group_by() %&gt;% summarise() dazu führt, dass die alle Beobachtungen der gruppierten Variablen (z.B. Subject & Bedingung) innerhalb von summarise() gezählt werden.\nAlso zum Beispiel, wieviele Beobachtung von Subject 1 gibt es in Bedingung A, B und C. Dies macht aber nur Sinn, wenn ihr Daten auf Trial-Ebene (also für jede Versuchsperson alle Antworten über das ganez Experiment) vorliegen habt. Dies ist hier der Fall, da für jede Person und jede Bedingung, die diese Person durchlaufen hat, die gegebenen Antworten im Datensatz in der Spalte responses vorliegen.\n\n\nfreq_dat &lt;- freq_dat %&gt;% group_by(subj_idx,stim) %&gt;% \n  summarise(hits = sum(ACC), \n            ntrials=n(), \n            miss=ntrials-hits)\n\n`summarise()` has grouped output by 'subj_idx'. You can override using the\n`.groups` argument.\n\nhead(freq_dat)\n\n# A tibble: 6 × 5\n# Groups:   subj_idx [2]\n  subj_idx stim   hits ntrials  miss\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n1        0 bg       30      31     1\n2        0 bp       28      29     1\n3        0 wg       23      27     4\n4        0 wp       28      33     5\n5        1 bg       28      30     2\n6        1 bp       16      18     2\n\n\nWas passiert hier genau ? Schritt für Schritt:\n\ngroup_by(subj_idx, stim) : Wir gruppieren zunächst nach Subject und Bedingung\n\nsummarise(...\n\nhits = sum(ACC) - Wir haben alle Hits mit case_when als 1 kodiert, also können wir die Hits einfach berechnen, indem wir diese aufsummieren.\nnTrials = n() - Auszählen wieviele Responses ein Subject in jeder stimulus Bedinungen gegeben hat (Gesamtanzahl von gegebenen Antworten in einer Bedingungen pro Person\nmiss = ntrials - hits Alles was kein Hit ist muss ein Miss sein ! Daher können wir einfach die Hits von der Gesamtzahl der Antworten abziehen und erhalten die Anzahl der Misses in jeder Bedinung pro Subject !"
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "title": "Advanced Dplyr",
    "section": "3 Long- und Wide-Datenformat - Datentransformation in dplyr",
    "text": "3 Long- und Wide-Datenformat - Datentransformation in dplyr\nNun haben wir die Daten von Frenken et al. neu kodiert und die nötigen Informationen im Datensatz, um mit MPTinR zu arbeiten. Allerdings liegen die Daten noch im sogenannten long - Format vor. Oftmals ist es jedoch notwendig, Daten entweder von einem long in ein wide Format oder umgekehrt zu transfomieren. Im Kontext von Datenanalyse und statistischer Modellierung gibt es in der Regel zwei Haupttypen von Datenformaten: wide-Format (breit-Format) und long-Format (lang-Format).\n\n3.1 Wide-Format\n\nIn einem wide Format sind die verschiedenen Merkmale einer einzigen Beobachtung in separaten Spalten dargestellt.\nEine einzige Zeile in einem breiten Datensatz repräsentiert eine Beobachtung.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, wäre in einem breiten Format dargestellt, wobei jede Spalte einem bestimmten Fach entspricht.\nVorteil: Es ist einfach, schnelle Übersichten über große Datenmengen zu erhalten.\n\nBeispiel:\n\nwide_df &lt;- data.frame(Schüler_ID = c(1, 2, 3), \n                      Mathe = c(89, 76, 92), \n                      Englisch = c(92, 88, 95), \n                      Wissenschaft = c(88, 72, 98))\nwide_df\n##   Schüler_ID Mathe Englisch Wissenschaft\n## 1          1    89       92           88\n## 2          2    76       88           72\n## 3          3    92       95           98\n\n\n\n3.2 Long-Format\n\nIm long Format sind alle Merkmale einer einzigen Beobachtung in einer Zeile dargestellt.\nEine einzige Spalte in einem long Datensatz repräsentiert ein bestimmtes Merkmal.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, könnte in einem langen Format dargestellt werden, wobei jede Zeile einer bestimmten Schüler-Fach-Kombination entspricht.\nVorteil: Es ist einfach, bestimmte Merkmale für verschiedene Beobachtungen zu vergleichen oder zu analysieren. Außerdem ist es für manche statistische Methoden, wie lineare Regression oder einer ANOVA, das bevorzugte Format.\n\nBeispiel:\n\nlong_df &lt;- data.frame(Schüler_ID = c(rep(1, 3), rep(2, 3), rep(3, 3)), \n                      Fach = c(rep(\"Mathe\", 3), rep(\"Englisch\", 3), rep(\"Wissenschaft\", 3)), \n                      Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\nlong_df\n##   Schüler_ID         Fach Note\n## 1          1        Mathe   89\n## 2          1        Mathe   92\n## 3          1        Mathe   88\n## 4          2     Englisch   76\n## 5          2     Englisch   88\n## 6          2     Englisch   72\n## 7          3 Wissenschaft   92\n## 8          3 Wissenschaft   95\n## 9          3 Wissenschaft   98\n\nEs ist wichtig, das Konzept des wide- und long-Formats zu verstehen, da es bei der Datenaufbereitung und Analyse eine wichtige Rolle spielt. Zum Beispiel kann ein wide -Datensatz schwer zu analysieren sein, wenn man Vergleiche zwischen bestimmten Merkmalen über mehrere Beobachtungen hinweg machen möchte (Beispiel Ergebnisse der Diffusionsmodellierung!). Hier ist es oft besser, den Datensatz in ein long-format zu bringen.\n\n\n3.3 pivot - Funktionen in dplyr\nDie Funktionen pivot_wider und pivot_longer gehören zu den Funktionen von dplyr und dienen dazu, Datensätze zu transformieren.\npivot_wider verwandelt einen long-Format Datensatz in einen wide-Format Datensatz, indem es die Werte einer bestimmten Spalte zu neuen Spalten umbenennt. Dies kann manuell durchgeführt werden, aber pivot_wider macht dies automatisch und erleichtert so die Datentransformation:\n\n\npivot_longer verwandelt einen wide-Format Datensatz in einen long-Format Datensatz, indem es die Spalten mit bestimmten Werten in einer neuen Spalte zusammenfasst.\nEin Beispiel für den Einsatz von pivot_wider:\n\n# long data example\nlong_data &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\", \"John\", \"Jane\", \"Jim\"),\n  subject = c(\"physics\", \"physics\", \"physics\", \"math\", \"math\", \"math\"),\n  score = c(85, 90, 80, 75, 80, 70)\n)\n\nlong_data\n## # A tibble: 6 × 3\n##   name  subject score\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 John  physics    85\n## 2 Jane  physics    90\n## 3 Jim   physics    80\n## 4 John  math       75\n## 5 Jane  math       80\n## 6 Jim   math       70\n\n\n# pivot to wide format\npivot_wider(long_data, names_from = subject, values_from = score)\n## # A tibble: 3 × 3\n##   name  physics  math\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n## 1 John       85    75\n## 2 Jane       90    80\n## 3 Jim        80    70\n\nEin Beispiel für den Einsatz von pivot_longer:\n\n# wide data example\nwide_df &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\"),\n  Mathe_Note = c(85, 90, 80),\n  Englisch_Note = c(75, 80, 70)\n)\n\n\nwide_df\n## # A tibble: 3 × 3\n##   name  Mathe_Note Englisch_Note\n##   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n## 1 John          85            75\n## 2 Jane          90            80\n## 3 Jim           80            70\n\n\n# pivot to long format\npivot_longer(wide_df, cols = c(Mathe_Note, Englisch_Note), \n             names_to = \"Schüler\", values_to = \"Note\")\n## # A tibble: 6 × 3\n##   name  Schüler        Note\n##   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n## 1 John  Mathe_Note       85\n## 2 John  Englisch_Note    75\n## 3 Jane  Mathe_Note       90\n## 4 Jane  Englisch_Note    80\n## 5 Jim   Mathe_Note       80\n## 6 Jim   Englisch_Note    70\n\n\n\n3.4 Argumente für pivot_longer und pivot_wider\npivot_longer benötigt mindestens zwei Argumente:\n\ncols: Dies ist ein Zeichenvektor, der angibt, welche Spalten im Datensatz zusammengefasst werden sollen. -Schüler_ID bedeutet zum Beispiel, dass alle Spalten außer Schüler_ID zusammengefasst werden sollen.\nnames_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die zusammengefassten Werte gespeichert werden.\nvalues_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die Werte gespeichert werden, die aus den zusammengefassten Spalten stammen.\n\n\npivot_wider benötigt mindestens zwei Argumente:\n\nnames_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Namen für die neuen Spalten verwendet werden soll.\nvalues_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Werte für die neuen Spalten verwendet werden soll.\n\n\nBeide Funktionen haben auch weitere optionale Argumente wie values_fill und names_prefix oder names_sep()um die Daten bei Bedarf weiter anzupassen.\nHier nun die Umformung der Daten von Frenken et al., welche wir vom long Format in das wide Format bringen müssen:\n\n\n#| echo: true\n#| output: true\n#| warning: false\n#| code-overflow: wrap\n#| collapse: true\n\nfreq_dat %&gt;% pivot_wider(names_from = c(\"stim\"),\n                         values_from = c(\"hits\",\"miss\"),\n                         id_cols=\"subj_idx\")\n\n# A tibble: 137 × 9\n# Groups:   subj_idx [137]\n   subj_idx hits_bg hits_bp hits_wg hits_wp miss_bg miss_bp miss_wg miss_wp\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1        0      30      28      23      28       1       1       4       5\n 2        1      28      16      27      11       2       2       2       1\n 3        3      25      10      33      23       0       0       1       1\n 4        4      35      24      27      33       1       0       0       0\n 5        5      31      28      25      23       1       2       4       4\n 6        6      34      26      18      31       0       0       5       0\n 7        7      29      24      21      36       2       0       1       1\n 8        8      22      29      25      28       0       2      10       2\n 9        9      28      23      22      22       2       0       2       1\n10       10      26      35      25      24       4       0       0       3\n# ℹ 127 more rows\n\n\nWas passiert hier genau ? Schritt für Schritt:\n\nnames_from = c(stim) - Die neuen Spalten sollen aus der Spalte stim benannt werden - (bg,bp,wp,wg)\nvalues_from = c(\"hits\",\"miss\") - in den neuen Spalten sollen die Werte der Hits und Miss Spalten stehen - durch den ersten und zweiten Schritt entstehen also Spalten, die jeweils hits_bg etc. enthalten. Also die Hits aus der Bedingung “bg” usw.\nid_cols - dies soll für jedes Subject einzeln geschehen."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#übungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R.html#übungen",
    "title": "Advanced Dplyr",
    "section": "4 Übungen",
    "text": "4 Übungen\n\n4.1 case_when\n1.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Note_Kategorie” zu erstellen, die “Sehr gut” für Noten über 90, “Gut” für Noten zwischen 80 und 90 und “Schlecht” für Noten unter 80 angibt.\n\ndf &lt;- data.frame(Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\n# Your Code Here\ndf_categorized &lt;- df %&gt;% \n  mutate(Note_Kategorie = case_when(\n    Note &gt;= 90 ~ \"Sehr gut\",\n    Note &gt;= 80 & Note &lt; 90 ~ \"Gut\",\n    Note &lt; 80 ~ \"Schlecht\"\n  ))\n\n2.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Bestanden” zu erstellen, um zu Prüfen ob ein Schüler einer bestimmten Schulform eine Prüfung bestanden hat. Die Bestehensgrenzen sind wie folgt.\n-   Für die Hauptschule liegt die Bestehensgrenze bei 50 %\n-   Für die Realschule liegt die Bestehensgrenze bei 60 %\n-   Für das Gymnasium liegt die Bestehensgrenze bei 70 %\nKodieren Sie das bestehen entweder mit “Pass” oder “Fail”.\n\ndf &lt;- data.frame(Note = runif(100,min=0, max=100),\n                 Schulform = sample(c(\"Gymnasium\",\"Realschule\", \"Hauptschule\"),\n                                    size = 100, replace = T))\n# Your Code here\ndf %&gt;% \n  mutate(Pruefungsergebnis = case_when(\n    Schulform == \"Hauptschule\" & Note &gt;= 50 ~ \"Pass\",\n    Schulform == \"Realschule\" & Note &gt;= 60 ~ \"Pass\",\n    Schulform == \"Gymnasium\" & Note &gt;= 70 ~ \"Pass\",\n    TRUE ~ \"Fail\"\n  ))\n\n3.) Nutzen Sie die Funktion case_when und die dplyr-Library in R, um eine neue Spalte in dem Datensatz “df” zu erstellen, die die Einkommenskategorie jeder Person basierend auf ihrem Berufsstatus und ihrem Einkommen kategorisiert. Die Einkommenskategorien sollten wie folgt sein:\n\nFür Angestellte mit einem Einkommen von bis zu 50.000: “niedrig”\nFür Angestellte mit einem Einkommen zwischen 50.000 und 75.000: “mittel”\nFür Angestellte mit einem Einkommen über 75.000: “hoch”\nFür Freiberufler mit einem Einkommen von bis zu 60.000: “niedrig”\nFür Freiberufler mit einem Einkommen zwischen 60.000 und 100.000: “mittel”\nFür Freiberufler mit einem Einkommen über 100.000: “hoch”\nFür Ruheständler mit einem Einkommen von bis zu 30.000: “niedrig”\nFür Ruheständler mit einem Einkommen über 30.000: “mittel_hoch”\n\n\ndf &lt;- data.frame(ID = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45),\n                 Berufsstatus = c(\"Angestellter\", \"Freiberufler\", \"Ruheständler\"),\n                 Einkommen = c(45000, 75000, 32000))\n\ndf %&gt;% \n  mutate(Einkommenskategorie = case_when(\n    Berufsstatus == \"Angestellter\" & Einkommen &lt;= 50000 ~ \"niedrig\",\n    Berufsstatus == \"Angestellter\" & Einkommen &gt; 50000 & Einkommen &lt;= 75000 ~ \"mittel\",\n    Berufsstatus == \"Angestellter\" & Einkommen &gt; 75000 ~ \"hoch\",\n    Berufsstatus == \"Freiberufler\" & Einkommen &lt;= 60000 ~ \"niedrig\",\n    Berufsstatus == \"Freiberufler\" & Einkommen &gt; 60000 & Einkommen &lt;= 100000 ~ \"mittel\",\n    Berufsstatus == \"Freiberufler\" & Einkommen &gt; 100000 ~ \"hoch\",\n    Berufsstatus == \"Ruheständler\" & Einkommen &lt;= 30000 ~ \"niedrig\",\n    Berufsstatus == \"Ruheständler\" & Einkommen &gt; 30000 ~ \"mittel_hoch\"\n  ))\n##      ID Alter Berufsstatus Einkommen Einkommenskategorie\n## 1 Peter    25 Angestellter     45000             niedrig\n## 2  Anna    35 Freiberufler     75000              mittel\n## 3   Max    45 Ruheständler     32000         mittel_hoch\n\n\n\n4.2 Pivotting\n1.) Konvertieren Sie die folgenden Datensatz von wide to long. Erstellen Sie aus den Spalten zwei neue Spalten mit den Namen (month und index). Nutzen Sie dazu das Argument names_sep = \"_\". Die Werte sollen in die Spalte “N” geschrieben werden.\nTip: Sie müssen bei names_to einen Vektor mit den Namen der neuen Spalten angeben.\n\ndf_wide &lt;- data.frame(\n  Jan_sales = c(10, 40, 70),\n  Feb_sales = c(20, 50, 80),\n  Mar_sales = c(30, 60, 90)\n)\n\n# Your Code here\ndf_long &lt;- df_wide %&gt;% pivot_longer(cols=everything(),names_to = c(\"month\",\"index\"), values_to = \"N\",names_sep = \"_\")\n\n2.) Konvertieren Sie die folgendn Datensatz von long in wide.\n\n    df_long &lt;- data.frame(\n      ID = c(\"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\"),\n      month = c(\"Jan\", \"Jan\", \"Jan\", \"Feb\", \"Feb\", \"Feb\", \"Mar\", \"Mar\", \"Mar\"),\n      sales = c(10, 40, 70, 20, 50, 80, 30, 60, 90))\n      \n\n    # Your Code Here\n    df_long %&gt;% pivot_wider(names_from = month, values_from = sales)"
  },
  {
    "objectID": "about/about_MLM_sose23.html",
    "href": "about/about_MLM_sose23.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Achtung 🚧\n\n\n\nDies ist ein “lebendiges” Dokument. Es ist möglich, dass einige Aktuallisierungen und Ergänzungen nach dem ersten Seminar-Block vorgenommen werden."
  },
  {
    "objectID": "about/about_MLM_sose23.html#seminarleitung",
    "href": "about/about_MLM_sose23.html#seminarleitung",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Seminarleitung",
    "text": "Seminarleitung\nJosé C. García Alanis\nAbteilung für Analyse und Modellierung komplexer Daten\nPsychologisches Institut\nJohannes Gutenberg-Universität Mainz Wallstraße 3, Raum 06-255\nD-55122 Mainz\njose.alanis at uni-mainz.de"
  },
  {
    "objectID": "about/about_MLM_sose23.html#organisatorisches-und-wichtige-infos",
    "href": "about/about_MLM_sose23.html#organisatorisches-und-wichtige-infos",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Organisatorisches und wichtige Infos",
    "text": "Organisatorisches und wichtige Infos\n\nAllgemeine Materialien für das Seminar\n\nWichtige Informationen und Kursmaterialien werden auf die LMS/Moodle-Seite des Seminars veröffentlicht:\n\nLink zum Seminar auf LMS.\n\nWann: Montags von 12:15 - 13:45 Uhr (17.04.22 - Mo. 17.07.23).\nWo: Seminarraum 01-211 (kleiner Hörsaal) im Psychologischen Institut (Binger Str.)\n\n\n\n\n\nInhalt des Seminars\n\nMulti-Level Modelle haben viele Namen. Häufig werden sie Mehrebenen-Modelle, Mixed Effects Models oder Random Coefficient Models genannt. Eine weitere häufige Bezeichnung für Multi-Level Modelle lautet Hierarchische Lineare Modelle. Dies hat zum Grund, dass Multi-Level Modelle eine statische Methode darstellen, die zur Analyse von hierarchisch strukturierten Daten (auch genestete Daten genannt) eingesetzt werden kann. Was genau versteckt sich hinter dem Begriff „hierarchisch strukturierte Daten“ und warum sind Multi-Level Modelle ein nützliches „Tool“, um Erkenntnisse aus dieser Art von Daten zu gewinnen? Mit diesen Fragen werden wir uns im Laufe des Seminars beschäftigen.\n\n\n\nLernziele\n\nIn diesem Seminar werden Sie lernen, wie Sie Multi-Level Modelle zur Analyse von „hierarchisch organisierten Daten“ anwenden können. Ziel des Seminars ist es, Sie zu theoretisch-konzeptionellen Überlegungen zu motivieren und Ihnen die statistisch-methodologischen Grundlagen zu vermitteln, sodass Sie in der Lage sind zu entscheiden, wann ein Multi-Level Modell zu Beschreibung von Zusammenhängen, die überlegene statistische Methode darstellt. Des Weiteren werden Sie Kennwerte und Methoden kennenlernen, die eigensetzt werden können, um die Aussagekraft und (potenziell inkrementelle) Validität eines Multi-Level Modells einzuschätzen. Am Ende des Seminars werden Sie im Stande sein, ein eigenes Analyseprojekt durchzuführen, in dem Multi-Level Modelle zum Einsatz kommen. Diese Fragen werden Sie im Laufe des Seminars bearbeiten:\n\n\n\nWas versteht man unter „hierarchisch strukturierten/organisierten Daten“?\n\nWie können diese erkannt werden?\nWelche Konsequenzen (im Sinne statistisch-methodologischer Einschränkungen) bringen hierarchisch strukturierte Daten mit sich?\nWelche theoretisch-konzeptuelle Überlegungen müssen berücksichtigt werden?\n\nWie unterschieden sich Multi-Level Modelle von anderen statistischen Analyseverfahren?\n\nWas sind Gemeinsamkeiten?\n\nWie werden Multi-Level Modelle geschätzt? (Was beschreiben sie?)\n\nWelche Arten von Modellen gibt es?\n\nWie können Multi-Level Modelle in der Programmiersprache R spezifiziert werden?\nWie sind die Ergebnisse eines Multilevel-Modells zu interpretieren?\nWie sind die Ergebnisse eines Multilevel-Modells zu bewerten?\n\nz.B. im Sinne ihrer Aussagekraft, Reliabilität und Validität."
  },
  {
    "objectID": "about/about_MLM_sose23.html#begleitendes-tutorium",
    "href": "about/about_MLM_sose23.html#begleitendes-tutorium",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Begleitendes Tutorium",
    "text": "Begleitendes Tutorium\n\nDas Seminar wird von einem Tutorium (Softwarekurs) begleitet. Im Tutorium werden Sie die Grundlagen des Statistiksoftware und Programmiersprache R. Sie werden in dem Umgang mit Daten geschult und werden verschiedene Techniken zur Datenaufbereitung erlernen und einüben. Dieser Teil des Tutoriums findet im ersten Teil des Semesters statt. Sie sollten in eines der beiden Tutorien angemeldet sein, entweder am Montag oder am Mittwoch. Besuchen das Tutorium. Erfahrungsgemäß können viele Fragen und Start-Schwierigkeiten im Umgang mit R im Tutorium gut aufgefangen und gelöst werden. Im zweiten Teil des Semesters findet ein vertiefendes Tutorium statt. Dieser baut auf die, Grundkenntnissen, die Sie im ersten Teil des Semersten erlernt haben werden. Im zweiten Teil des Semesters besuchen Sie das Tutorium bei mir. Die Termine für Tutorium bleiben wie gehabt. Sie müssen das Tutorium nicht wechseln. Bitte bleiben Sie in Ihren Gruppen, ich werde die Veranstaltung von den jeweiligen Kollegen:innen übernehmen und zur gewohnten Zeit anbieten. Wir eine Reihe von Materialien zusammengestellt, um Ihnen den Einstieg in die Programmiersprache R zur erleichtern. Diese können Sie unter dem folgenden Link erreichen:\n\n\n\nR-Kurs-Buch von der Abteilung Analyse und Modellierung komplexer Daten."
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression_parameters.html",
    "href": "mlm_seminar_sose23/lineare_regression_parameters.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "In den letzten Sitzungen haben wir uns mit der einfachen linearen Regression befasst. Dabei haben wir uns erst einmal dafür entschieden, eine Regression ohne Berücksichtigung der verschiedenen Subgruppen in den Daten zu berechnen.\nMit einem Prädiktor (\\(x_{1}\\) ), lautet die Formel der Regression:\n\\[\n\\widehat{y}_{m} = b_{0} + b_{1} \\cdot x_{m1}\n\\tag{1}\\]\nSie liefert uns zwei Parameter $b_{0} und \\(b_{1}\\).\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Ausprägung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Prädiktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt.\n\n\nUm die “Interpretierbarkeit” der Parameter (vorallem die des Interzepts.\nWir haben ebenfalls über “Zentrierung” gesprochen. Wenn wir eine Variable an ihren Mittelwert zentrieren, ziehen wir den Mittelwert der Variable von allen Werten (d.h. alle Beobachtungen) der Variable ab. Zentrierung definiert das Minimum und Maximum der Variable neu. Der Mittelwert einer am Mittelwert zentrierten Variable ist nun gleich Null. Wir können Zentrierung als eine “Daten-Vorverarbeitungsstrategie” betrachten, die die “Interpretierbarkeit” der Parameter (vor allem die des Intercepts) in einem linearen Modell steigert.\nBetrachten wir z.B. die folgende Abbildung:\n\n\n\n\n\n\n\n\n\nDargestellt sind die Ergebnisse zweier Regressionen. Beide Regressionen beschreiben den Zusammenhang zwischen Schulunlust und Schulleistung. Die linke Seite der Abbildung zeigt die Ergebnisse der Regression ohne Zentrierung von Schulunlust und die rechte Seite mit Zentrierung. Ein Unterschied ist, dass die Spannweite der zentrierten Daten ein anderer ist. Personen, die einen negativen Wert in der zeitrierten Schulunlust-Variable haben, befinden sich unterhalb des Mittelwert der Gesamtstichprobe. Personen mit positiven Werten befinden sich darüber. Ein weiterer Unterschied fällt auf, wenn wir die Intercepts der Modelle betrachten. Das Interzept auf der rechten Seite liegt inmitten der Verteilung (auf dem Mittelwert, welcher nach Zentrierung gleich Null ist). Zentrierung kann uns also helfen, das Interzept in einem für uns interpretierbaren Bereich “zu holen”.\n\n\n\nGehen wir nun zu einem Beispiel über, das die Subgruppen in den Daten berücksichtigt. Dafür benötigen wir die folgenden Daten:\n\n# die Daten können mit diesem Befehl geladen werden\nurlRemote &lt;- 'https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main'\nfpathData &lt;- '/data/schulunlust.txt'\ndata_schulleistung &lt;- read.table(paste0(urlRemote, fpathData),\n                          header = TRUE, dec = ',')\n\n# wir werden nur die ersten 5 klassen benutzen\n# wir können die Daten dieser Klassen mit\n# `dplyr`filtern\nrequire(dplyr)\ndata_schulleistung &lt;- data_schulleistung %&gt;%\n  filter(klasse_nr &lt;= 5) %&gt;%\n  # ebenfalls werden wir die variable `unlust`\n  # zentrieren\n  mutate(unlust_c = unlust - mean(unlust))\n\nDiese Daten sind identisch mit den Daten der obigen Abbildung.\n\n\n\nErster Schritt:\n\nBerechnen wir erst einmal eine Gesamt-Regression, ohne die Subgruppen (die einzelnen Klassen) in den Daten zu berücksichtigen.\nZur Erinnerung: Der R-Befehl, um eine lineare Regression zu berechnen lautet lm(fomula, data)\n\nZweiter Schritt:\n\nBerechnen wir 5 verschiedene Regressionen, eine für jede Klasse.\nTipp: Sie können die Daten der einzelnen Klassen mit dplyr() filtern.\n\nz.B.: klasse_1 &lt;- data_schulleistung %&gt;% filter(klasse_nr == 1)\nDanach können Sie die Regression mit lm() berechnen.\n\n\nSchreiben Sie \\(b_{0}\\) und \\(b_{1}\\) für alle Regressionen auf und vergleichen Sie.\n\n\n\nDie Ergebnisse sollten mit der folgenden Abbildung kompatibel sein.\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionsparameter für das jeweilige \"Klassenmodell\"Modelbeta_0beta_1119.086-0.552219.212-0.266319.310-1.397424.9371.087524.6160.087Note. 1 = Klasse 1; 2 = Klasse 2; etc.\n\n\n\n\n\n\nVergleichen Sie nun die Ergebnisse der einzelnen Regressionen mit den Ergebnissen eines “Gesamtregressionsmodells” (ein Modell über alle Klassen hinweg).\nHier sind die Ergebnisse:\n\nges_mod &lt;- lm(data = data_schulleistung,\n              leistung ~  unlust_c)\n\nges_df &lt;- data.frame(model = 'gesamt',\n                     beta_0 = summary(ges_mod)$coefficients[1],\n                     beta_1 = summary(ges_mod)$coefficients[2])\n\nnice_table(ges_df,\n           col.format.custom = 2:3, format.custom = \"fun\",\n           title = 'Regressionsparameter für das \"Gesamtmodell\"',\n           width = 0.5)\n\n\nRegressionsparameter für das \"Gesamtmodell\"modelbeta_0beta_1gesamt21.053-1.128"
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression_parameters.html#bezug-zur-vorherigen-stunde",
    "href": "mlm_seminar_sose23/lineare_regression_parameters.html#bezug-zur-vorherigen-stunde",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "In den letzten Sitzungen haben wir uns mit der einfachen linearen Regression befasst. Dabei haben wir uns erst einmal dafür entschieden, eine Regression ohne Berücksichtigung der verschiedenen Subgruppen in den Daten zu berechnen.\nMit einem Prädiktor (\\(x_{1}\\) ), lautet die Formel der Regression:\n\\[\n\\widehat{y}_{m} = b_{0} + b_{1} \\cdot x_{m1}\n\\tag{1}\\]\nSie liefert uns zwei Parameter $b_{0} und \\(b_{1}\\).\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Ausprägung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Prädiktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt.\n\n\nUm die “Interpretierbarkeit” der Parameter (vorallem die des Interzepts.\nWir haben ebenfalls über “Zentrierung” gesprochen. Wenn wir eine Variable an ihren Mittelwert zentrieren, ziehen wir den Mittelwert der Variable von allen Werten (d.h. alle Beobachtungen) der Variable ab. Zentrierung definiert das Minimum und Maximum der Variable neu. Der Mittelwert einer am Mittelwert zentrierten Variable ist nun gleich Null. Wir können Zentrierung als eine “Daten-Vorverarbeitungsstrategie” betrachten, die die “Interpretierbarkeit” der Parameter (vor allem die des Intercepts) in einem linearen Modell steigert.\nBetrachten wir z.B. die folgende Abbildung:\n\n\n\n\n\n\n\n\n\nDargestellt sind die Ergebnisse zweier Regressionen. Beide Regressionen beschreiben den Zusammenhang zwischen Schulunlust und Schulleistung. Die linke Seite der Abbildung zeigt die Ergebnisse der Regression ohne Zentrierung von Schulunlust und die rechte Seite mit Zentrierung. Ein Unterschied ist, dass die Spannweite der zentrierten Daten ein anderer ist. Personen, die einen negativen Wert in der zeitrierten Schulunlust-Variable haben, befinden sich unterhalb des Mittelwert der Gesamtstichprobe. Personen mit positiven Werten befinden sich darüber. Ein weiterer Unterschied fällt auf, wenn wir die Intercepts der Modelle betrachten. Das Interzept auf der rechten Seite liegt inmitten der Verteilung (auf dem Mittelwert, welcher nach Zentrierung gleich Null ist). Zentrierung kann uns also helfen, das Interzept in einem für uns interpretierbaren Bereich “zu holen”."
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression_parameters.html#berücksichtigung-von-subgruppen",
    "href": "mlm_seminar_sose23/lineare_regression_parameters.html#berücksichtigung-von-subgruppen",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Gehen wir nun zu einem Beispiel über, das die Subgruppen in den Daten berücksichtigt. Dafür benötigen wir die folgenden Daten:\n\n# die Daten können mit diesem Befehl geladen werden\nurlRemote &lt;- 'https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main'\nfpathData &lt;- '/data/schulunlust.txt'\ndata_schulleistung &lt;- read.table(paste0(urlRemote, fpathData),\n                          header = TRUE, dec = ',')\n\n# wir werden nur die ersten 5 klassen benutzen\n# wir können die Daten dieser Klassen mit\n# `dplyr`filtern\nrequire(dplyr)\ndata_schulleistung &lt;- data_schulleistung %&gt;%\n  filter(klasse_nr &lt;= 5) %&gt;%\n  # ebenfalls werden wir die variable `unlust`\n  # zentrieren\n  mutate(unlust_c = unlust - mean(unlust))\n\nDiese Daten sind identisch mit den Daten der obigen Abbildung.\n\n\n\nErster Schritt:\n\nBerechnen wir erst einmal eine Gesamt-Regression, ohne die Subgruppen (die einzelnen Klassen) in den Daten zu berücksichtigen.\nZur Erinnerung: Der R-Befehl, um eine lineare Regression zu berechnen lautet lm(fomula, data)\n\nZweiter Schritt:\n\nBerechnen wir 5 verschiedene Regressionen, eine für jede Klasse.\nTipp: Sie können die Daten der einzelnen Klassen mit dplyr() filtern.\n\nz.B.: klasse_1 &lt;- data_schulleistung %&gt;% filter(klasse_nr == 1)\nDanach können Sie die Regression mit lm() berechnen.\n\n\nSchreiben Sie \\(b_{0}\\) und \\(b_{1}\\) für alle Regressionen auf und vergleichen Sie.\n\n\n\nDie Ergebnisse sollten mit der folgenden Abbildung kompatibel sein.\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionsparameter für das jeweilige \"Klassenmodell\"Modelbeta_0beta_1119.086-0.552219.212-0.266319.310-1.397424.9371.087524.6160.087Note. 1 = Klasse 1; 2 = Klasse 2; etc.\n\n\n\n\n\n\nVergleichen Sie nun die Ergebnisse der einzelnen Regressionen mit den Ergebnissen eines “Gesamtregressionsmodells” (ein Modell über alle Klassen hinweg).\nHier sind die Ergebnisse:\n\nges_mod &lt;- lm(data = data_schulleistung,\n              leistung ~  unlust_c)\n\nges_df &lt;- data.frame(model = 'gesamt',\n                     beta_0 = summary(ges_mod)$coefficients[1],\n                     beta_1 = summary(ges_mod)$coefficients[2])\n\nnice_table(ges_df,\n           col.format.custom = 2:3, format.custom = \"fun\",\n           title = 'Regressionsparameter für das \"Gesamtmodell\"',\n           width = 0.5)\n\n\nRegressionsparameter für das \"Gesamtmodell\"modelbeta_0beta_1gesamt21.053-1.128"
  },
  {
    "objectID": "mlm_seminar_sose23/hierarchische_daten.html",
    "href": "mlm_seminar_sose23/hierarchische_daten.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Achtung 🚧\n\n\n\nDies ist ein “lebendiges” Dokument. Es ist möglich, dass einige Aktualisierungen und Ergänzungen nach der Sitzung vorgenommen werden."
  },
  {
    "objectID": "mlm_seminar_sose23/hierarchische_daten.html#ihre-aufgabe",
    "href": "mlm_seminar_sose23/hierarchische_daten.html#ihre-aufgabe",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "Ihre Aufgabe",
    "text": "Ihre Aufgabe\n\nFinden Sie sich in Ihren Gruppen zusammen und überlegen Sie sich ein passendes Beispiel.\n\n\n\nWo könnten hierarchische Daten in Ihrem Bereich vorkommen?\nWie würden diese aussehen?\n\nWie viele Ebenen wären angemessen, um einen repräsentativen Eindruck der Variation zwischen den Beobachtungen zu erhalten?\n\nWelche Probleme würden sich ergeben, wenn Sie eine oder mehrere Beobachtungsebenen außer Acht lassen würden?\n\nAuf welcher Ebene findet die meiste Variation statt?\n\nZeichnen Sie ein Bild der hierarchischen Datenstruktur.\n\n\n\nNutzen Sie diese Anregungen, um in Ihrer Gruppe das Konzept der hierarchischen Daten besser zu verstehen und auf Ihre spezifischen Anwendungsfälle anzuwenden. Diskutieren Sie die verschiedenen Aspekte und identifizieren Sie mögliche Herausforderungen, die sich aus der Verwendung hierarchischer Daten in Ihrem Kontext ergeben könnten.\n\n\n\n\n−+\n20:00"
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html",
    "href": "mlm_seminar_sose23/lineare_regression.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Das Ziel einer Regression besteht darin, eine Variable durch eine oder mehrere andere Variablen vorherzusagen. An dieser Stelle können wir von einer Art Prognose sprechen. Wenn wir Regressionsmodelle benutzen, wollen wir den Verlauf einer Variable (die abhängige Variable) anhand anderer Variablen (unabhängige Variablen) prognostizieren.\nDie vorhergesagte Variable wird Kriterium, Regressand oder auch abhängige Variable (AV) genannt und wird üblicherweise mit \\(y\\) symbolisiert. Die Variablen, die zur Vorhersage der abhängigen Variablen verwendet werden, werden Prädiktoren, Regressoren oder unabhängige Variablen (UV) genannt. Üblicherweise werden Prädiktoren mit \\(x_{1},~x_{2},~x_{3},~\\dots\\) (oder kurz, mit \\(X\\)) symbolisiert.\nSie erinnern sich wahrscheinlich an Statistik 1 und 2, wo wir meistens nur einen Prädiktor zur Vorhersage einer anderen Variable benutzt haben. In diesem Fall sprechen wir von einer einfachen Regression. In diesem Fall können wir eine Vorhersage von \\(y\\) mit der Gleichung der einfachen linearen Regression formalisieren:\n\\[\ny_{m} = b_{0} + b_{1} \\cdot x_{m1} + b_{2} \\cdot x_{m2} + \\dots + e_{m}\n\\tag{1}\\]\nHier steht der Index \\(\\mathbf{m}\\) für die Untersuchungseinheit (z.B. für eine Person, oder eine einzelne Messung).\nUm die Grundidee eines Regressionsmodells besser zu verstehen, schauen wir uns nun ein kleines Beispiel an.\n\n\n\nBetrachten Sie die folgende Abbildung:\n\n\n\nAnstieg des Meeresspiegels seit dem Jahr 1993 bis 2021 in Millimeter (Stand: September). In Statista. Zugriff am 15. Mai 2023, von https://de.statista.com/statistik/daten/studie/1056576/umfrage/hoehe-des-meeresspiegels/\n\n\n\n\n\n\n\n\nWas können wir aus dieser Abbildung lernen?\n\n\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Anhand der abgebildeten Zahlen lässt sich leicht erkennen, dass seit 1993 der Meeresspiegel pro Jahr um mehrere Millimeter angestiegen ist.\n\n\n\n\nGehen wir davon aus, dass wir die Veränderung im Meeresspiegel, die durch die Abbildung angedeutet wird, möglichst prägnant zusammenfassen wollen. An dieser Stelle stehen uns unterschiedliche Methoden zur Verfügung. Wir könnten sagen:\n\n\n\n“Der Meeresspiegel ist seit 1993 gestiegen”, oder “Der Meeresspiegel steigt seit 1993 um mehrere Millimeter”.\n\n\n\nSobald wir aber eine Aussage über die Ausprägung dieser Veränderung treffen möchten, bräuchten wir eine Methode, die uns erlaubt all die kleinen Veränderungen, welche von Jahr zu Jahr gemessen wurden, zusammenzufassen. Eine Möglichkeit wäre, die gemessenen Abweichungen zu mitteln. Damit könnten wir feststellen, um wie viel der Meeresspiegel im Mittel pro Jahr angestiegen (oder abgefallen ist). Lasst uns diese Berechnung in R durchführen.\n\n\n\n\n# die Daten können mit diesem Befehl geladen werden\nmeeresspiegel &lt;- read.csv(file=\"https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main/data/meeresspiegel.csv\")\n\n\n\n\n\n# wir können `dplyr` benutzen \n# um die mittlere Abweichung zu berechnen\nrequire(dplyr)\n\nmeeresspiegel %&gt;%\n  # Differenz zwischen zwei aufeinaderefolgende Werte\n  mutate(Abweichung = lead(Anstieg) - Anstieg) %&gt;%\n  # bilde Mittelwert (schließ Fehlendewerte aus)\n  summarize(Mittlere_Abweichung = mean(Abweichung, na.rm = TRUE))\n\n  Mittlere_Abweichung\n1            3.607143\n\n\n\nNun können wir die Abweichung im Meeresspiegel genauer Beschreiben:\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Tatsächlich steigt der Meeresspiegel im Vergleich zu 1993 pro Jahr um durchschnittlich 3,6 mm.\n\n\n\n\n\n\nAnhand dieser Zahlen liese sich gleich prognostizieren, wie der Meerespiegel in den komenden Jahren verändern wird. Wenn alles so bleibt wie bisher, können wir davon ausgehen, dass der Meeresspiegel weiteransteigen wird. Das sind keine gute Nachrichten.\nEine gute Nachricht ist allerdings, dass lineare Regressionsmodelle, im Prinzip nichts anderes als die mittlere Abweichung zu berechnen, um eine Vorhersage von \\(y\\) (in unser Beispiel die Abweichung im gemessenen Meeresspiegel) anhand von \\(x\\) (in unser Beispiel, das Jahr der Messung) vorzunehmen. Die genauen Berechnungsschritte sind ein kleines bisschen komplexer, aber die Logik ist im Prinzip die gleiche.\nWir können dies leicht mit einem linearen Regression in R überprüfen.\n\n\nWir können eine lineare Regression mit der Funktion lm() in R berechnen.\n\n# lm() nimmt mehrere Argumente. Heute brauchen wir `data` und `formula`\n# `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\nlin_reg &lt;- lm(\n  # `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\n  data = meeresspiegel, \n  # `formula = 1 +  Anstieg ~ Jahr` sagt der Funktion wie die Formel unserer\n  # Regression aussehen soll\n              formula = 1 +  Anstieg ~ Jahr)\n\nMit summary() können wir uns die Ergebnisse der Regression anzeigen lassen:\n\nsummary(lin_reg)\n\n\nCall:\nlm(formula = 1 + Anstieg ~ Jahr, data = meeresspiegel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4157  -2.7138   0.3881   2.3164   7.7297 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.728e+03  1.882e+02  -35.75   &lt;2e-16 ***\nJahr         3.375e+00  9.375e-02   36.00   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.224 on 27 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9788 \nF-statistic:  1296 on 1 and 27 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nWie kommen diese Zahlen zustande?\nWie können wir aus vergangenen Beobachtungen zukünftige Beobachtungen vorhersagen?\nWas ist eine gute Vorhersage, und warum sind Vorhersagen mehr oder weniger akkurat?\n\n\nSchauen wir uns ein weiteres Beispiel an.\n\n\n\n\n\nStellen Sie sich vor, dass Sie als ambulante Psychotherapeut:in in einer Praxis arbeiten. Am Anfang eines jeden Quartals erstellen Sie eine Kostenplanung. Die erwartete Anzahl an Behandlungsstunden spielt dabei eine große Rolle, da Sie damit den erwarteten Wert Ihres Einkommens berechnen können. Gehen wir davon aus, dass Sie pro Patient:in ein Betrag von 66,6 Euro (nach Abzug aller laufenden Kosten) in der Quartalsabrechnung erwarten können.\n\nBetrachten Sie die folgende Abbildung:\n\n\nDie Abbildung zeigt einen linearen Zusammenhang zwischen der Anzahl an Patienten und dem Betrag auf der Abrechnung. Wir können dieser Zusammenhang bildlich darstellen (wie in der Abbildung) aber auch mithilfe mit eines Modells.\n\n\\[Betrag = (wie~von~x~auf~y~kommen?) + Fehler\\]\n\nIn unseren Beispiel, welche Funktion muss auf \\(x\\) angewendet werden, um eine möglichst genaue Schätzung von \\(y\\) zu bekommen?\nWir können diese Fragen mithilfe einer mathematischen Formel ausdrucken.\n\n\\[y = f(x) + Fehler\\]\n\nDies ist die Grundlage eines Regressionsmodells. Regressionsmodelle sind nichts anderes als eine Speziefietzierung dieser Frage. Mit einem Regressionsmodell, können wir die Funktion, die auf \\(x\\) angewendet werden muss, genauer beschreiben.\nDies ist die allgemeine Formel der Regression:\n\n\\[y_{i} = b_{0} + b_{1}~x_{1} + e_{i}\\]\n\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Ausprägung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Prädiktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt\n\n\\(e_{i} =\\) Regressionsresiduum (kurz: Residuum), Residualwert oder Fehlerwert:\n\nDie Differenz zwischen einem vorhergesagten (\\(\\hat{y}\\)) und beobachteten (\\(y\\)) \\(y\\)-Wert.\nJe größer die Fehlerwerte, umso größer ist die Abweichung eines beobachteten vom vorhergesagten Wert."
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html#beispiel-1-der-meeresspiegel-steigt-an",
    "href": "mlm_seminar_sose23/lineare_regression.html#beispiel-1-der-meeresspiegel-steigt-an",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Betrachten Sie die folgende Abbildung:\n\n\n\nAnstieg des Meeresspiegels seit dem Jahr 1993 bis 2021 in Millimeter (Stand: September). In Statista. Zugriff am 15. Mai 2023, von https://de.statista.com/statistik/daten/studie/1056576/umfrage/hoehe-des-meeresspiegels/\n\n\n\n\n\n\n\n\nWas können wir aus dieser Abbildung lernen?\n\n\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Anhand der abgebildeten Zahlen lässt sich leicht erkennen, dass seit 1993 der Meeresspiegel pro Jahr um mehrere Millimeter angestiegen ist.\n\n\n\n\nGehen wir davon aus, dass wir die Veränderung im Meeresspiegel, die durch die Abbildung angedeutet wird, möglichst prägnant zusammenfassen wollen. An dieser Stelle stehen uns unterschiedliche Methoden zur Verfügung. Wir könnten sagen:\n\n\n\n“Der Meeresspiegel ist seit 1993 gestiegen”, oder “Der Meeresspiegel steigt seit 1993 um mehrere Millimeter”.\n\n\n\nSobald wir aber eine Aussage über die Ausprägung dieser Veränderung treffen möchten, bräuchten wir eine Methode, die uns erlaubt all die kleinen Veränderungen, welche von Jahr zu Jahr gemessen wurden, zusammenzufassen. Eine Möglichkeit wäre, die gemessenen Abweichungen zu mitteln. Damit könnten wir feststellen, um wie viel der Meeresspiegel im Mittel pro Jahr angestiegen (oder abgefallen ist). Lasst uns diese Berechnung in R durchführen.\n\n\n\n\n# die Daten können mit diesem Befehl geladen werden\nmeeresspiegel &lt;- read.csv(file=\"https://raw.githubusercontent.com/JoseAlanis/amdstatsem/main/data/meeresspiegel.csv\")\n\n\n\n\n\n# wir können `dplyr` benutzen \n# um die mittlere Abweichung zu berechnen\nrequire(dplyr)\n\nmeeresspiegel %&gt;%\n  # Differenz zwischen zwei aufeinaderefolgende Werte\n  mutate(Abweichung = lead(Anstieg) - Anstieg) %&gt;%\n  # bilde Mittelwert (schließ Fehlendewerte aus)\n  summarize(Mittlere_Abweichung = mean(Abweichung, na.rm = TRUE))\n\n  Mittlere_Abweichung\n1            3.607143\n\n\n\nNun können wir die Abweichung im Meeresspiegel genauer Beschreiben:\n\n\n\nDie Abbildung zeigt, dass der Meeresspiegel kontinuierlich ansteigt. Tatsächlich steigt der Meeresspiegel im Vergleich zu 1993 pro Jahr um durchschnittlich 3,6 mm."
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html#ist-die-mittlere-abweichung-ein-guter-schätzer",
    "href": "mlm_seminar_sose23/lineare_regression.html#ist-die-mittlere-abweichung-ein-guter-schätzer",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Anhand dieser Zahlen liese sich gleich prognostizieren, wie der Meerespiegel in den komenden Jahren verändern wird. Wenn alles so bleibt wie bisher, können wir davon ausgehen, dass der Meeresspiegel weiteransteigen wird. Das sind keine gute Nachrichten.\nEine gute Nachricht ist allerdings, dass lineare Regressionsmodelle, im Prinzip nichts anderes als die mittlere Abweichung zu berechnen, um eine Vorhersage von \\(y\\) (in unser Beispiel die Abweichung im gemessenen Meeresspiegel) anhand von \\(x\\) (in unser Beispiel, das Jahr der Messung) vorzunehmen. Die genauen Berechnungsschritte sind ein kleines bisschen komplexer, aber die Logik ist im Prinzip die gleiche.\nWir können dies leicht mit einem linearen Regression in R überprüfen.\n\n\nWir können eine lineare Regression mit der Funktion lm() in R berechnen.\n\n# lm() nimmt mehrere Argumente. Heute brauchen wir `data` und `formula`\n# `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\nlin_reg &lt;- lm(\n  # `data = meeresspiegel` sagt der Funktion wo sich die Daten befinden\n  data = meeresspiegel, \n  # `formula = 1 +  Anstieg ~ Jahr` sagt der Funktion wie die Formel unserer\n  # Regression aussehen soll\n              formula = 1 +  Anstieg ~ Jahr)\n\nMit summary() können wir uns die Ergebnisse der Regression anzeigen lassen:\n\nsummary(lin_reg)\n\n\nCall:\nlm(formula = 1 + Anstieg ~ Jahr, data = meeresspiegel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4157  -2.7138   0.3881   2.3164   7.7297 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.728e+03  1.882e+02  -35.75   &lt;2e-16 ***\nJahr         3.375e+00  9.375e-02   36.00   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.224 on 27 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9788 \nF-statistic:  1296 on 1 and 27 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nWie kommen diese Zahlen zustande?\nWie können wir aus vergangenen Beobachtungen zukünftige Beobachtungen vorhersagen?\nWas ist eine gute Vorhersage, und warum sind Vorhersagen mehr oder weniger akkurat?\n\n\nSchauen wir uns ein weiteres Beispiel an."
  },
  {
    "objectID": "mlm_seminar_sose23/lineare_regression.html#beispiel-2-more-patients-more-cash",
    "href": "mlm_seminar_sose23/lineare_regression.html#beispiel-2-more-patients-more-cash",
    "title": "Seminar Fortgeschrittene statistische Methoden II (1)",
    "section": "",
    "text": "Stellen Sie sich vor, dass Sie als ambulante Psychotherapeut:in in einer Praxis arbeiten. Am Anfang eines jeden Quartals erstellen Sie eine Kostenplanung. Die erwartete Anzahl an Behandlungsstunden spielt dabei eine große Rolle, da Sie damit den erwarteten Wert Ihres Einkommens berechnen können. Gehen wir davon aus, dass Sie pro Patient:in ein Betrag von 66,6 Euro (nach Abzug aller laufenden Kosten) in der Quartalsabrechnung erwarten können.\n\nBetrachten Sie die folgende Abbildung:\n\n\nDie Abbildung zeigt einen linearen Zusammenhang zwischen der Anzahl an Patienten und dem Betrag auf der Abrechnung. Wir können dieser Zusammenhang bildlich darstellen (wie in der Abbildung) aber auch mithilfe mit eines Modells.\n\n\\[Betrag = (wie~von~x~auf~y~kommen?) + Fehler\\]\n\nIn unseren Beispiel, welche Funktion muss auf \\(x\\) angewendet werden, um eine möglichst genaue Schätzung von \\(y\\) zu bekommen?\nWir können diese Fragen mithilfe einer mathematischen Formel ausdrucken.\n\n\\[y = f(x) + Fehler\\]\n\nDies ist die Grundlage eines Regressionsmodells. Regressionsmodelle sind nichts anderes als eine Speziefietzierung dieser Frage. Mit einem Regressionsmodell, können wir die Funktion, die auf \\(x\\) angewendet werden muss, genauer beschreiben.\nDies ist die allgemeine Formel der Regression:\n\n\\[y_{i} = b_{0} + b_{1}~x_{1} + e_{i}\\]\n\n\n\\(b_{0} =\\) \\(y\\)-Achsenabschnitt, Konstante, oder Interzept:\n\nDer Wert von \\(y\\) bei einer Ausprägung von 0 in \\(x\\).\n\n\\(b_{1} =\\) Regressionsgewicht des Prädiktors oder die Steigung der Regressionsgerade.\n\nInterpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten \\(y\\) zunimmt, wenn \\(x\\) um eine Einheit zunimmt\n\n\\(e_{i} =\\) Regressionsresiduum (kurz: Residuum), Residualwert oder Fehlerwert:\n\nDie Differenz zwischen einem vorhergesagten (\\(\\hat{y}\\)) und beobachteten (\\(y\\)) \\(y\\)-Wert.\nJe größer die Fehlerwerte, umso größer ist die Abweichung eines beobachteten vom vorhergesagten Wert."
  },
  {
    "objectID": "about/about_jan.html",
    "href": "about/about_jan.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "",
    "text": "\\[\\text{Wiener}(y|\\alpha, \\tau, \\beta, \\delta) =\n\\frac{\\alpha^3}{(y-\\tau)^{3/2}} \\exp \\! \\left(- \\delta \\alpha \\beta -\n\\frac{\\delta^2(y-\\tau)}{2}\\right) \\sum_{k = - \\infty}^{\\infty} (2k +\n\\beta) \\phi \\! \\left(\\frac{2k \\alpha + \\beta}{\\sqrt{y - \\tau}}\\right)\\]\nHier finden Sie alle nötigen Informationen zum Seminar, sowie alle Skripte, Aufgaben und Links zur notwendigen Software."
  },
  {
    "objectID": "about/about_jan.html#allgemeine-materialien-für-das-seminar",
    "href": "about/about_jan.html#allgemeine-materialien-für-das-seminar",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Allgemeine Materialien für das Seminar",
    "text": "Allgemeine Materialien für das Seminar\n\nWichtige Informationen werden auf dem Teams Channel des Seminars veröffentlicht:\n\nBeitrittslink zum Teams Channel\nKursmaterialien wie Literatur, R-Skripte und Präsentationen finden Sie hier auf unserer Website\n\nWann: Montags von 16:15 - 17:45 Uhr (17.04.22 - Mo. 17.07.23).\nWo: Raum 01-236 (CIP Pool) im Psychologischen Institut (Binger Str.)"
  },
  {
    "objectID": "about/about_jan.html#inhalt-des-seminars",
    "href": "about/about_jan.html#inhalt-des-seminars",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Inhalt des Seminars",
    "text": "Inhalt des Seminars\n\nMathematische Modelle kognitiver Prozesse sind ein machtvolles Werkzeug, um spezifische kognitive Prozesse wie beispielsweise Verarbeitungsgeschwindigkeit, exekutive Funktionen oder Arbeitsgedächtniskapazität genauer abzubilden. Die mathematische Formalisierung dieser Prozesse ermöglicht es, verbale Theorien in empirisch testbare Modelle zu überführen, die eine Ableitung und Überprüfung spezifischer Hypothesen und Vorhersagen über bspw. experimentelle Effekte ermöglichen. Ein Beispiel für ein sehr erfolgreich angewandtes Modell in der kognitiven Forschung ist das Diffusionsmodell, welches die Verarbeitungsgeschwindigkeit von einfachen Wahlreaktionszeitaufgaben modelliert. Das Seminar wird einen Überblick über die theoretischen Grundlagen und gängigen Modelle zur mathematischen Modellierung verschiedener Arbeitsgedächtnisprozesse geben. Im praktischen Teil des Seminars werden Anhand unterschiedlicher kognitiver Modelle wie des Diffusionsmodells die Modellimplementierung in R / STAN, die Schätzung anhand empirischer Daten sowie die Bewertung und Interpretation der geschätzten Modellparameter eingeübt.\n\n\nFahrplan\n\n\n\nKW\nThema Seminar\nLink\nMaterialien\n\n\n\n\n16\nOrganisation und Ablauf\nZusammenfassung\n\n\n\n17\nEinführung I: Grundlagen der Modellierung\n\n \n\n\n19\nEinführung II: Grundlagen der Modellierung\n\n  \n\n\n20\nParameterschätzung I: Diskrepanzfunktionen & Schätzalgorithmen\n\n  \n\n\n21\nParameterschätzung II: Maximum Likelihood & Beyond\n\n \n\n\n23\nParameterschätzung III: Hands On in R Parameter Estimation\n\n\n\n\n24\nAdvanced R for Cognitive Modeling\n\n\n\n\n25\nMultinomial Processing Tree Models (Theorie)\n\n\n\n\n26\nAnwendung von MPT Modellen (R-Sitzung)\n\n\n\n\n27\nDrift Diffusion Models (Theory)\n\n\n\n\n28\nMemory Measurement Model (M3)\n\n\n\n\n29\nAnwendung des M3 Modells\n\n\n\n\n30\nAbgabe des kompletten Portfolios"
  },
  {
    "objectID": "about/about_jan.html#begleitendes-tutorium",
    "href": "about/about_jan.html#begleitendes-tutorium",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Begleitendes Tutorium",
    "text": "Begleitendes Tutorium\n\nDas Seminar wird von einem Tutorium (Softwarekurs) begleitet. Im Tutorium werden Sie die Grundlagen des Statistiksoftware und Programmiersprache R. Sie werden in dem Umgang mit Daten geschult und werden verschiedene Techniken zur Datenaufbereitung erlernen und einüben. Dieser Teil des Tutoriums findet im ersten Teil des Semesters statt. Sie sollten für eines der beiden Tutorien angemeldet sein, entweder am Montag oder am Mittwoch. Besuchen das Tutorium! Erfahrungsgemäß können viele Fragen und Startschwierigkeiten im Umgang mit R im Tutorium gut aufgefangen und gelöst werden.\nIm zweiten Teil des Semesters findet ein vertiefendes Tutorium statt. Dieses baut auf den Grundkenntnissen, die Sie im ersten Teil des Semesters erlernt haben werden, auf. Im zweiten Teil des Semesters besuchen Sie dieses Tutorium entweder bei Hr. Alanis oder Fr. Hülsemann.Die Termine für die Tutorien ändern sich nicht. Sie müssen das Tutorium nicht wechseln, alle R Inhalte die spezifisch für das Seminar sind, werden an einem Termin im Seminar behandelt und vertieft. Dieser Termin wir auf den Inhalten der bisherigen Tutorien aufbauen.\nWir haben eine Reihe von Materialien zusammengestellt, um Ihnen den Einstieg in die Programmiersprache R zur erleichtern. Diese können Sie unter dem folgenden Link erreichen:\n\n\n\nR-Kurs-Buch von der Abteilung Analyse und Modellierung komplexer Daten."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lehre@AMD",
    "section": "",
    "text": "Willkommen!\nHerzlich Willkommen zur Lehre-Homepage der Abteilung Analyse und Modellierung komplexer Daten an der Johannes Gutenberg-Universität Mainz.\nAktuell finden Sie hier die Materialien für zwei Seminare zu fortgeschrittene statistische Methoden II im Sommersemester 23.\n\n\nAktuelle Seminare\n\n\n\n\n\n\n\n\n\n\nSeminar Fortgeschrittene statistische Methoden II (1)\n\n\nTermin 1: Allgemeine Informationen\n\n\n\nJosé C. García Alanis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeminar Fortgeschrittene statistische Methoden II (3)\n\n\nKognitive Modellierung\n\n\n\nJan Göttmann\n\n\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#fortgeschrittene-dplyr-funktionen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#fortgeschrittene-dplyr-funktionen",
    "title": "Advanced Dplyr",
    "section": "",
    "text": "Dplyr bietet neben dem berechnen von neuen Variablen und der Möglichkeit Datensätze effizient zusammenzufassen noch viele weitere Funktionen, die es ermöglichen tiefergehende Änderungen an einem Datensatz vorzunehmen. Dazu gehören bspw. die Funktionen case_when() und pivot_longer() bzw. pivot_wider(). Case_when() kann dazu genutzt werden, Variablen in Abhängigkeit von bestimmten Bedingungen umzuformen und wird in der Regel zusammen mit mutate() verwendet. pivot_longer() bzw. pivot_wider() wird dazu genutzt einen breiten (wide) Datensatz in einen langen (long) Datensatz umzuwandeln oder umgekehrt."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#case_when---if-else-verknüpfüngen-für-multiple-bedingungen",
    "title": "Advanced Dplyr",
    "section": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen",
    "text": "2 case_when() - if-else Verknüpfüngen für multiple Bedingungen\nDie case_when Funktion in dplyr ermöglicht es, basierend auf bestimmten Bedingungen verschiedene Werte für eine Spalte in einem Datensatz auszuwählen und abhängig vom Variablenwert einen neue Variable zu erstellen. Es ist eine sehr einfache Möglichkeit, if-else Statements in eine dplyr-Pipeline zu integrieren.\nDie Syntax eines case_when() Befehles entspricht einer einfachen if oder if-else Verknüpfung. Eine if-else Verknüpfung prüft eine Bedingung und führt wenn diese Erfüllt ist einen definierten Befehl aus. Ist die Bedingung nicht erfüllt, wird andernfalls (else) ein anderer Befehl ausgeführt:\n\n\n# Vektor von Zahlen darauf testen, ob sie gerade oder ungerade sind:\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei geteilt keinen Rest hat, dann schreibe Gerade, ansonsten Ungerade.\n\nifelse(zahlen %% 2 == 0,\"Gerade\",\"Ungerade\")\n\ncase_when() funktioniert nach der Gleichen Logik:\n\n\n\nDabei wird immer zuerst die if-Bedinungen und deren Output definiert und anschließend mit TRUE die else Bedingung, welche angibt was passiert wenn die if - Bedingung nicht zutrifft:\n\n\n\nzahlen &lt;- c(seq(1:10))\n\n# Wenn die aktuelle Zahl des Vektors durch zwei\n# geteilt keinen Rest hat, dann schreibe Gerade, \n# ansonsten Ungerade.\n\ncase_when(zahlen %% 2 == 0 ~ \"Gerade\",\n          TRUE ~\"Ungerade\")\n\nDer Unterschied von case_when() und ifelse() ist, dass mit case_when() auch mehrere Bedingungen definiert werden können (z.B. wenn eine Variable in mehrere Kategorien eingeteilt werden soll):\n\nHier gibt es auch ein finales else Statement, dieses muss aber nicht sein, solange die Kategorien alle Fälle abdecken!\n\n2.0.1 case_when: Beispiele\nDas folgende ist ein einfaches Beispiel, bei dem case_when() verwendet wird, um in einem Datensatz eine neue Spalte Alter_Kategorie anhand des Werts in der Spalte Alter zu kategorisieren. Wenn eine neue Spalte erstellt werden soll, muss case_when() immer mit mutate() kombiniert werden:\n\n# Beispiel Datensatz erstellen\ndf &lt;- data.frame(Name = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45))\nhead(df)\n\n# mutate und case_when verwenden, um neue Variable in Abhängigkeit vom Alter zu erstellen:\ndf %&gt;% \n  mutate(Alter_Kategorie = case_when(\n    Alter &lt;= 25 ~ \"jung\",\n    Alter &gt; 25 & Alter &lt;= 35 ~ \"mittel\",\n    Alter &gt; 35 ~ \"alt\"\n  ))\n\nEin weiteres Beispiel haben wir letzte Woche bei der MPT-Modellierung der Daten von Frenken et al. gesehen. Im Datensatz ist shoot als 0 und not shoot als 1 kodiert. Für die MPT Modellierung benötigen wir aber für jede Kategorie (Black / Gun, Black / Phone, White / Gun, White / Phone) die Hits und Misses, also die Fehlerraten. Der Datensatz hat praktischerweise eine Spalte, die genau kodiert, was die VP gesehen hat:\n\n\nhead(Study_2_dm)\n\n# A tibble: 6 × 6\n  subj_idx stimulus stim     rt response condition\n     &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt; &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;    \n1        0 gun      bg    0.506        0 black    \n2        0 phone    wp    0.437        0 white    \n3        0 phone    wp    0.483        0 white    \n4        0 phone    bp    0.594        1 black    \n5        0 phone    wp    0.552        1 white    \n6        0 gun      wg    0.417        1 white    \n\n\nIn der zweiten Spalte stimulus ist angebenen, welches Objekt der jeweils zusammen mit der Hautfarbe gezeigt wurde. Da wir wissen, dass nur bei “gun” geschossen werden darf, können wir nun in Kombination mit response (0 - shoot, 1 - not shoot) die Hits mit case_when() in einer neuen Spalte ACC kodieren.\nDazu Verknüpfen wir hier zwei Bedingungen mit dem & Operator. Die erste Bedingung bezieht sich auf die Spalte stimulus, hier muss geprüft werden, welcher Stimulus gezeigt wurde (Gun vs. Phone). Die zweite Bedingung bezieht sich auf die Spalte response. Hier muss geprüft werden, ob geschossen wurde oder nicht. Ingesamt müssen also vier Bedinungen definiert werden, für jede Kombination von Objekt und Response:\n\nAccuracy in Abhängigkeit von Gegebener Response und Stimulus\n\n\nStimulus\nResponse\nAccuracy\n\n\n\n\nGun\n0\n1\n\n\nGun\n1\n0\n\n\nPhone\n0\n0\n\n\nPhone\n1\n1\n\n\n\nDies müssen wir nun in ein case_when() Befehl übernehmen\n\nMit mutate() die neue Outputspalte benennen\nInnerhalb von mutate() mit case_when() die Accuracy in Abhängigkeit der Spalten Stimulus und Response umkodieren:\n\n\n\nfreq_dat &lt;- Study_2_dm %&gt;% mutate(ACC = case_when(stimulus == \"gun\" & response == 0 ~ 1,\n                                                  stimulus == \"gun\" & response == 1 ~ 0,\n                                                  stimulus == \"phone\" & response == 1 ~ 1,\n                                                  stimulus ==\"phone\" & response == 0 ~ 0))\n\nAnschließend müssen wir nun die Hits und Misses auszählen. Generell eignen sich zum Auszählen von bestimmten Bedingungskombinationen oder Trials die Funktionen group_by(), summarise() und n(). n() ist eine einfache Zählfunktion, welche innerhalb von group_by() %&gt;% summarise() dazu führt, dass die alle Beobachtungen der gruppierten Variablen (z.B. Subject & Bedingung) innerhalb von summarise() gezählt werden.\nAlso zum Beispiel, wieviele Beobachtung von Subject 1 gibt es in Bedingung A, B und C. Dies macht aber nur Sinn, wenn ihr Daten auf Trial-Ebene (also für jede Versuchsperson alle Antworten über das ganez Experiment) vorliegen habt. Dies ist hier der Fall, da für jede Person und jede Bedingung, die diese Person durchlaufen hat, die gegebenen Antworten im Datensatz in der Spalte responses vorliegen.\n\n\nfreq_dat &lt;- freq_dat %&gt;% group_by(subj_idx,stim) %&gt;% \n  summarise(hits = sum(ACC), \n            ntrials=n(), \n            miss=ntrials-hits)\n\n`summarise()` has grouped output by 'subj_idx'. You can override using the\n`.groups` argument.\n\nhead(freq_dat)\n\n# A tibble: 6 × 5\n# Groups:   subj_idx [2]\n  subj_idx stim   hits ntrials  miss\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n1        0 bg       30      31     1\n2        0 bp       28      29     1\n3        0 wg       23      27     4\n4        0 wp       28      33     5\n5        1 bg       28      30     2\n6        1 bp       16      18     2\n\n\nWas passiert hier genau ? Schritt für Schritt:\n\ngroup_by(subj_idx, stim) : Wir gruppieren zunächst nach Subject und Bedingung\n\nsummarise(...\n\nhits = sum(ACC) - Wir haben alle Hits mit case_when als 1 kodiert, also können wir die Hits einfach berechnen, indem wir diese aufsummieren.\nnTrials = n() - Auszählen wieviele Responses ein Subject in jeder stimulus Bedinungen gegeben hat (Gesamtanzahl von gegebenen Antworten in einer Bedingungen pro Person\nmiss = ntrials - hits Alles was kein Hit ist muss ein Miss sein ! Daher können wir einfach die Hits von der Gesamtzahl der Antworten abziehen und erhalten die Anzahl der Misses in jeder Bedinung pro Subject !"
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#long--und-wide-datenformat---datentransformation-in-dplyr",
    "title": "Advanced Dplyr",
    "section": "3 Long- und Wide-Datenformat - Datentransformation in dplyr",
    "text": "3 Long- und Wide-Datenformat - Datentransformation in dplyr\nNun haben wir die Daten von Frenken et al. neu kodiert und die nötigen Informationen im Datensatz, um mit MPTinR zu arbeiten. Allerdings liegen die Daten noch im sogenannten long - Format vor. Oftmals ist es jedoch notwendig, Daten entweder von einem long in ein wide Format oder umgekehrt zu transfomieren. Im Kontext von Datenanalyse und statistischer Modellierung gibt es in der Regel zwei Haupttypen von Datenformaten: wide-Format (breit-Format) und long-Format (lang-Format).\n\n3.1 Wide-Format\n\nIn einem wide Format sind die verschiedenen Merkmale einer einzigen Beobachtung in separaten Spalten dargestellt.\nEine einzige Zeile in einem breiten Datensatz repräsentiert eine Beobachtung.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, wäre in einem breiten Format dargestellt, wobei jede Spalte einem bestimmten Fach entspricht.\nVorteil: Es ist einfach, schnelle Übersichten über große Datenmengen zu erhalten.\n\nBeispiel:\n\nwide_df &lt;- data.frame(Schüler_ID = c(1, 2, 3), \n                      Mathe = c(89, 76, 92), \n                      Englisch = c(92, 88, 95), \n                      Wissenschaft = c(88, 72, 98))\nwide_df\n##   Schüler_ID Mathe Englisch Wissenschaft\n## 1          1    89       92           88\n## 2          2    76       88           72\n## 3          3    92       95           98\n\n\n\n3.2 Long-Format\n\nIm long Format sind alle Merkmale einer einzigen Beobachtung in einer Zeile dargestellt.\nEine einzige Spalte in einem long Datensatz repräsentiert ein bestimmtes Merkmal.\nBeispiel: Ein Datensatz, der Informationen über die Leistung von Schülern in verschiedenen Fächern (Mathematik, Englisch, Wissenschaft) enthält, könnte in einem langen Format dargestellt werden, wobei jede Zeile einer bestimmten Schüler-Fach-Kombination entspricht.\nVorteil: Es ist einfach, bestimmte Merkmale für verschiedene Beobachtungen zu vergleichen oder zu analysieren. Außerdem ist es für manche statistische Methoden, wie lineare Regression oder einer ANOVA, das bevorzugte Format.\n\nBeispiel:\n\nlong_df &lt;- data.frame(Schüler_ID = c(rep(1, 3), rep(2, 3), rep(3, 3)), \n                      Fach = c(rep(\"Mathe\", 3), rep(\"Englisch\", 3), rep(\"Wissenschaft\", 3)), \n                      Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\nlong_df\n##   Schüler_ID         Fach Note\n## 1          1        Mathe   89\n## 2          1        Mathe   92\n## 3          1        Mathe   88\n## 4          2     Englisch   76\n## 5          2     Englisch   88\n## 6          2     Englisch   72\n## 7          3 Wissenschaft   92\n## 8          3 Wissenschaft   95\n## 9          3 Wissenschaft   98\n\nEs ist wichtig, das Konzept des wide- und long-Formats zu verstehen, da es bei der Datenaufbereitung und Analyse eine wichtige Rolle spielt. Zum Beispiel kann ein wide -Datensatz schwer zu analysieren sein, wenn man Vergleiche zwischen bestimmten Merkmalen über mehrere Beobachtungen hinweg machen möchte (Beispiel Ergebnisse der Diffusionsmodellierung!). Hier ist es oft besser, den Datensatz in ein long-format zu bringen.\n\n\n3.3 pivot - Funktionen in dplyr\nDie Funktionen pivot_wider und pivot_longer gehören zu den Funktionen von dplyr und dienen dazu, Datensätze zu transformieren.\npivot_wider verwandelt einen long-Format Datensatz in einen wide-Format Datensatz, indem es die Werte einer bestimmten Spalte zu neuen Spalten umbenennt. Dies kann manuell durchgeführt werden, aber pivot_wider macht dies automatisch und erleichtert so die Datentransformation:\n\n\npivot_longer verwandelt einen wide-Format Datensatz in einen long-Format Datensatz, indem es die Spalten mit bestimmten Werten in einer neuen Spalte zusammenfasst.\nEin Beispiel für den Einsatz von pivot_wider:\n\n# long data example\nlong_data &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\", \"John\", \"Jane\", \"Jim\"),\n  subject = c(\"physics\", \"physics\", \"physics\", \"math\", \"math\", \"math\"),\n  score = c(85, 90, 80, 75, 80, 70)\n)\n\nlong_data\n## # A tibble: 6 × 3\n##   name  subject score\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 John  physics    85\n## 2 Jane  physics    90\n## 3 Jim   physics    80\n## 4 John  math       75\n## 5 Jane  math       80\n## 6 Jim   math       70\n\n\n# pivot to wide format\npivot_wider(long_data, names_from = subject, values_from = score)\n## # A tibble: 3 × 3\n##   name  physics  math\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n## 1 John       85    75\n## 2 Jane       90    80\n## 3 Jim        80    70\n\nEin Beispiel für den Einsatz von pivot_longer:\n\n# wide data example\nwide_df &lt;- tibble(\n  name = c(\"John\", \"Jane\", \"Jim\"),\n  Mathe_Note = c(85, 90, 80),\n  Englisch_Note = c(75, 80, 70)\n)\n\n\nwide_df\n## # A tibble: 3 × 3\n##   name  Mathe_Note Englisch_Note\n##   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n## 1 John          85            75\n## 2 Jane          90            80\n## 3 Jim           80            70\n\n\n# pivot to long format\npivot_longer(wide_df, cols = c(Mathe_Note, Englisch_Note), \n             names_to = \"Schüler\", values_to = \"Note\")\n## # A tibble: 6 × 3\n##   name  Schüler        Note\n##   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n## 1 John  Mathe_Note       85\n## 2 John  Englisch_Note    75\n## 3 Jane  Mathe_Note       90\n## 4 Jane  Englisch_Note    80\n## 5 Jim   Mathe_Note       80\n## 6 Jim   Englisch_Note    70\n\n\n\n3.4 Argumente für pivot_longer und pivot_wider\npivot_longer benötigt mindestens zwei Argumente:\n\ncols: Dies ist ein Zeichenvektor, der angibt, welche Spalten im Datensatz zusammengefasst werden sollen. -Schüler_ID bedeutet zum Beispiel, dass alle Spalten außer Schüler_ID zusammengefasst werden sollen.\nnames_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die zusammengefassten Werte gespeichert werden.\nvalues_to: Dies ist ein Zeichenvektor, der den Namen der neuen Spalte angibt, in der die Werte gespeichert werden, die aus den zusammengefassten Spalten stammen.\n\n\npivot_wider benötigt mindestens zwei Argumente:\n\nnames_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Namen für die neuen Spalten verwendet werden soll.\nvalues_from: Dies ist ein Zeichenvektor, der angibt, welche Spalte als Werte für die neuen Spalten verwendet werden soll.\n\n\nBeide Funktionen haben auch weitere optionale Argumente wie values_fill und names_prefix oder names_sep()um die Daten bei Bedarf weiter anzupassen.\nHier nun die Umformung der Daten von Frenken et al., welche wir vom long Format in das wide Format bringen müssen:\n\n\n#| echo: true\n#| output: true\n#| warning: false\n#| code-overflow: wrap\n#| collapse: true\n\nfreq_dat %&gt;% pivot_wider(names_from = c(\"stim\"),\n                         values_from = c(\"hits\",\"miss\"),\n                         id_cols=\"subj_idx\")\n\n# A tibble: 137 × 9\n# Groups:   subj_idx [137]\n   subj_idx hits_bg hits_bp hits_wg hits_wp miss_bg miss_bp miss_wg miss_wp\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1        0      30      28      23      28       1       1       4       5\n 2        1      28      16      27      11       2       2       2       1\n 3        3      25      10      33      23       0       0       1       1\n 4        4      35      24      27      33       1       0       0       0\n 5        5      31      28      25      23       1       2       4       4\n 6        6      34      26      18      31       0       0       5       0\n 7        7      29      24      21      36       2       0       1       1\n 8        8      22      29      25      28       0       2      10       2\n 9        9      28      23      22      22       2       0       2       1\n10       10      26      35      25      24       4       0       0       3\n# ℹ 127 more rows\n\n\nWas passiert hier genau ? Schritt für Schritt:\n\nnames_from = c(stim) - Die neuen Spalten sollen aus der Spalte stim benannt werden - (bg,bp,wp,wg)\nvalues_from = c(\"hits\",\"miss\") - in den neuen Spalten sollen die Werte der Hits und Miss Spalten stehen - durch den ersten und zweiten Schritt entstehen also Spalten, die jeweils hits_bg etc. enthalten. Also die Hits aus der Bedingung “bg” usw.\nid_cols - dies soll für jedes Subject einzeln geschehen."
  },
  {
    "objectID": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#übungen",
    "href": "cogmod_sem_sose23/R-Scripts/Advanced R/Advanced_R_stud.html#übungen",
    "title": "Advanced Dplyr",
    "section": "4 Übungen",
    "text": "4 Übungen\n\n4.1 case_when\n1.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Note_Kategorie” zu erstellen, die “Sehr gut” für Noten über 90, “Gut” für Noten zwischen 80 und 90 und “Schlecht” für Noten unter 80 angibt.\n\ndf &lt;- data.frame(Note = c(89, 92, 88, 76, 88, 72, 92, 95, 98))\n\n# Your Code Here\n\n2.) Verwenden Sie case_when(), um eine neue Spalte mit der Bezeichnung “Bestanden” zu erstellen, um zu Prüfen ob ein Schüler einer bestimmten Schulform eine Prüfung bestanden hat. Die Bestehensgrenzen sind wie folgt.\n-   Für die Hauptschule liegt die Bestehensgrenze bei 50 %\n-   Für die Realschule liegt die Bestehensgrenze bei 60 %\n-   Für das Gymnasium liegt die Bestehensgrenze bei 70 %\nKodieren Sie das bestehen entweder mit “Pass” oder “Fail”.\n\ndf &lt;- data.frame(Note = runif(100,min=0, max=100),\n                 Schulform = sample(c(\"Gymnasium\",\"Realschule\", \"Hauptschule\"),\n                                    size = 100, replace = T))\n# Your Code here\n\n3.) Nutzen Sie die Funktion case_when und die dplyr-Library in R, um eine neue Spalte in dem Datensatz “df” zu erstellen, die die Einkommenskategorie jeder Person basierend auf ihrem Berufsstatus und ihrem Einkommen kategorisiert. Die Einkommenskategorien sollten wie folgt sein:\n\nFür Angestellte mit einem Einkommen von bis zu 50.000: “niedrig”\nFür Angestellte mit einem Einkommen zwischen 50.000 und 75.000: “mittel”\nFür Angestellte mit einem Einkommen über 75.000: “hoch”\nFür Freiberufler mit einem Einkommen von bis zu 60.000: “niedrig”\nFür Freiberufler mit einem Einkommen zwischen 60.000 und 100.000: “mittel”\nFür Freiberufler mit einem Einkommen über 100.000: “hoch”\nFür Ruheständler mit einem Einkommen von bis zu 30.000: “niedrig”\nFür Ruheständler mit einem Einkommen über 30.000: “mittel_hoch”\n\n\ndf &lt;- data.frame(ID = c(\"Peter\", \"Anna\", \"Max\"),\n                 Alter = c(25, 35, 45),\n                 Berufsstatus = c(\"Angestellter\", \"Freiberufler\", \"Ruheständler\"),\n                 Einkommen = c(45000, 75000, 32000))\n\n\n\n4.2 Pivotting\n1.) Konvertieren Sie die folgenden Datensatz von wide to long. Erstellen Sie aus den Spalten zwei neue Spalten mit den Namen (month und index). Nutzen Sie dazu das Argument names_sep = \"_\". Die Werte sollen in die Spalte “N” geschrieben werden.\nTip: Sie müssen bei names_to einen Vektor mit den Namen der neuen Spalten angeben.\n\ndf_wide &lt;- data.frame(\n  Jan_sales = c(10, 40, 70),\n  Feb_sales = c(20, 50, 80),\n  Mar_sales = c(30, 60, 90)\n)\n\n# Your Code here\n\n2.) Konvertieren Sie die folgendn Datensatz von long in wide.\n\n    df_long &lt;- data.frame(\n      ID = c(\"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\", \"A1\", \"A2\", \"A3\"),\n      month = c(\"Jan\", \"Jan\", \"Jan\", \"Feb\", \"Feb\", \"Feb\", \"Mar\", \"Mar\", \"Mar\"),\n      sales = c(10, 40, 70, 20, 50, 80, 30, 60, 90))\n      \n\n    # Your Code Here"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin1.html",
    "href": "cogmod_sem_sose23/Termin1.html",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "",
    "text": "Prinzipiell besteht keine Anwesenheitspflicht im Seminar, allerdings rate ich dringend zur regelmäßigen Teilnahme ! Für meine eigene Übersicht wird eine Anwesenheitsliste geführt!"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin1.html#anwesenheit",
    "href": "cogmod_sem_sose23/Termin1.html#anwesenheit",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "",
    "text": "Prinzipiell besteht keine Anwesenheitspflicht im Seminar, allerdings rate ich dringend zur regelmäßigen Teilnahme ! Für meine eigene Übersicht wird eine Anwesenheitsliste geführt!"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin1.html#studienleistung",
    "href": "cogmod_sem_sose23/Termin1.html#studienleistung",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Studienleistung",
    "text": "Studienleistung\nDie Studienleistung für das Seminar besteht in der wöchentlichen Abgabe von Aufgaben, die sich auf den behandelten Stoff beziehen. Die Aufgaben werden über Assignments in Teams abgegeben und sind jeweils bis Montags 12:00 Uhr vor dem jeweilig nächsten Seminar fällig. Am Ende des Semesters wird dann ein Portfolio mit allen Aufgaben abgegeben, was die Gesamtstudienleistung darstellt. Es müssen zum bestehen alle Aufgaben eingereicht werden !\nDie Aufgaben werden mit fortschreitendem Semester anspruchsvoller und können folgendes umfassen:\n\nFragen zum Stoff\nProgrammieraufgaben in R\nDatenanalyse und Modellierung mit R"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin1.html#teams-channel",
    "href": "cogmod_sem_sose23/Termin1.html#teams-channel",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Teams Channel ",
    "text": "Teams Channel \nWir werden ausschließlich mit Teams und dieser Website arbeiten ! Alle wichtigen Ankündigungen werden im Teams Channel gemacht, daher ist es wichtig regelemäßig dort hineinzuschauen. Auf dieser Website werden kurze Zusammenfassungen und weitere Materialien (Slides, Literatur, Skripte und Videos) für die Seminareinheiten zur Verfügung gestellt, sodass ihr alles aus einer Hand nacharbeiten könnt.\n\nFahrplan\n\n\nFahrplan\n\n\n\nKW\nThema Seminar\nLink\nMaterialien\n\n\n\n\n16\nOrganisation und Ablauf\nZusammenfassung\n\n\n\n17\nEinführung I: Grundlagen der Modellierung\n\n \n\n\n19\nEinführung II: Grundlagen der Modellierung\n\n  \n\n\n20\nParameterschätzung I: Diskrepanzfunktionen & Schätzalgorithmen\n\n  \n\n\n21\nParameterschätzung II: Maximum Likelihood & Beyond\n\n\n\n\n23\nParameterschätzung III: Hands On in R Parameter Estimation\n\n\n\n\n24\nAdvanced R for Cognitive Modeling\n\n\n\n\n25\nMultinomial Processing Tree Models (Theorie)\n\n\n\n\n26\nAnwendung von MPT Modellen (R-Sitzung)\n\n\n\n\n27\nDrift Diffusion Models (Theory)\n\n\n\n\n28\nMemory Measurement Model (M3)\n\n\n\n\n29\nAnwendung des M3 Modells\n\n\n\n\n30\nAbgabe des kompletten Portfolios"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin2.html#why-models",
    "href": "cogmod_sem_sose23/Termin2.html#why-models",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Why Models?",
    "text": "Why Models?\nAls Psychologen wollen wir menschliches Verhalten beschreiben, vorhersagen und letztendlich erklären. Wir unterscheiden uns damit nicht wesentlich von einem Physiker, der Phänomene der Welt erklären möchte. Allerdings möchten Psychologen kognitive Phänomene erklären wie zum Beispiel\n\nGedächtnisprozesse\nSprachverarbeitung\nGedankenabschweifen\n\nund noch viele weitere kognitive Prozesse. Allerdings ist es schwierig nur anhand von Daten, wie zum Beispiel Reaktionszeiten und Fehlerraten, und verbalen Theorien kognitive Prozesse zu verstehen."
  },
  {
    "objectID": "cogmod_sem_sose23/Termin2.html#studienleistung",
    "href": "cogmod_sem_sose23/Termin2.html#studienleistung",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Studienleistung",
    "text": "Studienleistung\nDie Studienleistung für das Seminar besteht in der wöchentlichen Abgabe von Aufgaben, die sich auf den behandelten Stoff beziehen. Die Aufgaben werden über Assignments in Teams abgegeben und sind jeweils bis Montags 12:00 Uhr vor dem jeweilig nächsten Seminar fällig. Am Ende des Semesters wird dann ein Portfolio mit allen Aufgaben abgegeben, was die Gesamtstudienleistung darstellt. Es müssen zum bestehen alle Aufgaben eingereicht werden !\nDie Aufgaben werden mit fortschreitendem Semester anspruchsvoller und können folgendes umfassen:\n\nFragen zum Stoff\nProgrammieraufgaben in R\nDatenanalyse und Modellierung mit R"
  },
  {
    "objectID": "cogmod_sem_sose23/Termin2.html#teams-channel",
    "href": "cogmod_sem_sose23/Termin2.html#teams-channel",
    "title": "Seminar Fortgeschrittene statistische Methoden II (3)",
    "section": "Teams Channel ",
    "text": "Teams Channel \nWir werden ausschließlich mit Teams und dieser Website arbeiten ! Alle wichtigen Ankündigungen werden im Teams Channel gemacht, daher ist es wichtig regelemäßig dort hineinzuschauen. Auf dieser Website werden kurze Zusammenfassungen und weitere Materialien (Slides, Literatur, Skripte und Videos) für die Seminareinheiten zur Verfügung gestellt, sodass ihr alles aus einer Hand nacharbeiten könnt.\n\nFahrplan\n\n\nFahrplan\n\n\n\nKW\nThema Seminar\nLink\nMaterialien\n\n\n\n\n16\nOrganisation und Ablauf\nZusammenfassung\n\n\n\n17\nEinführung I: Grundlagen der Modellierung\n\n \n\n\n19\nEinführung II: Grundlagen der Modellierung\n\n  \n\n\n20\nParameterschätzung I: Diskrepanzfunktionen & Schätzalgorithmen\n\n  \n\n\n21\nParameterschätzung II: Maximum Likelihood & Beyond\n\n\n\n\n23\nParameterschätzung III: Hands On in R Parameter Estimation\n\n\n\n\n24\nAdvanced R for Cognitive Modeling\n\n\n\n\n25\nMultinomial Processing Tree Models (Theorie)\n\n\n\n\n26\nAnwendung von MPT Modellen (R-Sitzung)\n\n\n\n\n27\nDrift Diffusion Models (Theory)\n\n\n\n\n28\nMemory Measurement Model (M3)\n\n\n\n\n29\nAnwendung des M3 Modells\n\n\n\n\n30\nAbgabe des kompletten Portfolios"
  }
]